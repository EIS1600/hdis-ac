# Optimization and Exploratory Data Analysis

## Optimizations

### Data and Functions

At the moment we have our data pretty much in the original format and it will require a few steps to load and pre-process some elements so that it becomes suitable for further analyses. We can also prepare some data that we know for sure we are going to use. This is the case when we might also prepare it once and save so that we can reuse it easier. This is where a list is the best data structure to use---each new table can be added as an item to the list and the entire list can ve saved as an RSD object.

```{r message=FALSE}

library(tidyverse)

ac_data <- read_delim("data/AS_data.tsv", delim = "\t") %>% mutate(across(where(is.character), trimws))
ac_metadata <- read_delim("data/AS_metadata.tsv", delim = "\t") %>% mutate(across(where(is.character), trimws))
ac_texts <- read_delim("data/AS_texts.tsv", delim = "\t") %>% mutate(across(where(is.character), trimws))

acList <- list(metadata = ac_metadata, data = ac_data, texts = ac_texts)

ac_dates <- acList$data %>%
  filter(attribute_type == "attributes.year") %>%
  mutate(attribute_value = as.numeric(attribute_value)) %>%
  rename(date_year = attribute_value) %>%
  select(AS_ID, date_year)

acList$dates <- ac_dates

saveRDS(acList, "data/acData.rds")

```

**HW:** Think which subsets of data are most useful to have prepared, then prepare them and add them to the main data list.


We are often may need to reuse the same functions and variables again and again. What we can do is to collect those functions into a separate file and load almost like we do with libraries. For example, we can save the code below into a separate file (`custom_functions.R`)


```{r echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# some helping data and functions

AH2CEa <- function(AH) {
  CE <- round(AH - AH/33 + 622)
  return(CE)
}

AH2CEb <- function(AH) {
  CE <- round(AH - AH/33 + 622)
  AH <- ifelse(AH == 0, 1, AH)
  final <- paste0(AH, " AH\n", CE, " CE")
  return(final)
}

periodsAH <- seq(0, 1400, 100)
periodsCEa <- AH2CEa(periodsAH)
periodsCEb <- AH2CEb(periodsAH)
```

We can load these functions in the following manner (keep in mind that you need to give the correct path to the file; this path --- or no path, other than the file name itself --- means that our file is in the main folder of the project):

```{r}
source("custom_functions.R")

print(periodsCEb)
```

We can also write a script with all te necessary steps of preparing our data, which will make all the necessary tibbles and variables available. This could be even a better approach than keeping all the data in RDS, especially when we know that the data will be periodically updated, which is the case with the Audition Certificates data. This way our script can prepare data for analyses from from the latest versions of AC initial data.

## Scripts

So far we have been working with notebooks, which are convenient in a variety of ways, but they are not necessarily the most optimal way to run your code, especially when you need to run complex tasks and when you need to repeat this process multiple times. This is where scripts are very convenient --- preparing data from the latest data files would be a job for a script, for example, as we discussed above.

Running scripts:

- you can open it on RStudio, select everything and click `Run` button, or, more conveniently
- you can run it from the command line tool on your computer. The command is usually: `Rscript yourScript.R`, provided your script is in the same folder from which you are running the command.

Advantages of scripts:

- running a script from the command line takes less resources than using RStudio; it also usually faster;
- one can run multiple scripts at the same time, which is impossible with RStudio;
- one can still use RStudio, while a script is running on the command line;

## Scripts for Automated Workflows

What we discussed above as a script for preparing data can be categorized as an automated workflow---if your script systematically processes data, performs analysis, and perhaps even generates reports or visualizations without manual intervention, it can be considered an automated workflow. This term emphasizes the efficiency and repeatability of the process.

For example, we can design a script (it can also be a notebook), which will perform specific analytical routines on supplied data. For example, we can design a process that will run exploratory data analysis on our data---and we can rerun it on new data anytime it becomes available.

## Code chunks

Code chunks are what make all the analyses, but you may not want to include those chunks into reports which you want to share with those who have not use for them. Solution: you can change the settings for each chunk. You can toggle different settings with the gear button at the top right corner of any code chunk. You can also code them manually. For example, `{r echo=TRUE, eval=FALSE}` will show the code, but will not execute it during knitting; `{r echo=FALSE, eval=TRUE}` will run the code, but will not show the code chunk in the knitted document.

When you include images into your notebook, you can also control the size of the image at the beginning of the code chunk: `{r fig.width = 10, fig.height = 3}` will generate a plot of 10 inch wide and 3 inch tall.


## Exploratory Data Analysis

Exploratory Data Analysis (EDA) is a critical step in the data analysis process, which involves examining and summarizing the main characteristics of a dataset, often with visual methods. The primary objective of EDA is to understand the data, discover patterns and anomalies, and formulate hypotheses. Here are some key aspects of EDA:

- Understanding the Data Structure: This involves getting familiar with the dataset by looking at the number of rows and columns, types of variables (categorical, numerical), and identifying any missing values.
- Summary Statistics: Calculating basic statistical measures like mean, median, mode, standard deviation, etc., for each column in the dataset to understand the distribution and central tendencies of the data.
- Data Cleaning: Identifying and correcting errors or inconsistencies in the data, dealing with missing values, and potentially removing outliers that could skew the analysis.
- Data Visualization: Using graphs and plots to understand trends, patterns, and relationships within the data. Common visualizations include histograms, scatter plots, box plots, and bar charts.
- Identifying Patterns and Relationships: Looking for correlations or associations between variables. This can help in understanding the dynamics within the data and can be pivotal for predictive modeling.
- Formulating Hypotheses: Based on the insights gained, hypotheses can be developed about the data, which might then be tested more rigorously through statistical methods or predictive modeling.
- Feature Engineering: Creating new variables or modifying existing ones to improve the effectiveness of statistical models.
- Checking for Assumptions: In the context of applying statistical models or machine learning algorithms, it's important to check if the data meet the assumptions required for these methods.
- Documenting Findings: Keeping a record of findings and insights gained during EDA, which can be valuable for future reference and decision-making.
- Iterative Process: EDA is often an iterative process where the findings lead to more questions and subsequent analysis, refining the understanding of the dataset.

EDA is a flexible and context-dependent process. The specific methods and tools used can vary depending on the nature of the data and the goals of the analysis. The key is to remain open to new insights and be prepared to adjust hypotheses and approaches as new information is discovered.

## Tasks to discuss

- We looked at the number of sessions and participants by the days of the week;
  - we can also check months: are certain months (Ramaḍān?) are more important in this context?
  - we can also try to look at the solar months, perhaps there is some correlation with seasons (too hot or too cold)?
    - here the problem is in the conversion of Hijrī dates to Gregorian dates, but we should be able to get months more or less right.
    - converter: <https://www.muqawwim.com/>, code: <https://github.com/theodore-s-beers/muqawwim>; **very-hard-but-cool task**: 1) find the code for this app; 2) convert it into R code; 3) use it to convert all our dates date;

- Network potential:
  - We can start by simply counting how many times the same names appear across our dataset;
  - a histograms can help us interpret the numbers;
  - we can try to identify names that looks similar and may point to the same individuals

- Roles in sessions:
  - We can check what individuals, based on their roles, appear most frequently;

- Women, children, slaves, etc.:
  - How can we identify different categories of participants?
    - Onomastic analysis: length of the name and the presence of “specific indicators” (*bint* or *umm* as an indicator of a woman);
    - Machine learning: classify participants into groups (what groups can we single out?) and then use some machine learning technique to classify other participants
  
- Finding biographies of these individuals:
  - we can automatically match these names against some source of biographical data;

### Code for onomastic data

- tokenize names;
- create frequency list of name elements;
- manually classify them;
- importance of normalization:
  - discuss necessary steps


```{r}

names <- acList$data %>%
  filter(type == "person") %>%
  mutate(person.id = paste0(AS_ID, "__", startIndex)) %>%
  select(person.id, text, attribute_value) %>%
  rename(name = text, role = attribute_value)

peopleCount <- names %>%
  group_by(name) %>%
  summarize(freq = n()) %>%
  ungroup()

```







