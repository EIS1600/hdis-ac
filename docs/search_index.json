[["index.html", "HDIS: Computational Analysis of Data from Audition Certificates–with R Preface", " HDIS: Computational Analysis of Data from Audition Certificates–with R Maxim G. Romanov 2024-01-12 Preface This is a Handbook for Digital Islamicate Studies. Work in progress, based on courses tought. The current course, “Computational Analysis of Data from Audition Certificates–with R”, aims to introduce students to the main methods of data analysis that would be suitable for historical data. Students will learn the basics of the programming language R, which is one of the top choices in the field of Digital Humanities and Digital History. By the end of the course, students will understand how to work with data and how to extract the most valuable insights from it (exploratory data analysis). Students will practice on a dataset created from audition statements from medieval Damascus, thus also gaining original insights into the social history of medieval Islamic learning. There are no prerequisites for this course, except the basic knowledge of Arabic, since the data from audition statements is in Arabic. The course is complimentary to 57-521, where students will be learning how to read the original handwritten audition statements. "],["about-the-course.html", "About the Course Syllabus", " About the Course The course, “Computational Analysis of Data from Audition Certificates–with R”, aims to introduce students to the main methods of data analysis that would be suitable for historical data. Students will learn the basics of the programming language R, which is one of the top choices in the field of Digital Humanities and Digital History. By the end of the course, students will understand how to work with data and how to extract the most valuable insights from it (exploratory data analysis). Students will practice on a dataset created from audition certificates/statements from medieval Damascus, thus also gaining original insights into the social history of medieval Islamic learning. There are no prerequisites for this course, except the basic knowledge of Arabic, since the data from audition statements is in Arabic. The course is complimentary to 57-521, where students will be learning how to read the original handwritten audition statements. Syllabus Full nomenclature: [UHH AAI WiSe 23/24] 57-525 ONLINE: Computational Analysis of Data from Audition Certificates (Complimentary to 57-521) Language of instruction: English Hours per week: 2 Credits: 6.0 Meeting time: Fr 16:00-18:00 Additional meeting time: we will need to find a time slot when you can all join and work on your HW assignments together Meeting place: for convenience, all meetings will be held online via Zoom: https://uni-hamburg.zoom.us/j/63482607267?pwd=OVFIZWpndVZOU2RzOWtlVUtaM1lLUT09; Course resources: https://eis1600.aai.uni-hamburg.de/hdis-ac/; Meeting link: shared via Slack; other details are available via STiNE Office hours: tbd (on Zoom); if you have any questions, please, post them on Slack Instructor: Dr. Maxim Romanov, maxim.romanov@uni-hamburg.de General Description The course aims to introduce students to the main methods of data analysis that would be suitable for historical data. Students will learn the basics of the programming language R, which is one of the top choices in the field of Digital Humanities and Digital History. By the end of the course, students will understand how to work with data and how to extract the most valuable insights from it (exploratory data analysis). Students will practice on a dataset created from audition statements from medieval Damascus, thus also gaining original insights into the social history of medieval Islamic learning. There are no prerequisites for this course, except the basic knowledge of Arabic, since the data from audition statements is in Arabic. The course is complimentary to 57-521, where students will be learning how to read the original handwritten audition statements. Personal computers are required both for in-class work and for your homework. Your computer must run a full version of either Windows, MacOS, or Linux; unfortunately, neither tablets nor Chrome-based laptops are suitable for this course. No prior programming experience is required, but familiarity with the command line and basic principles of programming will be beneficial. Learning objectives Get basic familiarity with the programming language R; Learn basic analytical techniques; Gain an understanding of working with data and modeling data; Practice these skills on a collection of historical data; Didactic concept This is a hands-on practical course, which requires regular attendance and completion of homework assignments. The participants of the course are encouraged to attend weekly “office hours”, where they can work on their homework assignments and get immediate feedback from the instructor. The main didactic approach of the course is to maximize the engagement of the participants with the programming language: this will provide a level of confidence and comfort in dealing with admittedly an alien subject within the scope of African and Asian studies; attaining this level of comfort is the key to absorbing the necessary theoretical, conceptual, and practical knowledge. Upon the completion of assigned tasks, the students are encouraged to bring their own datasets, since the engagement with their own data will provide a better grounding in this new subject. Literature All the necessary study materials will be provided. Below you can find a list of additional materials. Course Evaluation: requirements for the full credit mandatory attendance and participation; timely homework assignments; final project (computational analysis + analytical commentary); no examination; Final projects can be prepared either individually or in groups. Class Participation Each class session will consist in large part of practical hands-on exercises led by the instructor. Personal computers are required. We can accommodate whatever operating system you use (Windows, Mac, Linux), but it should be a full computer/laptop, not a tablet that uses an “incomplete” version of any major operating system. Don’t forget that asking for help counts as participation! Homework Just as in research and real life, collaboration is a very good way to learn and is therefore encouraged. If you need help with any assignment, you are welcome to ask a fellow student. If you do work together on homework assignments, then when you submit it please include a brief note (just a sentence or two) to indicate who did what. NB: On submitting homework, see below. Final Project Final project will be discussed later. You will have an option to build on what we will be doing in class, but you are most encouraged to pick a topic of your own. The best option will be to work on something relevant to your field of study, your term paper or your thesis. Additional Study Materials You can also find recommended literature in the Bibliography below and in the section References. [#todo: add references to the references.bib file] (Arnold and Tilton 2015) Arnold, Taylor, and Lauren Tilton. Humanities Data in R. New York, NY: Springer Science+Business Media, 2015 (shared via Slack) (Healy 2018) Healy, Kieran. Data Visualization: A Practical Guide. Princeton University Press, 2018. ISBN: 978-0691181622. http://socviz.co/ (Hadley 2016) Hadley, Wickham. Ggplot2: Elegant Graphics for Data Analysis. New York, NY: Springer, 2016. (Hadley and Grolemund 2016) Hadley Wickham &amp; Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly, 2017. ISBN: 978-1491910399. https://r4ds.had.co.nz/ (Hadley 2014) Wickham, Hadley. Advanced R, Second Edition. 2 edition. Boca Raton: Chapman and Hall/CRC, 2019. http://adv-r.had.co.nz/ Also check https://bookdown.org/ for more books on R Coding Club R Tutorials (focus on Ecology and Environmental Sciences), https://ourcodingclub.github.io/tutorials.html NB: By the way, this website is also built with R. Check: Yihui Xie. bookdown: Authoring Books and Technical Documents with R Markdown, 2022 https://bookdown.org/yihui/bookdown/ Software, Tools, &amp; Technologies: The following is the list of software, applications and packages that we will be using in the course. Make sure to have them installed by the class when we are supposed to use them. The main tools for the course will be the programming language R and RStudio, the premier integrated development environment for R. R: https://cloud.r-project.org/ (choose the version for your operating system!) RStudio: https://rstudio.com/products/rstudio/download/ (RStudio Desktop, Open Source License — the free version) We will also use a variety of packages for R, which we will be installing when necessary. Submitting Homework: Homework assignments are to be submitted by the beginning of the next class; For the first few classes you must email them to the instructor (as attachments) In the subject of your email, please, use the following format: CourseAPPREVIATION-LessonID-HW-Lastname-matriculationNumber, for example, if I were to submit homework for the first lesson, my subject header would look like: CADAS-L01-HW-Romanov-12435687. DH is a collaborative field, so you are most welcome to work on your homework assignments in groups, however, you must still submit it. That is, if a groups of three works on one assignment, there must be three separate submissions: either emailed from each member’s email and published at each member’s github page. Schedule Location: Online 01 Fri, 20. Oct. 2023 16:00-18:00 02 Fri, 27. Oct. 2023 16:00-18:00 03 Fri, 03. Nov. 2023 16:00-18:00 04 Fri, 10. Nov. 2023 16:00-18:00 05 Fri, 17. Nov. 2023 16:00-18:00 06 Fri, 24. Nov. 2023 16:00-18:00 (*) 07 Fri, 01. Dec. 2023 16:00-18:00 08 Fri, 08. Dec. 2023 16:00-18:00 (*) 09 Fri, 15. Dec. 2023 16:00-18:00 (*) 10 Fri, 22. Dec. 2023 16:00-18:00 11 Fri, 12. Jan. 2024 16:00-18:00 12 Fri, 19. Jan. 2024 16:00-18:00 13 Fri, 26. Jan. 2024 16:00-18:00 14 Fri, 02. Feb. 2024 16:00-18:00 Lesson Topics (subject to modifications) [ #01 ] Introduction; Syllabus; Setting Everything Up; [ #02 ] Part I—Basics: RStudio, Basic R Commands, Swirl Tutorials [ #03 ] Part I—Basics: R Markdown and R Notebooks [ #04 ] Part I—Basics: Control Flow; Regular Expressions; [ #05 ] Part I—Basics: Data Manipulations; [ #06 ] Part I—Basics: Visualizations with plot() and ggplot(); [ #07 ] Part I—Basics: Tidying Data; [ #08 ] Part II—Analyses: Audition Statement Data; [ #09 ] Part II—Analyses: Audition Statement Data; [ #10 ] Part II—Analyses: Audition Statement Data; [ #11 ] Part II—Analyses: Audition Statement Data; [ #12 ] Part II—Analyses: Audition Statement Data; [ #13 ] Part II—Analyses: Audition Statement Data; [ #14 ] Part II—Analyses: Audition Statement Data; Bibliography "],["setting-everything-up.html", "1 Setting Everything Up 1.1 Links for things to install 1.2 Setting Things Up 1.3 Creating a Project 1.4 Homework assignments", " 1 Setting Everything Up [subject to updates and corrections] 1.1 Links for things to install https://cloud.r-project.org/ :: R is the programming language that we will be using; https://posit.co/downloads/ :: RStudio is an integrated development environment (IDE) for R (In other words, a convenient interface that makes it easier to code in R.) https://learnr-examples.shinyapps.io/ex-setup-r/ :: an interactive tutorial on how to install everything, with useful additional explanations. 1.2 Setting Things Up Important, like really-super-duper-important!!!: We will need to set everything up so that it is much easier during the course. Creating folders and subfolders it is crucially important that you create them the exact way it is described below: 1) use only small letters; 2) instead of spaces between words, use “_” (underscore). It is important that you set things up exactly like that. If you make mistakes in naming folders correctly, code that we will be using will most likely not work, because it will not be able to find files that it needs to load and/or save. Setting up our main folder: CADAS-R Create that folder in “Documents” (one of the default folders both on Mac and Windows); or anywhere else—ideally where you keep all your other folders related to other courses that you are taking; just remember where you create it; In your folder then create the following structure: CADAS-R/ ├── downloads/ ├── data/ │ ├── raw_data/ │ └── processed_pata/ ├── literature/ │ ├── articles/ │ └── books/ ├── classes/ │ ├── class_01/ │ ├── class_02/ │ └── class_03/ ├── pPresentations/ └── meeting_notes/ /downloads/ here we will store all the course-related files that you will be downloading; /data/ here we will keep files with our data; we will not really need /raw_data/ and /processed_data/, but, usually, in your own future projects you may want to have such a division, to keep the original data in one folder and data that you have processed (cleaned, updated, etc.) in a different folder; /literature/ This is a way to organize readings, relevant to your project; we will keep here relevant files; Using bibliography managers, like Zotero, is a better option though; Note, how the names of PDFs are following the same pattern: AuthorTitleYEAR.pdf. /classes/ here we will be placing files relevant to specific classes; mainly, these folders will keep the scripts that you will be working on; /presentations/ A folder for your final presentation/project; /meeting_notes/ You may want to store your notes separately; 1.3 Creating a Project now that you have created everything, you need to create what is called a “Project” in RStudio. This will create a single file, which you can open in order to load all the necessary settings relevant to our course. using “Projects” will help you to keep different research tasks separately and, most importantly, keep all the files relevant to specific tasks well-organized. So, how to create a project? open RStudio, then, in the main menu, choose: File &gt; New Project (you should then see “New Project Wizard”, as shown below); Select “Existing Directory” &gt; then click on “Browse”, and then select the folder “CADAS-R”, which you created in the first step; click “Create Project” to complete the process. Now, what will happen is that in the tab “Files” in the lower right part of RStudio you will see the contents of the folder “CADAS-R”. There will also be a new file, called CADAS-R.Rproj. Later on, you will be able to quickly open your project by double-clicking on that file. When you open the project, R automatically sets the working directory (setwd()) to the folder of the project and many things just become much easier. Now, you can add and create new files relevant to your project in the folder “CADAS-R” and they will be easy to find directly from RStudio, using its “Files” Tab, which is usually available in the lower right corner. Suggestion: you will benefit greatly from keeping some order in your project folder. For example, keep all the files that you download in a subfolder “downloads”; your your data files in a subfolder “data”, and so on. 1.4 Homework assignments Homework (Links will take you to tutorials): http://programminghistorian.org/ is a great resource for learning the basics of practical digital humanities. I encourage you to browse the lessons carefully, just to get a sense of what you can learn to do: http://programminghistorian.org/en/lessons/. Please, read the following assigned lessons carefully. In some cases, it is well worth trying to repeat all the steps on your own computer. I will mark those. Data organization: understanding and applying basic principles of data organization will save you a lot of time in the future and will help you to keep your data well organized and easily accessible. James Baker, “Preserving Your Research Data,” Programming Historian 3 (2014), https://doi.org/10.46430/phen0039. Command Line is the most foundational tool for anyone interested in doing anything remotely interesting with computers. Unlike graphical-user-interfaces (GIU), here you give commands to the computer by typing them in directly. You will need one of the following two tutorials, depending on whether you use Mac or Windows. You should try to repeat at least some of the commands given in tutorials. These tutorials are a little bit more detailed than what you will need for now; the most important thing for you to understand is how to navigate your computer (i.s., move from one folder to another), copy and move files, create folders, etc. Mac. Ian Milligan and James Baker, “Introduction to the Bash Command Line,” Programming Historian 3 (2014), https://doi.org/10.46430/phen0037 Windows. Ted Dawson, “Introduction to the Windows Command Line with PowerShell,” Programming Historian 5 (2016), https://doi.org/10.46430/phen0054 "],["rstudio-basic-r-commands-swirl-tutorials.html", "2 RStudio, Basic R Commands, Swirl Tutorials 2.1 This Chapter 2.2 Overview of R 2.3 RStudio Interface 2.4 Main File Types 2.5 Projects 2.6 Basic Commands 2.7 Built-in Functions 2.8 Libraries/Packages 2.9 Values and their types 2.10 Basic Data Structures and Their Practical Usage 2.11 Homework with swirl()", " 2 RStudio, Basic R Commands, Swirl Tutorials 2.1 This Chapter Overview of R RStudio and its interface; Describe the RStudio interface and its main components (Console, Script Editor, Environment, and Plots/Packages/Help); Explain the benefits of using RStudio for R programming; File types and projects; Basic operators and commands; Built-in functions and libraries; Basic data structures and when they are used; swirl() tutorials as homework assignments; 2.2 Overview of R R, its features, and its relevance to historians; R as an open-source programming language and its community; Comprehensive R Archive Network (CRAN); 2.3 RStudio Interface RStudio interface and its main components (Console, Script Editor, Environment, and Plots/Packages/Help) The benefits of using RStudio for R programming Rstudio Interface top-left: Open Scripts and Files: this is where you will be working with your R scripts; bottom-left: R console: this is where you can run R code (without committing it to a script; very handy for quickly testing something); Terminal: skipping for now top-right: Environment: all active variables are listed here; you can also load data through the “Import Dataset” dialog from here; this tab is useful to keep track of what is loaded into your current state of R; History: here you will find the complete list of commands that you have run already; plus, some other tabs which are not too relevant for now; bottom-right: Files: this is your file browser, which is useful when you work with projects; Plots: all your temporary plots will be appearing here; Packages: this is the list of all installed and loaded packages (not the most important tab); Help: whenever you invoke help, details will be shown here; Viewer: this tab will be showing the results of HTML output; most useful for working with notebooks (next class); plus, some other tabs which are not too relevant for now; for a video explanation: https://www.youtube.com/watch?v=XdgfhqNtje4 2.4 Main File Types main file types: R script, R notebook, and R markdown; R script: contains only R code and comments; most useful for “silent” processing of data and time-consuming analyses that are best run from command line; R notebook (R markdown): allows one to combine executable R code and academic prose; with this format one can create documents that can be regenerated when data is updated scenario: you describe some statistical data—on population, for example; in a year or two you get an updated file; your old descriptions may still be true—overall population, distribution of population over years, etc., but the data changed; you can simply regenerate your notebook with new data and all the numbers and graphs will be updated. you can create them from: File &gt; New File or the icon with green plus (top left corner); for now, we will try an R script; next class we will start with R notebooks; 2.5 Projects RStudio projects make it straightforward to divide your work into multiple contexts, each with their own working directory, workspace, history, and source documents. Creating our project: File &gt; New Project Existing Directory choose the directory for the course that we created last time: CADAS-R (we agreed that you create it in your “Documents” folder—both Mac ands Windows have such a folder) click Create Project Using your Project: you now can open RStudio by clicking on the CADAS-R.Rproj file in your course folder; this will automatically adjust RStudio settings to that folder in Files (bottom-right corner) you will automatically see the folder of your project; you can now use paths relative to your project’s folder: review: what are relative and absolute paths? Closing your Project: If you did any work with your project and then want to close it, RStudio will ask you if you want to Save workspace image to ..., click Do not save; while saving may be useful occasionally, in most cases that creates a large temporary file which makes your Rstudio slower. 2.6 Basic Commands let’s create an R script file in classes/class_02/ and call it class_02.R here is some little filler for our script. Simply copy-paste it into the new script. # arithmetic operations 3 + 5 3 - 5 3 / 5 3 * 5 # assignment: `&lt;-` or `=`; x &lt;- 3 + 5 x x * 5 x - 5 y &lt;- 3 * 5 y + x z &lt;- y + x # = Vs == x == y x = y x == y now we can try basic R operations: we can write them in our R script (top-left part of RStudio interface); we can also run them directly in Console (bottom-left part of RStudio interface); the main differences are: we can save all our commands in R script, but we cannot do that in console; console, however, is very useful for running commands that we do not want to save (for example, using help commands) R script is most useful when we want to do something complex and we need a series of commands that must run in a specific order; this will be the main use of R scripts (as well as R notebooks); In R script you can execute all commands one after another (as they appear in the script); in console you can execute only one command at a time; basic arithmetic operations: +, -, *, /; let’s try some examples from what we pasted into our script; values and variables: values: when we simply typed our commands as we just did we use values, but they are gone as soon as we type the next one; variables allow us to preserve results for later reuse; variables: assignment operator (&lt;- or =) and variable assignment; difference between = and ==; comments: we can use # in front of any line to turn it into a comment; comment means that R will not try to execute it. 2.7 Built-in Functions Let’s add the following few lines to the end of our R script: # built-in functions numbers &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9) max(numbers) min(numbers) mean(numbers) sum(numbers) R comes with some built-in functions, which have this format: function(arguments) functions usually pack a few steps that transform and manipulate supplied data (argument) and return the result of those manipulations; in the code we created a vector (I will explain in a moment what vector is) names numbers we can try some of the built-in functions and use this variable as an argument. What do you think the following functions will do? max() :: ? min() :: ? mean() :: ? sum() :: ? functions may take/require a different number of arguments; arguments may be data that you supply and/or some parameters that trigger a specific behavior of a function; in order to find out how a specific function works—and what arguments it takes, you can use help function or operator. For example, if we want to get help for the function mean(), we can do the following (type and execute): ?mean (or ??mean — this will give broader results); help(mean); Thus, help tells us that mean() can take the following arguments: mean(x, trim = 0, na.rm = FALSE, ...) Alternatively, you can always google how to do a certain thing in R… 2.8 Libraries/Packages The concept of R packages and their role in extending R’s functionality; you can think of packages as additional programs for R (like MS Word on your computers that gives you a useful tool for writing your papers); packages give you access to more functions; Packages can be installed: using the install.packages() function (the name of the library should be in quotation marks); let’s install ggplot2 with this command; let’s also install stringi with this command; using interface: Tools &gt; Install Packages let’s install tidyverse via the interface; let’s also install library swirl :: you will need it for your homework; Packages can be loaded using function library() (or require()); library(ggplot2) library(tidyverse) library(stringi) Alternatively, you can call specific functions directly from specific libraries: stringi::stri_rand_lipsum(2) :: try this example; this way is sometimes necessary when there are functions with the same name in different loaded packages; you can try to run the following line of code to generate a graph: ggplot2::ggplot(mtcars) + ggplot2::geom_point(ggplot2::aes(x = mpg, y = hp)) this code becomes simple, if you have loaded the library ggplot2 in advance: ggplot(mtcars) + geom_point(aes(x = mpg, y = hp)) 2.9 Values and their types There are several main types of values in R (these are the most important ones): numeric: 3 or 3,14 character: character or \"3.14\" logical: TRUE or FALSE You can check the type of value by using class() function, suppling the value or the variable as an argument. You can convert between types: as.numeric() to convert to numeric; you will not be able to convert to numeric a vector that contains letter characters; as.character() to convert to character; any vector can be converted to character; There are some other types, which we will cover when they become relevant. 2.10 Basic Data Structures and Their Practical Usage One can say that everything in R revolves around vectors and vector algebra. This is not as scary as it may sound, but it has an extremely profound effect on how everything is done in R. A vector is a fundamental data structure that represents a one-dimensional array containing elements of the same data type. Vectors are used to store and manipulate collections of values, such as numbers, characters, or logical values. They are the basic building blocks for more complex data structures like matrices, data frames, and lists. Unless there is evidence to the opposite, you can assume that a variable/value is a vector. All operations in R work like vector operations, which makes R effective, efficient, and quite elegant. Vectors in R are versatile and can be used to perform various operations, such as element-wise arithmetic, comparisons, and aggregations. R has a rich set of built-in functions for working with vectors, which makes it a powerful language for data manipulation and analysis. Keep in mind that the elements of a vector must be of the same type. If you try to combine different types of elements in a vector, R will attempt to coerce the elements to a common type, following a hierarchy of types (logical &lt; integer &lt; double &lt; character). For example, if you combine numeric and character values in a vector, R will convert the numeric values to character values. If you need to store elements of different types, you can use a list, which is a more flexible data structure in R. 2.10.1 Basic data structures vectors; matrices; dataframes/tibbles; lists; 2.10.2 Main properties of basic data structures single dimension (x axis): vectors: each vector is a vector a vector can hold data of only one type! two-dimensions (x-axis: rows, y-axis: columns): matrices: a vector folded into two dimensions; (each element can be accessed as if it were a vector - 1dim coordinates, or as if it were a matrix – 2dim coordinates;) like a vector, a matrix can only hold data of one type! dataframes / tibbles; columns and rows, where each column is a vector (i.e., must be the same type); heterogeneous: lists: anything can be an item in a list; 2.10.3 Most Common Usage of Basic Data Structures: storing data: most commonly, data frames / tibbles are used for storing data; you will load data into R in a form of a data frame and then you will start your analyses; calculations (modifications, alterations, updates, etc.): these are most commonly performed on vectors, or, most commonly, columns of data frames or, in case of matrices, on complete matrices or their columns or rows. Matrices are used almost exclusively for complex calculations (for example, we can use them for identifying groups of individuals with similar characteristics). export of data from complex functions: list are very convenient to return results of complex functions, since they can hold any types of data structures in a single object; we are not going to use them much (if at all); they become useful after you reach a certain level of complexity in your work with R; 2.10.4 Basic Data Structures Examples 2.10.4.1 Vectors In R, a vector is a one-dimensional array that can store a collection of values of the same type. You can create a vector using the c() function, which combines multiple values into a single vector. Here’s an example of vectors. (Check what happens with mixed_vector). numeric_vector &lt;- c(10, 20, 30, 40, 50) character_vector &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;, &quot;date&quot;, &quot;fig&quot;) mixed_vector &lt;- c(10, 20, 30, 40, 50, &quot;banana&quot;) Print out: [1] 10 20 30 40 50 [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; &quot;date&quot; &quot;fig&quot; [1] &quot;10&quot; &quot;20&quot; &quot;30&quot; &quot;40&quot; &quot;50&quot; &quot;banana&quot; 2.10.4.2 Matrices In R, a matrix is a two-dimensional array that can store a collection of values of the same type, organized in rows and columns. You can create a matrix using the matrix() function. Here’s an example of a numeric matrix: numeric_matrix &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3) Print out: [,1] [,2] [,3] [1,] 1 3 5 [2,] 2 4 6 2.10.4.3 Data.frames/tibbles A data frame is a table-like data structure in R, where each column can contain different types of data, and rows represent observations. A tibble (short for “tidy data frame”) is a modern version of a data frame, introduced by the tidyverse package collection, which offers some improvements over the traditional data frame, such as better printing and subsetting. # Load required libraries library(dplyr) library(tibble) # Define the data names &lt;- c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;) ages &lt;- c(25, 30, 22) gender &lt;- c(&quot;F&quot;, &quot;M&quot;, &quot;M&quot;) # Create the data frame my_dataframe &lt;- data.frame(Name = names, Age = ages, Gender = gender) # Create the tibble my_tibble &lt;- tibble(Name = names, Age = ages, Gender = gender) Print out for data.frame: Name Age Gender 1 Alice 25 F 2 Bob 30 M 3 Charlie 22 M Print out for tibble: # A tibble: 3 × 3 Name Age Gender &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 Alice 25 F 2 Bob 30 M 3 Charlie 22 M 2.10.4.4 Lists In R, a list is a versatile data structure that can store a collection of values, where each element can be of a different type or even another data structure like a vector, matrix, or another list. You can create a list using the list() function. Here’s an example of a list: # Define the data names &lt;- c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;) matrix_data &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3) sublist &lt;- list(temperature = 72, weather = &quot;sunny&quot;) # Create the list my_list &lt;- list(Names = names, Data = matrix_data, Sublist = sublist) Print out: $Names [1] &quot;Alice&quot; &quot;Bob&quot; &quot;Charlie&quot; $Data [,1] [,2] [,3] [1,] 1 3 5 [2,] 2 4 6 $Sublist $Sublist$temperature [1] 72 $Sublist$weather [1] &quot;sunny&quot; 2.10.4.5 RDS Files They provide a convenient and efficient way to save and load R objects such as vectors, matrices, data frames, lists, and models. RDS files use a compact representation, which makes them smaller in size and faster to read and write compared to text-based file formats like CSV or TSV. You can save an R object to an RDS file using the saveRDS() function and load an R object from an RDS file using the readRDS() function. RDS files are particularly useful when working with large datasets or complex objects, as they allow for fast and efficient storage and retrieval. However, keep in mind that RDS files are specific to R and may not be compatible with other programming languages or software without additional conversion. Summary: quick and small, because they are binary; very handy, because one can use the same command to save/load any data structure; 2.11 Homework with swirl() Homework assignment: Swirl Course Tutorials; Swirl Course: R Programing(you will be asked to install it in the beginning); other course can be installed with install_course(\"name of the course\") (perhaps, the only relevant to us course will be “Exploratory Data Analysis”); Swirl: start the R Programming course; Swirl tutorials to complete: for Lesson 02: Basic Building Blocks; Workspace and Files; Sequences of Numbers; Vectors; for Lesson 03: Missing Values; Subsetting Vectors; Matrices and Dataframes; Logic; Swirl commands: library(swirl) to load the library; then type swirl() to start it; your main course is R Programming start it and complete the assigned modules; in the middle of the lesson, you can use play() to go into a free mode, if you need to check something in R; type nxt() to return to the lesson; Swirl Additional Courses: http://swirlstats.com/scn/title.html there are some more courses, with quite a few in German and Spanish; recommend you to try for practice; Swirl assignment submission: when you reach the end of the course, choose yes if you want to receive credit; type in your email; type in XXXXX for your Corsera.org token; then, you will get an error; do not worry, I want you to take a screenshot of this screen and email it to me as your confirmation; (take a screenshot of either your entire screen, or of the entire RStudio screen). in the topic of our email, use the format described in the syllabus; please, send one email per module, that is to say, you will need to send me 4 emails by the next class; I recommend you to do these whenever you have time; it is best to do them not in one go, but rather take breaks between them; "],["r-markdown-and-r-notebooks.html", "3 R Markdown and R Notebooks 3.1 This Chapter 3.2 R Notebook/Markdown 3.3 Homework", " 3 R Markdown and R Notebooks 3.1 This Chapter we will start with R notebooks which will become our main format for the class; you will be creating them for each class, completing assignments, and generating final document which you will be sending to me as your homework; we will need some new packages: rmarkdown; knitr; and some files to get us started: place these three files into the main folder of your project: bibliography.bib :: a file with bibliographical records; chicago-author-date.csl :: Chicago Manual of Style 17th edition (author-date) (2022-12-12 04:02:09) chicago-fullnote-bibliography.csl :: Chicago Manual of Style 17th edition (full note) (2023-03-31 12:44:56) 3.2 R Notebook/Markdown Technically, there are two types of R documents: R notebooks and R markdown. R notebooks are supposed to be more interactive, but they also seems to cause too many issues when one tries to generate them. We will focus on R markdown documents. R Notebooks/Markdown are complex documents that combine executable R code, formatted text, images, and other elements in a single file. They are a powerful tool for data exploration, analysis, and visualization, as well as for creating reproducible research documents and reports. R Notebooks are a part of the R Markdown ecosystem, which provides a suite of tools for creating dynamic, self-contained documents using the R programming language. R Notebooks are created and edited using RStudio, a popular integrated development environment (IDE) for R. RStudio provides a user-friendly interface for creating and working with R Notebooks, which are saved as files with the extension .Rmd. These files are plain text files that use the Markdown language for formatting text, and R code is embedded using code chunks. Some key features of R Notebooks include: YML Header: this block is always at the very beginning of R notebook. YAML (short for “YAML Ain’t Markup Language”, or “Yet Another Markup Language”) is a human-readable data serialization format used to store metadata and configuration settings for the notebook. YAML is used in the R Notebook’s header, which is also known as the YAML front matter or YAML header. This header is placed at the very beginning of the .Rmd file and is enclosed between two sets of triple dashes (---). The YAML header contains key-value pairs that define various settings and options for the R Notebook, such as the title, author, date, and output format. These settings are used by the rmarkdown package to control how the notebook is rendered and displayed. Code Chunks: R code is embedded in the notebook using code chunks, which are enclosed in triple backticks (```). These chunks can be executed independently or as part of the entire document, with the results (output, tables, or plots) displayed directly below the corresponding code chunk. Inline Code: R code can also be embedded directly within the text using inline code expressions, which are enclosed in single backticks and start with the letter “r”. The result of the inline code expression is inserted directly into the text when the notebook is rendered. Rich Text Formatting: R Notebooks support Markdown syntax for formatting text, which allows you to create well-structured documents with headers, lists, tables, images, and other useful elements. Output Formats: R Notebooks can be rendered into various output formats, such as HTML, PDF, or Microsoft Word, using the knitr and rmarkdown packages. This makes it easy to share your work with others, even if they don’t use R or RStudio. Reproducibility: R Notebooks enable reproducible research by storing the code, data, and results together in a single document. This makes it easy for others to replicate your analyses and build upon your work. To create a new R Notebook in RStudio, go to the “File” menu, select “New File”, and then choose “R Notebook”. This will open a new .Rmd file with some basic content, which you can edit and customize as needed. 3.2.1 R Notebook/ YML Header When you create an R notebook via RStudio interface, it automatically creates a template, which looks like what is shown on the image below. You can save it and generate it (or knit it)—an HTML file will appear next to it. You can preview it in “Viewer” (bottom right part of the RStudio interface). Important: the files must be named Class_03.Rmd and saved into this folder: ./57528_DH_in_AAS_PUA_R/Classes/Class_03. Default R Notebook Template Let’s modify the YML header in the following manner (but use you name :): --- title: &quot;Class 02: R Markdown and R Notebook&quot; author: &quot;Maxim Romanov&quot; date: &quot;2023-04-12&quot; output: html_document: toc: true toc_depth: 3 theme: &quot;united&quot; bibliography: ../../bibliography.bib csl: ../../chicago-author-date.csl --- Tick [ ] Preview on Save (this option is nice for small documents, but you would want to switch it off if your document is too large and takes some time to generate. For now, keep it on.) Preview menu will change to Knit when we change html_notebook to html_document; in that menu you can choose the format of the document that you want to knit; The Gears menu (next to Preview): make sure to choose “Preview in viewer Pane” — your document then will be automatically shown in the bottom-right section of RStudio. Note: You can customize the YAML header to change the appearance and behavior of your R Notebook, as well as to include additional metadata. The rmarkdown package documentation provides more information on the available options and settings: https://rmarkdown.rstudio.com/lesson-1.html You can also automatically generate a MS Word file, by changing the YML header to (note that we added word_document output parameters): --- title: &quot;Class 02: R Markdown and R Notebook&quot; author: &quot;Maxim Romanov&quot; date: &quot;2023-04-12&quot; output: word_document: toc: true toc_depth: 3 html_document: toc: true toc_depth: 3 theme: &quot;united&quot; bibliography: ../../bibliography.bib csl: ../../chicago-author-date.csl --- 3.2.1.1 Bibliography and References Important! Make sure to create file bibliography.bib and save it to the main folder of your class project. Note the path in our YML header: ../../bibliography.bib. What does it mean? Use the following contents for the bibliography.bib file: @book{ArnoldHumanities2015, title = {Humanities Data in {{R}}}, author = {Arnold, Taylor and Tilton, Lauren}, date = {2015}, publisher = {{Springer Science+Business Media}}, location = {{New York, NY}}, isbn = {978-3-319-20701-8}, langid = {english} } @book{HadleyAdvanced2014, title = {Advanced {{R}}}, author = {Hadley, Wickham}, date = {2014}, publisher = {{CRC Press}}, location = {{London}}, annotation = {OCLC: 904449443}, isbn = {978-1-4665-8697-0}, langid = {english} } @book{HadleyGgplot22016, title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}}, author = {Hadley, Wickham}, date = {2016}, langid = {english}, publisher = {{Springer Science+Business Media, LLC}}, location = {{New York, NY}}, isbn = {978-3-319-24275-0} } @book{HadleyRfoDataScience2017, title = {{R} for Data Science: Import, Tidy, Transform, Visualize, and Model Data}, author = {Hadley, Wickham and Grolemund, Garrett}, date = {2016}, langid = {english}, publisher = {{O’Reilly}}, location = {{Boston}}, isbn = {978-1491910399} } @book{HealyData2018, title = {Data {{Visualization}}: {{A Practical Introduction}}}, shorttitle = {Data {{Visualization}}}, author = {Healy, Kieran}, date = {2018-12-18}, publisher = {{Princeton University Press}}, location = {{Princeton, NJ}}, isbn = {978-0-691-18161-5}, langid = {english}, pagetotal = {272} } As you might have guessed already, bibliography.bib is a file with bibliographical information. You can also probably guess that it now contains three references—to the books that we use as references in our course. *.bib is a BibTeX format for recording machine-actionable bibliographical data: you can probably quite easily understand these records. (You can find a detailed description of this format here: https://en.wikipedia.org/wiki/BibTeX). You can quite easily create such records manually, but in most cases you can actually download them from practically any online library. For example, the two screenshots below show you how you can get bibTeX data from the online catalog of our UHH Library: A record in Katalogplus of the UHH Library An export interface: click on BibTeX to download the record Unfortunately, records do not always look perfect: @book{ Solr-037583980, title = {The classical age of Islam}, series = {Hodgson, Marshall G. S. 1922-1968 The venture of Islam.}, author = {Hodgson, Marshall G. S. 1922-1968}, publisher = {Univ. of Chicago Press}, year = {1974}, } You want to change it into something like the following: @book{HodgsonVentureI1974, title = {The venture of Islam, Vol 1: The classical age of Islam}, author = {Hodgson, Marshall G. S.}, date = {1974}, publisher = {{University of Chicago Press}}, location = {{Chicago}} } The most important part is the KEY (HodgsonVentureI1974) — it must be unique and, ideally, easy to remember so that you can use it in the document. Using bibTeX references is very easy: simply add [@key] to wherever you want the reference to appear. The complete bibliographical record will be also automatically added at the end of the document. For this reason, make sure that your document ends with the following line (empty line, followed by # Bibliography and then followed by another empty line): # Bibliography 3.2.1.1.1 Collecting Bibliographical References: Zotero + BetterBibTeX the best and easiest way to do that is to collect them via Zotero (https://www.zotero.org/), although other bibliography managers should be suitable for this purpose; with Zotero you can “harvest” bibliographical records from book stores (like Amazon), most digital catalogs (like our UHH Library), as well as journal databases/websites (like JSTOR); collected bibliographical data can be then easily exported into BibTeX format using Zotero plugin better BibTeX (https://retorque.re/zotero-better-bibtex/); in this plugin you can also define how your keys should be formatted, and it will take care of ensuring that every key is unique. 3.2.1.1.2 Zotero Zotero is an open-source reference management software that helps researchers, students, and professionals collect, organize, cite, and share research sources, such as journal articles, books, web pages, and multimedia content. Zotero is particularly useful for managing bibliographies and citations, making it easier to maintain an accurate and up-to-date record of sources used in a research project or academic paper. Some key features of Zotero include: Source Collection: Zotero can automatically extract bibliographic information from web pages, online databases, and library catalogs, allowing you to quickly add sources to your Zotero library. It also supports importing reference data from other reference management tools, such as EndNote, Mendeley, or BibTeX. Organizing and Tagging: Zotero provides tools for organizing your research sources into collections and sub-collections, which can be shared with collaborators. You can also use tags and notes to further categorize and annotate your sources. Citation and Bibliography Generation: Zotero integrates with popular word processing software like Microsoft Word, LibreOffice, and Google Docs, enabling you to easily insert citations and generate bibliographies in a wide range of citation styles, such as APA, MLA, Chicago, and many others. Zotero’s citation engine is built on the open-source Citation Style Language (CSL), which supports thousands of citation styles. Collaboration: Zotero allows you to create group libraries, which can be shared among multiple users for collaborative research projects. Group libraries can be set up with different access and editing permissions, making it easy to manage collaboration among team members. File Storage and Synchronization: Zotero provides cloud-based storage for your library data and attached files, such as PDFs or images, allowing you to access your Zotero library from multiple devices and keep everything in sync. Zotero offers 300 MB of free storage and additional storage plans for purchase. Zotero is available as a standalone desktop application for Windows, macOS, and Linux, as well as a browser extension for Firefox, Chrome, and Safari. You can learn more about Zotero and download the software from the official website: https://www.zotero.org/ 3.2.1.1.3 Citation Styles As you may know, Zotero allows one to dynamically change style of your citations and bibliography. This can be done with R markdown as well. We just need to provide a file with the style that you want to use. https://www.zotero.org/styles is the website where you can find the style that you need. That style needs to be downloaded and placed into the folder of your project, and linked to in the same manner as we did with the bibliography.bib file. I have downloaded the following two styles. Please, download them as well. We will experiment with them. Chicago Manual of Style 17th edition (author-date) (2022-12-12 04:02:09) Chicago Manual of Style 17th edition (full note) (2023-03-31 12:44:56) 3.2.2 Basics of Markdown Markdown is a lightweight markup language designed for formatting plain text documents in a human-readable and easily-editable way. Created by John Gruber and Aaron Swartz in 2004, Markdown has become a popular choice for authoring web content, documentation, and even academic papers due to its simplicity and ease of use. The main motto of markdown: “A Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions.” (John Gruber) The primary goal of Markdown is to make writing for the web as natural and intuitive as possible, while still allowing for basic text formatting, such as headings, lists, bold, italics, links, images, and more. Markdown uses simple punctuation and symbols to represent formatting elements, which makes it easy to learn and write without the need for complex tags or HTML code. In markdown each structural element is coded explicitly, but in a very simple manner (if compared to HTML, XML, LaTeX, or any other language that is used for similar purposes). Additionally, explicit structural encoding helps to maintain clean structure of a document. Markdown offers all the features you might possibly need for any kind of writing, including academic writing: Headers of different levels Lists: ordered, unordered, mixed Links Images Block quotes Latex equations Horizontal rules (i.e., horizontal lines to visually break up your content) Tables Footnotes Citations/References (multiple styles) Bibliography Slide breaks Text formatting Italicized text Bold text Superscripts Subscripts Strikethrough text You can download and use this cheatsheet, until you get comfortable with markdown: https://github.com/rstudio/cheatsheets/raw/main/rmarkdown-2.0.pdf RStudio also offers a very nice and detailed presentation of markdown: https://rmarkdown.rstudio.com/lesson-1.html. You can also use this free guide: https://bookdown.org/yihui/rmarkdown/ (can be also downloaded as a PDF). Let’s try the most important elements of markdown—in the file that we created in the beginning of the class. Basics of markdown: simple explicit formatting basic principles: main elements: YML header; headers; paragraphs; text: bold, italics, strikethrough, superscript, subscript, code; lists; tables; images; executable code blocks; executable inline code; footnotes; bibliography; file; include some references; remaining must be added by the students; collecting; generating notebook: HTML and DOCX only; PDF — try on your own; HW: create a notebook for the description of PUA: https://www.eea.csic.es/pua/info/proyecto.php (use English version); references: you can find some references here: https://www.eea.csic.es/pua/info/otroscolaboradores.php 3.3 Homework complete 4 more modules in swirl(): Missing Values; Subsetting Vectors; Matrices and Dataframes; Logic; create a R notebook and recreate the content of this web page in markdown: https://www.eea.csic.es/pua/info/proyecto.php. This will give you some practice with the main elements of markdown syntax and formatting. NB: the document does not include some important elements, so, please, at your own discretion, use the following: add a few footnotes; add a few references (you can use the ones from the bibliography.bib, but also at least two-three new references.) this task took me about 20 mins to finish; it will take longer for you, but it should not be too much longer. In any case, the goal is to make you practice markdown—it is a very efficient system which is now everywhere and you will benefit greatly, if you know how to use it. With just a little bit of practice you can become proficient in markdown. "],["data-manipulation-i.html", "4 Data Manipulation I 4.1 This Chapter 4.2 Why tidyverse? 4.3 Main Data File Formats: CSV and TSV 4.4 Loading Data into R 4.5 Loading PUA data 4.6 Exploring PUA Data 4.7 Main commands for engaging with a tibble 4.8 Important Functions from dplyr 4.9 Important Functions from tidyr 4.10 Function usage 4.11 Homework Tasks 4.12 Appendix: “Window” function in tidyverse", " 4 Data Manipulation I 4.1 This Chapter last class focused mainly on formatting, this class will add code chunks into our R markdown documents; we will learn and practice working with tabular data (simply put, tables): main relevant data format: TSV / CSV; loading data; manipulating data; generating and extracting observations from data; we will start analyzing some data from the PUA database (“Prosopografía de ulemas de al-Andalus”, https://www.eea.csic.es/pua/); you will need the following files. Make sure to put it in the correct folder as shown below (create subfolders if they are missing): Data/PUA_Processed/PUA_allDataTables_asList.rds (PUA_allDataTables_asList.rds):: this file contains all the data from the original relational database (MySQL) of the PUA project. Like any relational database, this one consists of a series of interconnected tables, which are all described in the appendix to this lesson. Data/Pua_Processed/PUA_personaje.tsv (PUA_personaje.tsv) :: this is the main table from the PUA database; in fact, this one is included in the RDS file given above; we will use this file only for practicing loading data into RStudio from TSV, since this is one of the most common operations for loading data. questions: what format is this data stored in and how can we load it in R? what is the internal structure of the data? 4.2 Why tidyverse? The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. Here we find a set of functions to perform common data manipulation tasks on data frames, making the process more efficient, readable, and expressive. Although tidyverse is not specifically designed for historians, it can be highly relevant and useful for them when working with structured data. (Hadley Wickham is one of the key creators and developers of tydyverse.) Data organization: we often work with large datasets containing various types of information, such as dates, names, events, and locations. The tidyverse package allows us to organize and clean these datasets efficiently. Data filtering: we may need to focus on specific subsets of data, such as filtering by time periods, regions, or other relevant criteria. The tidyverse package provides functions like filter() and slice() that make it easy to filter and subset data. Data transformation: we often need to create new variables or modify existing ones. The mutate() function allows users to create new columns based on existing data, while transmute() can be used to create a new data frame with only the newly created variables. Data aggregation: Analyzing data by grouping it based on certain variables, such as time periods, regions, or categories, is a common task for us as historians. The tidyverse package provides the group_by() function, which makes it easy to group data, and functions like summarize() and tally() to perform summary operations on the grouped data. Data merging: we often need to combine data from multiple sources or datasets. The tidyverse provides functions like left_join(), right_join(), inner_join(), and full_join() that make it easy to merge data frames based on common variables. Data reshaping: we may need to reshape our data for different types of analysis, such as converting data from wide format to long format or vice versa. The tidyverse facilitates data reshaping. Readability: tidyverse uses a syntax that is easy to read and understand, even for those who are not familiar with R. This is particularly helpful for humanists who may not have extensive programming experience but still need to work with data. To sum up, we can efficiently manipulate, analyze, and visualize our data, which enables us to gain insights and draw conclusions from historical records more effectively. The tidyverse includes several popular packages for data manipulation, visualization, and modeling, such as: readr: For reading and writing data, particularly CSV and TSV files. tibble: For working with modern, enhanced data frames called tibbles. dplyr: For data manipulation, including filtering, selecting, and transforming data. tidyr: For cleaning and reshaping data, making it easier to work with. stringr: For working with strings, providing functions to manipulate, extract, and search text. ggplot2: For data visualization, creating powerful and flexible graphics. purrr: For functional programming and iteration, making it easier to work with lists and apply functions to data. lubridate: For working with dates and times, making it easier to parse, manipulate, and perform arithmetic with date-time objects. Together, these packages provide a comprehensive set of tools for data science workflows. Last but not least, as the tidyverse approach has gained significant popularity in recent years, numerous additional libraries adhering to the tidyverse principles for working with data have emerged. This positions the tidyverse as the central hub for everything related to R. 4.3 Main Data File Formats: CSV and TSV First, let’s talk about data formats just a little bit. We are primarily focusing on tabular data, which essentially means tables—like what you might create in MS Excel, Google Spreadsheets, of some other program like that. These programs, however, usually rely on their own proprietary formats. Meanwhile, most analytical programs and programming languages “prefer” plain text formats like CSV or TSV, which are essentially two flavors of the same format. CSV (Comma-Separated Values) and TSV (Tab-Separated Values) are both simple and widely-used file formats for storing and exchanging tabular data. They are plain-text formats that represent data in a table, with rows and columns. The primary difference between the two formats lies in the delimiter used to separate the values in each row. CSV Format: In a CSV file, values are separated by commas (,). Each row in the file represents a record (an observation or an instance), and each column represents a variable (a feature or an attribute). The first row often contains column headers, indicating the names of the variables. Since CSV files are plain text, they can be opened and edited using any text editor or spreadsheet program, such as Microsoft Excel, Google Sheets, or LibreOffice Calc. Example of a CSV file: Name,Age,Occupation Alice,30,Engineer Bob,25,Data Scientist Charlie,35,Teacher TSV Format: In a TSV file, values are separated by tabs (\\t). Similar to CSV files, each row represents a record, and each column represents a variable. TSV files also often have a header row with column names. TSV files can be opened and edited using text editors and spreadsheet programs, just like CSV files. Example of a TSV file (\\t is actually an invisible character; here is it made visible for demonstration purposes; \\t is a regular expression for a TAB character): Name\\tAge\\tOccupation Alice\\t30\\tEngineer Bob\\t25\\tData Scientist Charlie\\t35\\tTeacher Both CSV and TSV formats are popular because they are easy to read and write, both for humans and for computer programs. They can be used to store and share various types of data, such as numerical, categorical, or textual data. However, when working with data that contains commas or tabs within the values themselves, it is essential to use appropriate quoting or escaping methods to avoid confusion between the delimiters and the actual data values. TSV has an advantage over CSV in a way that its values can contain commas without complicating the format. \\t characters are highly unlikely to appear in values of your data (if they do, you need to clean your data!). Note: technically, CSV can accommodate commas in values: the value of each cell has to be surrounded by quotation marks—example below—but even with this, some applications are likely to read the format incorrectly. Example of CSV with commas in values: &quot;Name&quot;,&quot;Age&quot;,&quot;Occupation&quot; &quot;Parker, Alice&quot;,&quot;30&quot;,&quot;Engineer&quot; &quot;Marley, Bob&quot;,&quot;25&quot;,&quot;Data Scientist&quot; &quot;Sheen, Charlie&quot;,&quot;35&quot;,&quot;Teacher&quot; Neither TSV nor CSV can support multi-line values (since they encode the entire row as a single line). Luckily, there is an easy workaround that helps to deal with this issue: one can replace \\n (new line characters) with some arbitrary combination of characters that is unlikely to occur in your data (for example, %%%%% or #####), which will convert multi-line values into single-line values; one can then easily replace the combination of characters back with \\n when values need to be displayed as multi-liners. 4.4 Loading Data into R 4.4.1 Loading CSV and TSV files There are several functions and additional libraries that can help us load data into R, but we will focus on readr library, since it is a part of tidyverse. If you load some data for the first time, it is best to use Import Dataset wizard (Tab “Environment” in the top-right section of RStudio) &gt; from the drop-down menu, select “From Text (readr)…”. Below is the image of the open wizard, where I already selected the file that I want to load: This wizard is extremely helpful, since you can adjust a number of parameters and see right away if your data is read correctly. You have plenty of options that you can tweak and they will be reflected in “Code Preview” (bottom-right section). “Code Preview” is perhaps the most convenient feature—you can simply copy-paste that code into your script and then reuse later. In our case the code will look like (I have slightly changed the file name): library(tidyverse) # technically, library(readr) PUA_personaje &lt;- read_delim(&quot;Data/Pua_Processed/PUA_personaje.tsv&quot;, delim = &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) the function takes a relative path to the file (remember, we are working with our project, so the working directory is the main folder of our project); it specifies that the delimiter is \\t; escape characters (\") are not used; white spaces are automatically trimmed. This is a very useful feature, since there is always a chance that some unnecessary white space character will creep into your data. “White spaces” are, for example, empty lines at the end of the file, and unnecessary spaces at the beginning and/or at the end of each value. last but not least, readr does a quite decent job guessing the type of values for each column; but you can also manually change the type for each column and that will be reflected in the code preview. If, by any chance, you got your data into R as a data.frame, you can easily convert it to tibble with as_tibble() function (do not forget to save it into a variable!). You can save any data frame / tibble into a file using another function from the readr library in the following manner: write_delim(PUA_personaje, &quot;Data/Pua_Processed/PUA_personaje_saved.tsv&quot;, delim = &quot;\\t&quot;) The saved file will be slightly different from the original (we have not done any modification to the data)—empty cells will have values NA. The use of NA (not available) is one of the highly recommended practices when you work with data: when a cell is empty it is not entirely clear whether the data is missing (i.e., it was not added yet) or not available (someone already tried to add the value, but it was not possible to find it). When NA is used explicitly, we can safely treat it as not available. 4.4.2 R-specific Data “Format”: RDS Note. When you need to save data generated in R into a file, you will have to use different functions for saving, depending on the structure of of your generated data. This may complicate things significantly. If cases when you are not planning to use this data outside of R, the easiest way is to save your data into an *.RDS file—any data structure can be saved easily into this format and loaded back from it. It may so happen that your data comes in *.RDS files. Standing for “R Data Single” (or “R Data Storage”, or “R Data Structure”), RDS is a binary file format used by R for storing R objects, such as data frames, vectors, matrices, and lists (in fact, one can save a number of R objects into a single RSD file). RDS files are compact, efficient, and platform-independent, which makes them suitable for storing large datasets and sharing data between different R environments. There are two primary functions in R for working with RSD files: saveRDS(): This function is used to save a single R object to an RSD file. # Save an R object to an RSD file saveRDS(object, &quot;your_file.rds&quot;) Replace object with the name of the R object you want to save, and \"file_name.rds\" with the desired file name. readRDS(): This function is used to read an R object from an RSD file. # Read an R object from an RSD file object &lt;- readRDS(&quot;your_file.rds&quot;) Replace \"your_file.rds\" with the name of the RSD file you want to read, and object with the name you want to assign to the loaded R object. RDS files are also extremely convenient in cases when you need to generate intermediate/temporary results as a part of your data analysis routine. 4.5 Loading PUA data PUA &lt;- readRDS(&quot;Data/PUA_processed/PUA_allDataTables_asList.rds&quot;) 4.6 Exploring PUA Data PUA data is quite complex, consisting of 25 interconnected tables. We have two options: load 25 tables, one after another; or, save all tables into a single object—list—which we save as an RDS file, which can be loaded with a single command. summary(). We can get a general summary of any R object with the command summary(). Thus, if we run summary(PUA), we will get a summary of the PUA object, which should look like the following: &gt; summary(PUA) Length Class Mode actividad 5 spec_tbl_df list autor 7 spec_tbl_df list bibliografia 6 spec_tbl_df list caracteristica 5 spec_tbl_df list cargo 6 spec_tbl_df list disciplina 5 spec_tbl_df list familia 13 spec_tbl_df list fuente 7 spec_tbl_df list keywords 2 spec_tbl_df list lugar 8 spec_tbl_df list nisba 6 spec_tbl_df list personaje 19 spec_tbl_df list personaje_actividad 4 spec_tbl_df list personaje_alias 4 spec_tbl_df list personaje_caracteristica 3 spec_tbl_df list personaje_cargo 6 spec_tbl_df list personaje_disciplina 4 spec_tbl_df list personaje_fuente 4 spec_tbl_df list personaje_lugar 6 spec_tbl_df list personaje_nisba 4 spec_tbl_df list personaje_obra 5 spec_tbl_df list personaje_referencia 5 spec_tbl_df list personaje_relacion 4 spec_tbl_df list tiporelacion 2 spec_tbl_df list tiporelacionlugar 2 spec_tbl_df list We can see the names of all the tables (spec_tbl_df), and the Length column tells us how many columns each table has. (You can also get a similar summary in the “Environment” tab in the top-right corner—for the variable PUA). $. We can access each and every table with the $ operator. For example, we can access personaje table with PUA$personaje. We can also combine that with summary() command to get the summary for that specific table, where the data from each column is summarized: &gt; summary(PUA$personaje) idPersonaje idFamilia nombreA Min. : 1 Min. : 0.00 Length:12813 1st Qu.: 3219 1st Qu.: 0.00 Class :character Median : 6436 Median : 0.00 Mode :character Mean : 6440 Mean : 85.16 3rd Qu.: 9665 3rd Qu.: 0.00 Max. :12896 Max. :829.00 NA&#39;s :115 nombreE suhra nacimiento Length:12813 Length:12813 Min. : 0.00 Class :character Class :character 1st Qu.: 0.00 Mode :character Mode :character Median : 0.00 Mean : 71.41 3rd Qu.: 0.00 Max. :806.00 NA&#39;s :115 nacimiento_comentario muerte muerte_comentario Length:12813 Min. : 0.0 Length:12813 Class :character 1st Qu.: 0.0 Class :character Mode :character Median :312.0 Mode :character Mean :275.9 3rd Qu.:533.0 Max. :921.0 NA&#39;s :115 edad edad_comentario resumenBiografico Min. : 0.00 Length:12813 Length:12813 1st Qu.: 0.00 Class :character Class :character Median : 0.00 Mode :character Mode :character Mean : 11.16 3rd Qu.: 0.00 Max. :118.00 NA&#39;s :115 creado Min. :2010-12-29 01:12:44.00 1st Qu.:2010-12-29 01:13:21.00 Median :2010-12-29 01:14:23.00 Mean :2011-03-26 14:31:49.92 3rd Qu.:2010-12-29 01:15:49.00 Max. :2019-08-02 11:34:43.00 NA&#39;s :328 modificado publicado Min. :2010-12-29 01:12:44.00 Min. :0.0000 1st Qu.:2010-12-29 01:16:40.00 1st Qu.:1.0000 Median :2013-12-02 11:10:37.50 Median :1.0000 Mean :2014-05-29 14:09:49.69 Mean :0.9867 3rd Qu.:2016-10-06 09:28:28.75 3rd Qu.:1.0000 Max. :2019-09-02 19:40:39.00 Max. :1.0000 NA&#39;s :115 maestrosOrientales nacimientoec muerteec Length:12813 Min. : 0.0 Min. : 0.0 Class :character 1st Qu.: 0.0 1st Qu.: 0.0 Mode :character Median : 0.0 Median : 871.0 Mean : 161.7 Mean : 571.2 3rd Qu.: 0.0 3rd Qu.:1126.0 Max. :1403.0 Max. :1515.0 NA&#39;s :115 NA&#39;s :210 fuentesCitadas Length:12813 Class :character Mode :character As you can see, this function provides, in the case of character data: 1) the number of of values in each column; 2) their type; and 3) their mode. In the context of our work, there is practically no difference between type and mode, as you can see. In the case of numeric data, you can see that a general statistical summary (for example, in nacimiento): minimal value (Min.), quartiles (1st Qu., Median, 3rd Qu.), average mean (Mean), and maximum value (Max.). In the case of numeric data, summary() gives you insight into the general distribution of your data. View() Another way to look at the loaded data is to use View(). Thus, View(PUA) will give us the following summary view: In a similar manner, we can view a specific tibble inside the PUA value. Thus, View(PUA$personaje) will give us the following view: Usually, you can also open this table view by simply clicking on the name of the loaded tibble in “Environment” in top-right section; in our current case, however, we have our tibbles inside a list (but you can always assign a specific tibble from the list to a variable). This view can be searched/filtered and you can rearrange the entire table by any of the columns. This is not necessarily something that you will be using a lot, but sometimes it does come in handy. Additionally, you can click on a blue circle with a white triangle button next to the name of your tibble and you will get another convenient summary, like shown on the image below: if you simply type the name of your tibble in the console and hit enter, you will get a shortened version of the tibble. Here you can use commands like head() and tail()—the first one will show the first 6 rows of data, while the last one —the last 6 rows of data. tibble$column. You can use $ to look at the contents of a specific column. If you type the name of the tibble and add $ right after, you will see a pop-up window which will suggest you the names of available columns. This is the easiest way to refer to a specific column. For example, PUA$personaje$nombreA will print out the nombreA column, in the tibble personaje of our main loaded list PUA. unique() is your standard way to get only unique values for a vector/column. Thus, for example, we can combine $ and unique to get only unique values fro many column. Using length() will give us the count. PUA$personaje$nombreA will print out the nombreA column, in the tibble personaje of our main loaded list PUA; length(PUA$personaje$nombreA) will tell us how many individuals we have altogether. Try it. (Should be 12,813); length(unique(PUA$personaje$idFamilia)) will give us the number of unique families in the dataset. Try it. (Should be 815; technically); any operation that can be applied to a vector, can be applied to a column. 4.7 Main commands for engaging with a tibble 4.7.1 Chaining with %&gt;% There are two main ways to manipulate tibbles in R: you create a new state of a tibble and you save it into a new/same variable, and then keep repeating this type of step creating new modifications of your data; alternatively, you can “chain” multiple modifications into a single readable expression. In tidyverse, for chaining we use a pipe operator that looks like this %&gt;%. As an example, let’s get a count of how many people we have for the 6th AH century, whose age was between 70 and 79 (septuagenarians). In order to get this number, we will need to go through a sequence of the following steps: we filter all individuals by their death dates, which must fall between 501 and 600 (in other words, their death dates must be within this range); the main column for this is muerte; we then filter out those whose age is below 70 and those, whose age is above 79 (column edad) ; we then count the number of resulting rows; And now, let’s take a look how these operations can be done in these two different ways: without chaining: septuagenarians &lt;- PUA$personaje septuagenarians &lt;- filter(septuagenarians, muerte &gt;= 501, muerte &lt;= 600) septuagenarians &lt;- filter(septuagenarians, edad &gt;= 70, edad &lt;= 79) septuagenarians &lt;- nrow(septuagenarians) septuagenarians with chaining: septuagenarians &lt;- PUA$personaje %&gt;% filter(muerte &gt;= 501, muerte &lt;= 600) %&gt;% filter(edad &gt;= 70, edad &lt;= 79) %&gt;% nrow() septuagenarians The method with chaining gives us shorter expressions, and, in general, it is much easier and more readable. It makes complex operations really simple and easy. That said, occasionally, you would need to save your intermediate results into new variables. It is often best to keep original data in the initial variable and save any modifications that you want to work with (i.e., you want to use them multiple times) into new variables. This way you will be able to easily get the original data without reloading it. The %&gt;% (pipe operator) from the magrittr package is an essential tool when working with dplyr functions. It allows you to chain multiple dplyr functions together, making your code more readable and easier to understand. 4.8 Important Functions from dplyr dplyr is a key package within the Tidyverse ecosystem that provides a set of tools for data manipulation. Here is an overview of some of the most important dplyr functions: filter(): Filters rows based on specified conditions. This function helps you to extract a subset of rows from a dataset that meet certain criteria. select(): Selects specific columns from a dataset. This function is useful when you want to work with a subset of columns, either by specifying their names or using helper functions like starts_with(), ends_with(), and contains(). mutate(): Creates new columns or modifies existing ones based on expressions involving existing columns. This function is often used to perform calculations or transformations on the data. (transmute() is similar to mutate(), but it only keeps the newly created or modified columns in the output, discarding the original columns.) arrange(): Orders the rows in a dataset based on one or more columns. By default, it sorts the data in ascending order, but you can use the desc() function to sort in descending order. group_by(): Groups data by one or more columns, typically used in combination with aggregation or window functions to perform grouped calculations. summarise() or summarize(): Calculates summary statistics for each group after using group_by(). Common summary statistics include mean, median, sum, count, minimum, and maximum. rename(): Renames columns in a dataset, making it easier to work with datasets with non-descriptive or unclear column names. join_* functions: A family of functions for combining datasets based on common columns. Some common join functions are inner_join(), left_join(), right_join(), and full_join(). Window functions: These functions allow you to perform calculations across a set of rows related to the current row, often used with time series data or data that has a natural ordering. Examples include lag(), lead(), cumsum(), cumprod(), cummin(), cummax(), and various ranking functions like row_number(), dense_rank(), and min_rank(). Window functions are particularly useful for time series analysis, when we need to calculate how some variable changes from one period to the next. By mastering these key dplyr functions, you can perform a wide range of data manipulation tasks effectively and efficiently in R using the tidyverse. 4.9 Important Functions from tidyr tidyr is a another package within the tidyverse ecosystem that focuses on cleaning and reshaping data. Here is an overview of some of the most important tidyr functions: pivot_longer(): Converts data from wide to long format, gathering multiple columns into a single column. This function is useful when you have multiple columns representing different categories or time periods, and you want to bring them together into a single column. pivot_wider(): Converts data from long to wide format, spreading a single column into multiple columns. This function is useful when you have data in long format with one row per observation and you want to spread it across multiple columns. separate(): Splits a single column into multiple columns based on a specified delimiter or pattern. This function is helpful when you have a single column containing multiple pieces of information that need to be separated into distinct columns. unite(): Combines multiple columns into a single column, often with a specified delimiter. This function is useful when you need to merge information from multiple columns into a single column. drop_na(): Removes rows with missing values (NA) in one or more specified columns. This function helps you clean your data by removing incomplete records. replace_na(): Replaces missing values (NA) with a specified value or values. This function can be used to fill in missing data with a default value or an estimate. fill(): Fills missing values (NA) in a column with the last non-missing value encountered, either forward or backward. This function is particularly useful when working with time series data or data with a natural ordering. nest(): Creates a nested data frame by collapsing multiple columns into a single column containing a list of data frames. unnest(): Reverses the operation of nest(), expanding a list column of data frames into multiple columns. 4.10 Function usage The list above gives you an idea of what kind of operations you can do with them. In what follows you will learn how to use most of these functions through practical examples (some functions are needed only for rather complicated cases, which we will not be covering here). 4.10.1 filter() This function filters rows based on specified conditions. This function helps you to extract a subset of rows from a dataset that meet certain criteria. In this example, we will get individuals from the 6th AH century: cent_600AH &lt;- PUA$personaje %&gt;% filter(muerte &gt;= 501, muerte &lt;= 600) Note: it is important to note that names of your variables cannot start with digits; for example, 600AH will not be possible—that is why I opted for cent_600AH. In this example, we will get septuagenarians from the 6th century AH: septuagenarians600AH &lt;- PUA$personaje %&gt;% filter(muerte &gt;= 501, muerte &lt;= 600) %&gt;% filter(edad &gt;= 70, edad &lt;= 79) Note: we could have also used the variable cent_600AH, in which case we would have needed only the last filter operation. 4.10.2 select() This function selects specific columns from a dataset. This function is useful when you want to work with a subset of columns, either by specifying their names or using helper functions like starts_with(), ends_with(), and contains(). Most commonly you will use this function to make a copy of your data with a smaller number of columns, dropping those that you do not need for analysis. In the example below, we will create a light version of our personaje tibble, keeping only the most important columns: personajeLite &lt;- PUA$personaje %&gt;% select(idPersonaje, idFamilia, nombreA, nacimiento, muerte, edad) You can drop specific columns by adding a - (minus/dash) or ! (exclamation mark) in front of the name of that columns. Let’s drop edad from our light data, but, also, let’s not save the results: personajeLite %&gt;% select(-edad) In some cases you may want to try something out without saving the results of your manipulations in a variable. You can do it in the manner shown above. Note, we are not saving our results (there is no &lt;- assignment operator, and no new variable). If you run this code, R will simply print out the results in console—the same tibble, but without column edad. Additionally, you can use starts_with(), ends_with(), and contains() if you want to quickly select columns that have the same element in their names—either in the beginning, in the end, or anywhere. For example, we have columns whose names begin with id. We can use starts_with(\"id\"). You can use - or ! in front of these functions to drop those columns instead. For example, we can drop “comment” columns, whose names end with _comentario. personaje_IDs &lt;- PUA$personaje %&gt;% select(starts_with(&quot;id&quot;)) personaje_noComments &lt;- PUA$personaje %&gt;% select(!ends_with(&quot;_comentario&quot;)) 4.10.3 mutate() and transmute() The mutate() function creates new columns or modifies existing ones based on expressions involving existing columns. This function is often used to perform calculations or transformations on the data. The transmute() function is similar to mutate(), but it only keeps the newly created or modified columns in the output, discarding the original columns; transmute() would be similar to using mutate() to create a new column and then using select() to drop all the old columns. Let’s take our personajeLite and fix some information there. There are too many zeroes in columns nacimiento, muerto, and edad, which are clearly used as fillers (i.e., they do not actually mean 0). To ensure that our analyses are done correctly, we need to replace them with NA, which would mean that data is not available and it should not be used in calculations. Note on the importance of NA: Let’s dwell a little bit on why we need to have NAs where appropriate. Say, we want to calculate the average age. In a vector ages1 we have 0 instead of NA, so if we try to calculate average mean, zeroes will be considered as numbers and that will give us the average age of 25.2. Meanwhile, when we have NA, as in vector ages2, we can exclude them from the calculation of the average mean and get the value of 75.67, which would be much more appropriate for the existing ages of 67, 70, and 90. ages1 &lt;- c(0,0,0,0,0,0,67,70,90) ages2 &lt;- c(NA,NA,NA,NA,NA,NA,67,70,90) mean(ages1) # 25.22222 mean(ages2, na.rm = TRUE) # 75.66667 Let’s update our personajeLite accordingly. There are two ways we can write it out—in a more explicit manner, or a more packed manner. # more explicit personajeLite &lt;- PUA$personaje %&gt;% select(idPersonaje, idFamilia, nombreA, nacimiento, muerte, edad) %&gt;% mutate(nacimiento = na_if(nacimiento, 0)) %&gt;% mutate(muerte = na_if(muerte, 0)) %&gt;% mutate(edad = na_if(edad, 0)) # more packed personajeLite &lt;- PUA$personaje %&gt;% select(idPersonaje, idFamilia, nombreA, nacimiento, muerte, edad) %&gt;% mutate(nacimiento = na_if(nacimiento, 0), muerte = na_if(muerte, 0), edad = na_if(edad, 0)) We can use mutate() to reproduce some of the columns that we dropped in our light version. For example, we have dropped the dates in common era. We can recalculate them, using the function that you used in the previous lesson. First, here is our conversion function: AH2CE &lt;- function(AH){ CE &lt;- round(AH - AH/33 + 622) AH &lt;- ifelse(AH == 0, 1, AH) final &lt;- paste0(AH, &quot;AH/&quot;, CE, &quot;CE&quot;) return(final) } Now, let’s update our personajeLite. Note that I have added ifelse() function in order to keep NA: personajeLite &lt;- personajeLite %&gt;% mutate(muerteCE = ifelse(is.na(muerte), NA, AH2CE(muerte))) 4.10.4 rename() This function simply renames columns in a dataset, making it easier to work with datasets with non-descriptive or unclear column names. For example, we may want to have English names for the columns: personajeLiteEN &lt;- personajeLite %&gt;% rename(idPerson = idPersonaje, idFamily = idFamilia, nameA = nombreA, born = nacimiento, died = muerte, diedCE = muerteCE, age = edad) 4.10.5 arrange() This function orders the rows in a dataset based on one or more columns. By default, it sorts the data in ascending order, but you can use the desc() function to sort in descending order. When you use multiple columns as arguments (connected with commas), the data will be sorted by the first mentioned column, and then, inside, by the second column, and so on. Thus, the following code will arrange the data first by idFamilia, and then, within each family (just keep in mind that 0 means no family, so we best filter them out), individuals will be arranges from the oldest to the youngest. Run the code and take a look for yourself. oldestInFamilies &lt;- personajeLite %&gt;% filter(idFamilia != 0) %&gt;% filter(!is.na(edad)) %&gt;% arrange(idFamilia, desc(edad)) The results should look something like this: 4.10.6 group_by() / ungroup() With group_by() we are switching to really cool functions that will allow us to do lots of interesting thing with our dataset. The group_by() function groups data by one or more columns. Its results are not quite visible. Take a look at the screenshot below of the same tibble: in the first case (top) it is the original tibble, in the second case (bottom) the data is grouped by idFamilia (note the underlined # Groups: idFamilia [815]): Usually, when you are done working with groups and you do not need them anymore, you would use function ungroup() to remove them. Typically, group_by() is used in combination with aggregation or window functions to perform grouped calculations or manipulations. We will skip on windows functions for now, but we will look into some most useful aggregation functions. In fact, the results of group_by() are not very useful until you apply some aggregation function. 4.10.7 summarise() or summarize() The most common function that is used right after group_by() is summarize() or summarise(), if you prefer British spelling :). This function calculates summary statistics for each group after using group_by(). Common summary statistics include mean(), median(), sum(), n(), min(), and max(). You can also select top items from each group withtop_n(). Let’s give it a try. For example, we can count how many members each family has: miembrosDeFamilias &lt;- personajeLite %&gt;% filter(idFamilia != 0) %&gt;% group_by(idFamilia) %&gt;% summarize(miembros = n()) %&gt;% arrange(desc(miembros)) And if we print, we can see the most prominent families, in terms of the numbers of their members who made it into historical records of al-Andalus: &gt; familias # A tibble: 813 × 2 idFamilia miembros &lt;dbl&gt; &lt;int&gt; 1 89 23 2 255 23 3 459 20 4 20 18 5 44 18 6 21 16 7 700 16 8 2 15 9 9 15 10 17 15 # ℹ 803 more rows # ℹ Use `print(n = ...)` to see more rows In a similar manner we can calculate the average age of members of each family: familiasEdadPromedio &lt;- personajeLite %&gt;% filter(idFamilia != 0) %&gt;% group_by(idFamilia) %&gt;% summarize(edadPromedio = mean(edad, na.rm = TRUE), miembros = n()) %&gt;% arrange(desc(edadPromedio)) Some results from the top (note, that we now included two operations into the summarize() function; it is important to keep in mind that all summarization steps must be put into the same summarization function!): &gt; familiasEdadPromedio # A tibble: 813 × 3 idFamilia edadPromedio miembros &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 724 100 2 2 290 99 2 3 224 98 4 4 207 95 3 5 453 95 2 6 551 95 2 7 561 95 6 8 158 94 2 9 484 94 2 10 77 93 2 # ℹ 803 more rows # ℹ Use `print(n = ...)` to see more rows We an also select the longest living member from each family (top_n(1, wt = edad)—1 can be replaced with the desired number). familiasMasAntiguo &lt;- personajeLite %&gt;% filter(idFamilia != 0) %&gt;% group_by(idFamilia) %&gt;% top_n(1, wt = edad) %&gt;% arrange(desc(edad)) %&gt;% select(-nombreA) Results (I have dropped the column with the name, so that the results are more readable here): &gt; familiasMasAntiguo # A tibble: 502 × 6 # Groups: idFamilia [487] idPersonaje idFamilia nacimiento muerte edad muerteCE &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 2455 44 190 295 118 295AH/908CE 2 1307 277 NA 577 100 577AH/1182CE 3 9407 724 514 614 100 614AH/1217CE 4 5540 290 603 702 99 702AH/1303CE 5 4257 449 447 545 98 545AH/1150CE 6 6705 224 483 581 98 581AH/1185CE 7 6070 212 201 297 96 297AH/910CE 8 9875 605 272 368 96 368AH/979CE 9 10700 537 299 394 96 394AH/1004CE 10 2420 453 326 421 95 421AH/1030CE # ℹ 492 more rows # ℹ Use `print(n = ...)` to see more rows 4.10.8 join_* functions You might have noticed already that working with numeric IDs can be rather unhelpful: in the examples above we could only see the numeric IDs of families, but we also had no idea what those families actually are. The family of join_* functions is very helpful here, since they allow us to join additional information from other tables. This is a family of functions for combining datasets based on common columns. Some common join functions are inner_join(), left_join(), right_join(), and full_join(). These are incredibly useful functions since they allow you to take advantage of the functionality which is available in relational databases or through the linked-open-data (LOD) approach. The main principle of rDB (and LOD, at least to a certain extent) is as follows: if you duplicate some data in your table, you should move it to a separate one. Such division into separate tables has lots of advantages that contribute to efficient data management and retrieval. For example, in our personajeLite data we have many members of the same families. In order not to repeat all the relevant information on each family in the personaje table, this information is moved to a separate table. Such an approach makes it much easier to manage, update, and expand your data. The data on families is stored in PUA$familia. Both tables are interconnected by the field idFamilia. We can use one of the join_* functions to connect both tables. First, let’s get a light version of family data — essentially, just the English name of each family (idFamilia is the key column and must always be preserved). familiaDataLite &lt;- PUA$familia %&gt;% select(idFamilia, nombreE) Now, we can take all the results that we generated above and have much more readable results: Families with the highest numbers of members: miembrosDeFamilias &lt;- miembrosDeFamilias %&gt;% left_join(familiaDataLite, by = c(&quot;idFamilia&quot; = &quot;idFamilia&quot;)) &gt; miembrosDeFamilias # A tibble: 813 × 3 idFamilia miembros nombreE &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; 1 89 23 Banū l-Bağī 1 2 255 23 Banū Saʿīd al-ʿAnsī 3 459 20 Banū Ḥağğağ al-Laḫmī 4 20 18 Banū Abī Ğamra 5 44 18 Banū ʿAmīra 6 21 16 Banū Ḫalīl al-Sakūnī 7 700 16 Banū Hāniʾ 8 2 15 Banū Dīnār 9 9 15 Banū Qāsim b. Hilāl 10 17 15 Banū Wāğib # ℹ 803 more rows # ℹ Use `print(n = ...)` to see more rows Families with the highest average age. familiasEdadPromedio &lt;- familiasEdadPromedio %&gt;% left_join(familiaDataLite, by = c(&quot;idFamilia&quot; = &quot;idFamilia&quot;)) &gt; familiasEdadPromedio # A tibble: 813 × 4 idFamilia edadPromedio miembros nombreE &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; 1 724 100 2 Banū Saʿāda al-Nafzī 2 290 99 2 Banū Hārūn 2 3 224 98 4 Banū l-Ḥallāʾ / Banū Ṯaʿbān 4 207 95 3 Banū l-Gaššāʾ 5 453 95 2 Banū l-Bağğānī 6 551 95 2 Banū Ğarīr 7 561 95 6 Banū Sulaymān al-Gāfiqī 8 158 94 2 Banū Kawzān 9 484 94 2 Banū l-Šabulārī 10 77 93 2 Banū Ḥunayn # ℹ 803 more rows # ℹ Use `print(n = ...)` to see more rows Families with the longest living members: familiasMasAntiguo &lt;- familiasMasAntiguo %&gt;% left_join(familiaDataLite, by = c(&quot;idFamilia&quot; = &quot;idFamilia&quot;)) %&gt;% select(-nacimiento, -muerte) &gt; familiasMasAntiguo # A tibble: 502 × 5 # Groups: idFamilia [487] idPersonaje idFamilia edad muerteCE nombreE &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 2455 44 118 295AH/908CE Banū ʿAmīra 2 1307 277 100 577AH/1182CE Banū ʿAmīra 3 9407 724 100 614AH/1217CE Banū Saʿāda al-Nafzī 4 5540 290 99 702AH/1303CE Banū Hārūn 2 5 4257 449 98 545AH/1150CE Banū Abī l-Rağāʾ 6 6705 224 98 581AH/1185CE Banū l-Ḥallāʾ / Banū Ṯaʿbān 7 6070 212 96 297AH/910CE Banū Našr 8 9875 605 96 368AH/979CE Banū Fahd 9 10700 537 96 394AH/1004CE Banū Barṭāl 10 2420 453 95 421AH/1030CE Banū l-Bağğānī # ℹ 492 more rows # ℹ Use `print(n = ...)` to see more rows If the key columns have the same name in both tables, the by = c(\"colNameA\" = \"colNameB\") can be omitted: the function can figure it on its own. Personally, I prefer to be explicit in my code and strongly recommend that you stick to this practice as well, at least in the beginning. This will help you to avoid possible issues when R runs off doing something irrelevant—and that may happen. 4.10.9 pivot_*() functions There are two pivot functions, pivot_long() and pivot_wide(), which are extremely helpful when your data comes in some sub-optimal format, or when sometimes you need to transform your data to be used with some other tool that require a different format. Let’s first calculate how many individuals we have for each place and each century. We will then pick the top 10 places with the highest number of individuals. # get readable names of places lugarNombres &lt;- PUA$lugar %&gt;% select(idLugar, nombre_castellano) # count people in places lugar &lt;- PUA$personaje_lugar %&gt;% select(idLugar, idPersonaje, idRelacion) %&gt;% left_join(personajeLite, by = c(&quot;idPersonaje&quot; = &quot;idPersonaje&quot;)) %&gt;% select(-nombreA, -nacimiento, -edad, -muerteCE) %&gt;% mutate(century = plyr::round_any(muerte, 100, f = ceiling)) %&gt;% filter(!is.na(century)) # sleect the top 10 paces lugarTop10 &lt;- lugar %&gt;% group_by(idLugar) %&gt;% summarize(total = n()) %&gt;% arrange(desc(total)) %&gt;% top_n(10, wt = total) # creating the summary lugarSummary &lt;- lugar %&gt;% group_by(idLugar, century) %&gt;% summarize(individuals = n()) %&gt;% filter(idLugar %in% lugarTop10$idLugar) %&gt;% left_join(lugarNombres) Ok, our summary looks like this: &gt; lugarSummary # A tibble: 78 × 4 # Groups: idLugar [10] idLugar century individuals nombre_castellano &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; 1 24 300 1 Almería 2 24 400 6 Almería 3 24 500 116 Almería 4 24 600 194 Almería 5 24 700 52 Almería 6 24 800 60 Almería 7 24 900 1 Almería 8 4 100 2 Córdoba 9 4 200 33 Córdoba 10 4 300 220 Córdoba # ℹ 68 more rows # ℹ Use `print(n = ...)` to see more rows We can actually pivot this summary table to make it more friendly for including into a publication: lugarSummaryWide &lt;- lugarSummary %&gt;% ungroup() %&gt;% select(century, individuals, nombre_castellano) %&gt;% pivot_wider(names_from = nombre_castellano, values_from = individuals) The results look much better: We can also return it back into its original long state in the following manner: lugarSummaryLong &lt;- lugarSummaryWide %&gt;% pivot_longer(!century, names_to = &quot;places&quot;, values_to = &quot;individuals&quot;) %&gt;% arrange(places) Results will look like: &gt; lugarSummaryLong # A tibble: 100 × 3 century places individuals &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 100 Almería NA 2 200 Almería NA 3 300 Almería 1 4 400 Almería 6 5 500 Almería 116 6 600 Almería 194 7 700 Almería 52 8 800 Almería 60 9 900 Almería 1 10 1000 Almería NA # ℹ 90 more rows # ℹ Use `print(n = ...)` to see more rows Note: most commonly this pivot operation is performed to convert long tables into wide and the other way around. However, pivoting also has an additional valuable function—as you can see in the example above, we now have NA for those centuries that did not have any values in the original data. Having such “blindspots” may lead to faulty visualizations and analyses; pivoting can help to ensure that the data is complete and all necessary values are accounted for. 4.11 Homework Tasks Essentially, this will be your first research assignment: you will actually learn some things — probably, quite a lot — which are not a common knowledge in the field. Simultaneously, you will practice all the data manipulation skills that we have covered in the lesson. Your homework task is to create a notebook for this lesson. For part I, rerun all the code from this lesson in your notebook to make sure it works and you are gaining some understanding of how it works; for the second part, give solutions to all the problems that are given below. Your code chunks should print the answer that comes from your calculations and manipulations. After your code chunk provide a human-readable answer — one or two sentences is sufficient. Who is the oldest individual we have in the PUA data? Which places did this individual visit? What kind of activities was that individual involved in? Who is the oldest jurist (faqīh) in the PUA data? Can you find a person who traveled the most? Can you find a century in which Andalusians traveled the most? What are the 10 most common activities Andalusians were involved into? What are the most common activities in the 5th AH century? What are the main locations of the 5 largest families? What are the most visited locations among the members of those families? In which century do we have the largest number of families? What is the peak century in the history of al-Andalus? (This can be measured by the highest number of people.) What is the most prominent location in al-Andalus in general? What about by centuries? (Prominent means that it has the highest number of people associated with it.) What is the most prominent location outside of al-Andalus? What about by centuries? Do the same task, but for common era centuries. Hint: you will also need to use a function from the previous lesson that converts AH dates to CE dates. Think carefully where you need to plug it in, i.e. between what steps. 4.12 Appendix: “Window” function in tidyverse Just in case, I am including some information on window functions. In the context of the tidyverse (package dplyr), there is a concept of window functions that allow you to perform calculations across a set of rows related to the current row. These functions are particularly useful when working with time series data or data that has a natural ordering. Some common window functions include: lag(): Accesses previous rows in a dataset. By default, it retrieves the value from the row immediately preceding the current row, but you can specify the number of rows to go back. lead(): Accesses following rows in a dataset. By default, it retrieves the value from the row immediately after the current row, but you can specify the number of rows to go forward. cumsum(), cumprod(), cummin(), and cummax(): Calculate the cumulative sum, product, minimum, and maximum, respectively, up to the current row. row_number(): Generates row numbers, often used for ranking. dense_rank(), min_rank(), and percent_rank(): Generate ranking scores based on different ranking methods. ntile(): Divides the dataset into a specified number of groups (tiles) based on the order of the data. To use these window functions, you generally need to use group_by() first to define the grouping structure, and then you can apply the window functions using mutate() or summarise(). Additionally, you can use the arrange() function to ensure the data is ordered correctly before applying the window functions. For example, here’s a simple code snippet that demonstrates the use of lag(): library(dplyr) data &lt;- tibble( date = seq(as.Date(&quot;2021-01-01&quot;), as.Date(&quot;2021-01-10&quot;), by = &quot;day&quot;), value = runif(10) ) data %&gt;% arrange(date) %&gt;% mutate(previous_value = lag(value)) In this example, the previous_value column is created using the lag() function, which contains the value from the row immediately preceding the current row. "],["data-manipulation-ii.html", "5 Data Manipulation II 5.1 This Chapter 5.2 Problems", " 5 Data Manipulation II 5.1 This Chapter we will go over the solutions to the problems from the previous Chapter 5.2 Problems … "],["control-flow-regular-expressions.html", "6 Control Flow; Regular Expressions 6.1 This Chapter 6.2 Control Flow 6.3 Regular Expressions in R", " 6 Control Flow; Regular Expressions 6.1 This Chapter we will discuss and learn about control flow in R, which includes: conditional statements; loops; functions; and also: regular expressions; 6.2 Control Flow Control flow refers to the order in which code is executed in a programming language. It is used to create logical structures and conditionally execute code based on specific conditions, enabling you to build more advanced and powerful programs. In R, there are several constructs that allow you to control the flow of your code, such as conditional statements, loops, and functions. 6.2.1 Conditional statements 6.2.1.1 if, else and else if statements In R, if, else, and else if are used to create conditional statements that control the flow of your code based on specific conditions. These constructs allow you to execute different blocks of code depending on whether certain conditions are met. Here’s a brief explanation of each: if: The if statement is used to test a condition. If the condition is true, the code within the curly braces {} following the if statement is executed. If the condition is false, the code is skipped. x &lt;- 5 if (x &gt; 0) { print(&quot;x is positive&quot;) } In this example, since x &gt; 0 is true, the message “x is positive” will be printed. else: The else statement is used in conjunction with an if statement. If the condition in the if statement is false, the code within the curly braces {} following the else statement is executed. x &lt;- -5 if (x &gt; 0) { print(&quot;x is positive&quot;) } else { print(&quot;x is non-positive&quot;) } In this example, since x &gt; 0 is false, the message “x is non-positive” will be printed. else if: The else if statement is used to test additional conditions when the previous if or else if conditions are false. If the condition in the else if statement is true, the code within the curly braces {} following the else if statement is executed. If the condition is false, the code is skipped, and the next else if or else statement (if any) is evaluated. x &lt;- 0 if (x &gt; 0) { print(&quot;x is positive&quot;) } else if (x &lt; 0) { print(&quot;x is negative&quot;) } else { print(&quot;x is zero&quot;) } Keep in mind that only the first condition which is evaluated to True will be executed - the other cases are ignored afterwards. Therefore consider the order in which conditions are tested. x &lt;- 15 if (x &gt; 0) { print(&quot;x is greater than zero&quot;) } else if (x &gt; 10) { print(&quot;x is greater than ten&quot;) } [1] &quot;x is greater than zero&quot; The output is greater than zero because the first condition is already True, so the second case is ignored even though that condition would be True as well. 6.2.1.2 ifelse() function ifelse() is a vectorized function in R that takes three arguments: a test condition, a value to return if the condition is True, and a value to return if the condition is False. It can be used to create a new vector by applying a condition to an existing vector. Here’s an example of how to use ifelse(): # Create a vector of numbers numbers &lt;- c(1, -2, 3, -4, 5) # Create a new vector using ifelse() number_signs &lt;- ifelse(numbers &gt;= 0, &quot;positive&quot;, &quot;negative&quot;) # Print the resulting vector print(number_signs) In this example, we create a numeric vector called numbers containing both positive and negative values. We then use ifelse() to create a new character vector called number_signs. For each element in numbers, ifelse() checks if the number is greater than or equal to 0 (i.e., positive). If the condition is TRUE, the corresponding element in number_signs will be “positive”. If the condition is FALSE, the corresponding element in number_signs will be “negative”. The resulting number_signs vector will look like this: [1] &quot;positive&quot; &quot;negative&quot; &quot;positive&quot; &quot;negative&quot; &quot;positive&quot; 6.2.1.3 Tasks modify this code that would check whether the date (year in Common Era) is pre-Islamic or not; try both approaches. dates &lt;- c(748, 600, 1500, 902, 571, 314, 3) 6.2.2 Loops Loops are a fundamental programming concept used to execute a block of code repeatedly until a certain condition is met. In R, there are two primary types of loops: for loops and while loops. for: This loop is used to iterate over a sequence (e.g., a vector, list, or range) and execute a block of code for each element in the sequence. while: This loop is used to execute a block of code as long as a specified condition is true. # for loop for (i in 1:5) { print(i) } # while loop counter &lt;- 1 while (counter &lt;= 5) { print(counter) counter &lt;- counter + 1 } 6.2.2.1 for loops: A for loop iterates over a sequence (e.g., a vector, list, or range) and executes the code block for each element in the sequence. The syntax for a for loop in R is: for (variable in sequence) { # Code to execute for each element in the sequence # The current element is stored in variable and is accessible inside the loop } For example, to iterate over a vector of numbers and print each number: numbers &lt;- c(1, 2, 3, 4, 5) for (number in numbers) { print(number) } 6.2.2.2 while loops: A while loop executes a block of code as long as a specified condition is true. The syntax for a while loop in R is: while (condition) { # Code to execute while the condition is true } For example, to print the numbers from 1 to 5 using a while loop: counter &lt;- 1 while (counter &lt;= 5) { print(counter) counter &lt;- counter + 1 } While loops can be powerful, but they can also lead to infinite loops if the specified condition never becomes false. Make sure to include logic inside the loop that eventually makes the condition false to avoid infinite loops. In addition to while loops, R provides the repeat loop. repeat { # Do something if (condition) { # If true exit the loop break } # Do something else only if the condition was evaluated as False } The repeat loop is executed until the command to exit it is called: break. Compared to the while loop, the repeat loop test the condition at the point you instruct to test the condition (i.e. somewhere in the middle or at the end of the statements inside the loop). The while loop always tests before executing the next cycle of the loop statements. counter &lt;- 5 while (counter &lt;= 5) { print(&#39;while-loop&#39;) print(counter) counter &lt;- counter + 1 } The output looks as follows: [1] &quot;while-loop&quot; [1] 5 counter &lt;- 5 repeat { print(&#39;repeat-loop&#39;) print(counter) if (counter &gt; 5) { break } print(&#39;Increment counter&#39;) counter &lt;- counter + 1 } The output looks as follows: [1] &quot;repeat-loop&quot; [1] 5 [1] &quot;Increment counter&quot; [1] &quot;repeat-loop&quot; [1] 6 Since everything in R revolves around vectors, there are other efficient ways for doing loop-like operation and we will not need to use loops in most cases. However, there are cases when loops are still the only way to go. We will come back to them in the next lessons. 6.2.2.3 break and next break: This statement is used to exit a loop prematurely. Hardly necessary in while loops as the condition should take care of exiting the loop. next: This statement is used to skip the current iteration of a loop and continue with the next iteration. fruits &lt;- list(&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;) for (x in fruits) { print(&#39;The current fruit is:&#39;) if (x == &quot;banana&quot;) { next } print(x) } The output looks as follows: [1] &quot;The current fruit is:&quot; [1] &quot;apple&quot; [1] &quot;The current fruit is:&quot; [1] &quot;The current fruit is:&quot; [1] &quot;cherry&quot; 6.2.2.4 Tasks sort the dates (year in Common Era) into two vectors: one should hold pre-Islamic dates and the other dates after the Hijra. Use a for loop. Add elements to a vector with append(vector, new_element) dates &lt;- c(748, 600, 1500, 902, 571, 314, 3) preislamic &lt;- c() postislamic &lt;- c() sort the dates (year in Common Era) into two vectors: one should hold pre-Islamic dates and the other dates after the Hijra. Use a while loop. 6.2.3 Functions Functions are reusable blocks of code that can be defined and called by name. They can take input arguments, perform a specific task, and return a result. We have already seen some build-in functions as well as functions that we load from different packages. What is important to stress now is that we can also build our own functions. # Define a function that adds two numbers add_numbers &lt;- function(a, b) { x &lt;- a + b return(x) } # Call the function result &lt;- add_numbers(3, 4) print(result) # Define a function that prints &#39;hello world!&#39; print_hello_world &lt;- function() { print(&#39;~^^~&#39;) print(&#39;hello world!&#39;) print(&#39;~^^~&#39;) } for (i in 1:10) { print(&#39;=====&#39;) if (i == 2) { # Print those statements from the function print_hello_world() } } print_hello_world() The output will look as follows: [1] &quot;=====&quot; [1] &quot;=====&quot; [1] &quot;~^^~&quot; [1] &quot;hello world!&quot; [1] &quot;~^^~&quot; [1] &quot;=====&quot; [1] &quot;=====&quot; [1] &quot;=====&quot; [1] &quot;=====&quot; [1] &quot;=====&quot; [1] &quot;=====&quot; [1] &quot;=====&quot; [1] &quot;=====&quot; [1] &quot;~^^~&quot; [1] &quot;hello world!&quot; [1] &quot;~^^~&quot; We want to group certain statements into a separate function whenever we execute the same block of statements multiple times in our code. Grouping those statements into a function ensures the statements are called in the exact same way in each of the occurrences without having to type the same code again (which is usually prone to errors). 6.2.3.1 Tasks write a function that converts AH dates (just years) to CE dates; think of additional convenient features, like converting a year to a nice date statement. For example, we take 750 and it gets converted into 750AH/1349CE, or something like that. write a function that converts CE dates to AH dates; write a function that converts period statements like 132-656 and converts it to 132-656AH/750-1258CE, and the other way around. Useful functions for these tasks: - paste() and paste0() — they allow to “paste” things together. For example, paste0(750, \"CE\") will give 750CE. - paste() — automatically inserts a single space between pasted elements; - paste0() — pastes elements together, creating a single string of characters; print(paste(750, &quot;CE&quot;)) [1] &quot;750 CE&quot; print(paste0(750, &quot;CE&quot;)) [1] &quot;750CE&quot; print(paste(750, &quot;CE&quot;, sep=&quot;;&quot;)) [1] &quot;750;CE&quot; print(paste(fruits, collapse=&quot;, &quot;)) [1] &quot;apple, banana, cherry&quot; Try to apply the final function to a vector of dates: all values in the vector should be converted. Solution (partial): AH2CEa &lt;- function(AH) { CE &lt;- round(AH - AH/33 + 622) return(CE) } AH2CEb &lt;- function(AH) { CE &lt;- round(AH - AH/33 + 622) AH &lt;- ifelse(AH == 0, 1, AH) final &lt;- paste0(AH, &quot; AH / &quot;, CE, &quot; CE&quot;) return(final) } periodsAH &lt;- seq(0, 1400, 50) periodsCEa &lt;- AH2CEa(periodsAH) periodsCEb &lt;- AH2CEb(periodsAH) &gt; periodsCEa [1] 622 670 719 767 816 864 913 961 1010 1058 1107 1155 1204 1252 1301 1349 1398 1446 1495 1543 1592 1640 1689 [24] 1737 1786 1834 1883 1931 1980 &gt; periodsCEb [1] &quot;1 AH / 622 CE&quot; &quot;50 AH / 670 CE&quot; &quot;100 AH / 719 CE&quot; &quot;150 AH / 767 CE&quot; &quot;200 AH / 816 CE&quot; [6] &quot;250 AH / 864 CE&quot; &quot;300 AH / 913 CE&quot; &quot;350 AH / 961 CE&quot; &quot;400 AH / 1010 CE&quot; &quot;450 AH / 1058 CE&quot; [11] &quot;500 AH / 1107 CE&quot; &quot;550 AH / 1155 CE&quot; &quot;600 AH / 1204 CE&quot; &quot;650 AH / 1252 CE&quot; &quot;700 AH / 1301 CE&quot; [16] &quot;750 AH / 1349 CE&quot; &quot;800 AH / 1398 CE&quot; &quot;850 AH / 1446 CE&quot; &quot;900 AH / 1495 CE&quot; &quot;950 AH / 1543 CE&quot; [21] &quot;1000 AH / 1592 CE&quot; &quot;1050 AH / 1640 CE&quot; &quot;1100 AH / 1689 CE&quot; &quot;1150 AH / 1737 CE&quot; &quot;1200 AH / 1786 CE&quot; [26] &quot;1250 AH / 1834 CE&quot; &quot;1300 AH / 1883 CE&quot; &quot;1350 AH / 1931 CE&quot; &quot;1400 AH / 1980 CE&quot; 6.3 Regular Expressions in R Regular expressions (often abbreviated as regex or regexp) are a powerful pattern-matching tool used in text processing and searching. They are essentially a sequence of characters that define a search pattern, which can then be used to find, replace, or manipulate text based on that pattern. Regular expressions are widely used in programming languages, text editors, search engines, and other tools that deal with text data. Like most other programming languages, R has support for regular expressions and in this tutorial will guide you through their usage in R. 6.3.1 Basics of Regular Expressions Here are some examples for the first part of the tutorial, focusing on the basics of regular expressions: Literals: Literal characters match themselves exactly. Example: Pattern: cat Matches: 'cat' in the string 'The cat is on the mat.' Metacharacters: Metacharacters have special meanings in regex and are used to build more complex patterns. Example metacharacters: . (matches any single character), * (matches zero or more repetitions of the preceding character), + (matches one or more repetitions of the preceding character) Example: Pattern: c.t Matches: 'cat', 'cot', 'c1t', etc. Character classes: Character classes are used to match specific types of characters. Examples: \\d (matches digits), \\w (matches word characters), \\s (matches whitespace characters) Example: Pattern: \\d{4}-\\d{2}-\\d{2} Matches: '2021-09-30' in the string 'The event will take place on 2021-09-30.' Custom character classes: You can create custom character classes using square brackets [...]. Example: [aeiou] (matches any vowel), [A-Za-z0-9] (matches any alphanumeric character) Example: Pattern: b[aeiou]t Matches: 'bat', 'bet', 'bit', 'bot', 'but' Quantifiers: Quantifiers specify how many times a character or a group of characters should be repeated. Examples: * (zero or more), + (one or more), ? (zero or one), {n} (exactly n times), {n,} (at least n times), {n,m} (at least n, but not more than m times) Example: Pattern: ca{2,4}t Matches: 'caat', 'caaat', 'caaaat' Grouping with parentheses: Parentheses are used to group characters and apply quantifiers to the entire group. Example: Pattern: (ab)+ Matches: 'ab', 'abab', 'ababab', etc. Alternation with the pipe symbol: The pipe symbol | is used to represent alternation (i.e., a choice between multiple patterns). Example: Pattern: apple|banana Matches: 'apple' or 'banana' Anchors: Anchors are used to specify the position of the match in the input string, which can be extremely helpful in a great number of research scenarios: ^ and $ match the beginning and the end of a string respectively. Example: ^this will only match the first instance of “this” in 'this is ridiculous and this is ridiculous', while ridiculous$ will only match the last instance of “ridiculous”. \\b matches word boundary. Example: cat will get two matches in 'This cat is a catastrophe waiting to happen!': the first match will be cat in “cat” , while the second match will be cat in “catastrophe”. \\bcat\\b will only match cat in “cat”. These examples should help you demonstrate the basics of regular expressions and how they can be used to create patterns for matching text. Remember to provide explanations and context for each example, so readers can understand the concepts being introduced. 6.3.2 Regular Expressions in R There is a number of regular expression functions in R, but we will stick to what we have in tidyverse. (You can learn about others on your own.) In the tidyverse, regular expressions are often used in conjunction with string manipulation functions from the stringr package. The stringr package provides a consistent, simple, and efficient set of functions for working with strings, and it is part of the tidyverse. Here are some examples of using regular expressions with stringr functions (Run these lines in R to check the results!): str_detect(): Test if a pattern is present in a string. library(tidyverse) words &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;, &quot;date&quot;) pattern &lt;- &quot;a.&quot; words_with_a &lt;- str_detect(words, pattern) print(words_with_a) str_replace(): Replace the first occurrence of a pattern with a specified string. text &lt;- &quot;The quick brown fox jumps over the lazy dog.&quot; pattern &lt;- &quot;o\\\\w+&quot; replacement &lt;- &quot;XXXX&quot; new_text &lt;- str_replace(text, pattern, replacement) print(new_text) str_replace_all(): Replace all occurrences of a pattern with a specified string. text &lt;- &quot;The quick brown fox jumps over the lazy dog.&quot; pattern &lt;- &quot;o\\\\w+&quot; replacement &lt;- &quot;XXXX&quot; new_text &lt;- str_replace_all(text, pattern, replacement) print(new_text) str_extract(): Extract the first occurrence of a pattern from a string. text &lt;- &quot;The price is $25.99, and the discount is $5.&quot; pattern &lt;- &quot;\\\\$\\\\d+\\\\.\\\\d{2}&quot; price &lt;- str_extract(text, pattern) print(price) str_extract_all(): Extract all occurrences of a pattern from a string. text &lt;- &quot;The price is $25.99, and the discount is $5.&quot; pattern &lt;- &quot;\\\\$\\\\d+(\\\\.\\\\d{2})?&quot; prices &lt;- str_extract_all(text, pattern) print(prices) str_split(): Split a string based on a pattern. text &lt;- &quot;apple,banana;cherry|date&quot; pattern &lt;- &quot;,&quot; split_text &lt;- str_split(text, pattern) print(split_text) text &lt;- &quot;apple,banana;cherry|date&quot; pattern &lt;- &quot;[,;|]&quot; split_text &lt;- str_split(text, pattern) print(split_text) In each example, the pattern argument is a regular expression, and the functions from the stringr package are used to manipulate the strings based on the pattern. The tidyverse ecosystem makes it easy to integrate regular expressions with data manipulation tasks, such as filtering, transforming, or summarizing data in data frames or tibbles. Regular expressions can be used in if statements, loops, functions, as well as, generally, in the processing of all main data structures. 6.3.3 Tips for Regular Expressions in R Start simple and build complexity gradually. Regular expressions is not an exact science, so to speak, but rather an art. It takes some time to get used to them. Regular expressions are greedy, i.e. they tend to catch more than you really need. Usually, it is relatively easy to write a regular expression that catches what you need, but it is much more difficult to write a regular expression in such a way that it does not what you do not need. This takes practice. Test your regex patterns using online tools like https://regex101.com/ (although it does not seem to support R). It also helps to use text editors (Sublime Text, Kate, etc.) that support regular expressions: paste a sample of a text you are working with and test your regular expression—in such text editors matches are usually automatically highlighted. Use comments and whitespace to make complex regex patterns more readable. In R, you can use comments and whitespace to make complex regular expressions more readable by employing the (?#...) syntax for inline comments and the (?x) modifier to enable free-spacing mode. In free-spacing mode, whitespace between regex tokens is ignored, allowing you to format the pattern for readability. Here’s an example: library(stringr) # Sample text text &lt;- &quot;Phone numbers: (123) 456-7890, 321-654-0987, +1 (987) 654-3210&quot; # Complex regex pattern to match phone numbers pattern &lt;- regex(&quot; (?x) (?# Enable free-spacing mode) (\\\\+\\\\d\\\\s)? (?# Optional international prefix with a space) (\\\\(?\\\\d{3}\\\\)?\\\\s?) (?# Optional area code with optional parentheses and space) \\\\d{3} (?# First three digits of the phone number) [-] (?# Separator: hyphen) \\\\d{4} (?# Last four digits of the phone number) &quot;, comments = TRUE) # Extract phone numbers from the text using the pattern phone_numbers &lt;- unlist(str_extract_all(text, pattern)) print(phone_numbers) In this example, we create a complex regex pattern to match different phone number formats. We use inline comments with (?#...) and free-spacing mode with (?x) to make the pattern more readable. The str_extract_all() function from the stringr package is then used to extract the phone numbers from the sample text. Be aware of R’s string escaping rules when using regex patterns (e.g., using double backslashes \\\\ instead of single ones \\). You can find the most detailed description of all the possible options in for regular expressions stringr at https://stringr.tidyverse.org/articles/regular-expressions.html. 6.3.4 In-Class Practice get some long-ish text variable; assign a series of tasks to find something very specific in that text; perhaps, the “old” practice file can be used in Sublime Text (it is best to use the same editor for this exercise). 6.3.5 The Practice Regex File ====================================================== ===Regular Expression Practical Session=============== ====================================================== [Regex Interactive Tutorial: http://regexone.com/] Best works in text editors: EditPad Lite/Pro on Windows (decent support of Arabic) Sublime Text or TextMate on Mac (limited support of Arabic) Alternatively, either of web RE testers: http://regexpal.com/, http://regexr.com/, http://regex101.com/: copy-paste the text into the lower window; test your regular expressions in the upper one. Regex Cheat Sheets: http://www.rexegg.com/regex-quickstart.html INTRO 1. Try the following regex: [ch]at that at chat cat fat phat 3. Try the following regex: [10][23] 02 03 12 13 4. Try the following regex: \\d\\d\\d[- ]\\d\\d\\d\\d 501-1234 234 1252 652.2648 713-342-7452 PE6-5000 653-6464x256 5. Try the following regex: runs? runs run 6. Try the following regex: 1\\d* 12345 122345 111111 113456 098097 109493 510349 673452 005645 7. Try the following regexes: ar?t, a[fr]?t, ar*t, ar+t, a.*t 1: “at” 2: “art” 3: “arrrrt” 4: “aft” PART I 1. What regular expression matches each of the following? “eat”, “eats”, “ate”, “eaten”, “eating”, “eater”, “eatery” 2. Find all Qadhdhafis... ... the name of the country&#39;s head of state [is] Colonel Gaddafi. Wait, no, that&#39;s Kaddafi. Or maybe it&#39;s Qadhafi. Tell you what, we&#39;ll just call him by his first name, which is, er ... hoo boy. (SRC: http://tinyurl.com/4839sks) The LOC lists 72 alternate spellings (SRC: http://tinyurl.com/3nnftpt) Maummar Gaddafi, Moamar AI Kadafi, Moamar al-Gaddafi, Moamar el Gaddafi, Moamar El Kadhafi, Moamar Gaddafi, Moamar Gadhafi, Moamer El Kazzafi, Moamer Gaddafi, Moamer Kadhafi, Moamma Gaddafi, Moammar el Gadhafi, Moammar El Kadhafi, Mo&#39;ammar el-Gadhafi, Moammar Gaddafi, Moammar Gadhafi, Mo&#39;ammar Gadhafi, Moammar Ghadafi, Moammar Kadhafi, Moammar Khadaffy, Moammar Khadafy, Moammar Khaddafi, Moammar Qudhafi, Moammer Gaddafi, Mouammer al Gaddafi, Mouammer Al Gaddafi, Mu`amar al-Kad&#39;afi, Mu`ammar al-Qadhdhāfī, Mu&#39;amar al-Kadafi, Muamar Al-Kaddafi, Muamar Gaddafi, Muamar Kaddafi, Muamer Gadafi, Muammar al Gaddafi, Muammar Al Ghaddafi, Muammar Al Qaddafi, Muammar Al Qathafi 3. Find all variations of Iṣbahān (construct the shortest possible regular expression): EASY: Iṣbahān, Iṣfahān, Isbahan, Isfahan, Esfāhān‎, Esfahān, Espahan, Ispahan, Sepahan, Esfahan, Hispahan, Nesf-e Jahān, iṣbahān, iṣfahān, isbahan, isfahan, esfāhān‎, esfahān, espahan, ispahan, sepahan, esfahan, hispahan, nesf-e jahān TRICKY: اصفهان، أصفهان، اسپهان، أصفهــان، أصبهان، آصفهان، إصفهان، آسپهان، ٱصفهــان، اصبهان، إصبهان، آصبهان PART II (more practice) 1. Conversion: Convert “Qaddafi, Muammar” &gt; “Muammar Qaddafi” Qaddafi, Muammar Al-Gathafi, Muammar al-Qadhafi, Muammar Al Qathafi, Muammar Al Qathafi, Muammar El Gaddafi, Moamar El Kadhafi, Moammar El Kazzafi, Moamer El Qathafi, MuAmmar Vader, Darth 2. Find all nisbas: EASY: Al-Iṣbahānī, al-Isfahani, Iskandarii, al-Baghdadiya, al-Baġdādīya, al-Kūfī, al-Dhahabi, Jawziyya, aṭ-Ṭabarī TRICKY: الاصبهاني، الاصفهانى، إسكندري، البغدادية، بغدادي، الكوفي، الذهبي، جوزية، الطبري 3. Find all words of the mafʿūl pattern: EASY: al-maqtūl, al-mafʿūl, al-maktūb, al-masʾūlaŧ, al-manṣūraŧ, al-maksūraŧ, maqtūl, mafʿūl, maktūb, masʾūlaŧ, manṣūraŧ, maksūraŧ TRICKY: المقتول، ٱلمفعول، المكتوب، المسؤولة، المنصورة، المكسورة، مقتول، مفعول، مكتوب، مسؤولة، منصورة، مكسورة 4. Find all given variations of the strong root: EASY: ḫabaza al-ḫabbāzu aḫbazan wa-maḫbūzātin fī maḫbazīhi TRICKY: خبز الخباز أخبازا ومخبوزات في مخبزه 5. Construct regular expressions that find references to the regions of al-Kūfa, al-Baṣra, Wāsiṭ, Baġdād and Ḥulwān. For convenience, toponyms are separated with commas. (Excerpt from al-Muqaddasī): # فاما الكوفة فمن مدنها حمام ابن عمر ، الجامعين ، سورا ، النيل ، القادسية ، عين التمر . # واما البصرة فمن مدنها الأبلة ، شق عثمان ، زبان ، بدران ، بيان ، نهر الملك ، دبا ، نهر الأمير ، ابو الخصيب ، سليمانان ، عبادان ، المطوعة ، والقندل ، المفتح ، الجعفرية . # واما واسط فمن مدنها فم الصلح ، درمكان ، قراقبة ، سيادة ، باذبين ، السكر ، الطيب ، قرقوب ، قرية الرمل ، نهر تيري ، لهبان ، بسامية ، اودسة . # واما بغداد فمن مدنها النهروان ، بردان ، كارة ، الدسكرة ، طراستان ، هارونية ، جلولا ، باجسرى ، باقبة ، إسكاف ، بوهرز ، كلواذى ، درزيجان ، المدائن ، كيل ، سيب ، دير العاقول ، النعمانية ، جرجرايا ، جبل ، نهر سابس ، عبرتا ، بابل ، عبدس ، قصر ابن هبيرة . # واما حلوان فمن مدنها خانقين ، زبوجان ، شلاشان ، الجامد ، الحر ، السيروان ، بندنيجان . 6. [In pseudocode] Construct a regular expression that finds dates in Arabic (limit to years) مات في سنة اثنتين واستقر بعده محمد بن غرلو. ولد البهاء في سنة ثمان وسبعين وخمسمائة، وسمع من فلان. ولد بالقاهرة في سنة أربع عشرة تقريبا وأمه أم ولد. مات بالطاعون في سنة ثلاث وثلاثين. ولد سنة تسع وتسعين بدمشق. وقد حج صاحب الترجمة في سنة تسع وثمانين. توفي في ذي الحجة سنة تسع عشرة. توفي سنة تسع وثلاثين. ولد سنة ثلاث عشرة وست مائة وتوفي سنة عشر وسبع مائة. حدث بشيراز سنة نيف وأربعين عن يعقوب بن سفيان. "],["getting-your-data-right.html", "7 Getting Your Data Right 7.1 This Chapter 7.2 1. Getting your own data 7.3 2. Tidying Data 7.4 3. Modeling Data 7.5 Conceptual Structure for Your Data 7.6 Reference Materials 7.7 Appendix: OCR in R", " 7 Getting Your Data Right 7.1 This Chapter we will discuss how to get your data into a proper shape: how to extract data from printed sources; how to “tidy” your data, that is to say to how prepare it for the use with the tidyverse approach; how to normalize your data, which is also a part of tidying your data; how to model your data, which is about getting more from your data during analyses; then, we will look into the audition certificates data in its original format and the format that we will be using in this class. 7.2 1. Getting your own data 7.2.1 Ways of obtaining data Reusing already produced data One may require to mold data into a more fitting structure . Creating one’s own dataset Digitizing data from printed and/or hand-written sources 7.2.2 Major formats Relational databases or Tables/Spreadsheets (tabular data)? Tabular format: tables; spreadsheets; CSV/TSV files; Unique identifiers: tables with different data can be connected via unique identifiers Note: A relational database (rDB) is a collection of interconnected tables. Tables in an rDB are connected with each other via unique identifiers which are usually automatically created by the database itself when new data is added. In the PUA data, we have idPersonaje in the main personaje table, and then in all the personaje_x tables, which connect individuals and their relevant descriptions. For example, we can connect personaje and lugar via personaje_lugar and analyze the geography of people from the database. One can maintain interconnected tables without creating a rDB with a Linked Open Data approach (LOD); or, better Linked Local Data approach. The main idea is that we create unique identifiers to all entities that we use—then we can use these identifiers to expand our data either manually, semi-automatically, or automatically. 7.2.3 Ways of obtaining data Reusing already produced data One may require to mold data into a more fitting structure. Creating one’s own dataset Digitizing data from printed and/or hand-written sources 7.2.4 Major formats Relational databases or Tables/Spreadsheets (tabular data)? Tabular format: tables; spreadsheets; CSV/TSV files. Unique identifiers: tables with different data can be connected via unique identifiers Note: A relational database (rDB) is a collection of interconnected tables. Tables in an rDB are connected with each other via unique identifiers which are usually automatically created by the database itself when new data is added. One can maintain interconnected tables without creating a rDB: Open Linked Data; (Local Linked Data: you simply connect datasets that you create and have on your computer;) Example: Table of the growth of cities. One table includes information on population over time; Another table includes coordinates of the cities from the dataset. It is more efficient and practical (reducing error rate from typos) to work on these tables separately, and connect them via unique identifiers of cities which are used in both tables. 7.2.4.1 Note on the CSV/TSV format CSV stands for comma-separated values; TSV — for tab-separated values. Below is an examples of a CSV format. Here, the first line is the header, which provides the names of columns; each line is a row, while columns are separated with , commas. DATE,West,East,DATE,West,East 4000 BCE,0,0,1 BCE/CE,0.12,0.08 3000 BCE,0.01,0,100 CE,0.12,0.08 2500 BCE,0.01,0,200 CE,0.11,0.07 2250 BCE,0.01,0,300 CE,0.10,0.07 2000 BCE,0.01,0,400 CE,0.09,0.07 1750 BCE,0.02,0,500 CE,0.07,0.08 1500 BCE,0.02,0.01,600 CE,0.04,0.09 1400 BCE,0.03,0.01,700 CE,0.04,0.11 1300 BCE,0.03,0.01,800 CE,0.04,0.07 1200 BCE,0.04,0.02,900 CE,0.05,0.07 1100 BCE,0.03,0.02,1000 CE,0.06,0.08 1000 BCE,0.03,0.03,1100 CE,0.07,0.09 900 BCE,0.04,0.03,1200 CE,0.08,0.09 800 BCE,0.05,0.02,1300 CE,0.09,0.11 700 BCE,0.07,0.02,1400 CE,0.11,0.12 600 BCE,0.07,0.03,1500 CE,0.13,0.10 500 BCE,0.08,0.04,1600 CE,0.18,0.12 400 BCE,0.09,0.05,1700 CE,0.35,0.15 300 BCE,0.09,0.06,1800 CE,0.50,0.12 200 BCE,0.10,0.07,1900 CE,5.00,1.00 100 BCE,0.11,0.08,2000 CE,250.00,12.50 Example for pasting into Excel: the very last value will be misinterpreted. (The example is: War-making capacity since 4000 BCE (in social development points), from: Morris, Ian. 2013. The Measure of Civilization: How Social Development Decides the Fate of Nations. Princeton: Princeton University Press.) TSV is a better option than a CSV, since TAB characters (\\t) are very unlikely to appear in values. Neither TSV not CSV are good for preserving new line characters (\\n)—or, in other words, text split into multiple lines/paragraphs. As a workaround, one can convert \\n into some unlikely-to-occur character combination (for example, ;;;), which would be easy to restore into \\n later, if necessary. 7.3 2. Tidying Data 7.3.1 Basic principles of organizing data: Tidy Data Tidy data is a concept in data organization and management introduced by statistician Hadley Wickham. It refers to a specific structure of organizing data sets in a way that is easy to analyze and manipulate, typically in the context of data science and statistical analysis. Tidy data adheres to the following principles: Each variable is in its own column: This means that every column represents a single variable or feature, making it easy to understand and analyze the data. Each observation is in its own row: This ensures that each row represents a unique observation or data point, allowing for simple indexing and filtering of the data. Each value is in its own cell: By having individual values in separate cells, the data is clearly organized and easy to manipulate or analyze using various data processing tools and techniques. The need to use tidy data arises for several reasons: Simplified analysis: Tidy data makes it easier to perform exploratory data analysis, as the consistent organization allows for the straightforward application of various data manipulation and statistical analysis techniques. Improved readability: The structure of tidy data is intuitive and easy to understand, even for those with limited experience in data analysis. This makes the data more accessible for interpretation, collaboration, and communication. Code efficiency: With a consistent data structure, analysts can write more efficient and reusable code, as the same functions can be applied across various tidy data sets. For example, you can create an analytical routine in R that requires your data to be in a specific format—after that you can take any relevant data, convert it into the needed structure and simply reuse your R routine. Reduced errors: Tidy data reduces the potential for errors in data analysis by minimizing the need for manual data reshaping and transformation, which can introduce errors or inconsistencies. Better data quality: Tidy data encourages good data management practices by promoting the organization of data in a clear and consistent manner, making it easier to identify and address data quality issues. In summary, adopting tidy data principles helps streamline data analysis processes, enhance collaboration, and improve the overall quality and accuracy of data-driven insights. The original paper: Wickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10. (The article in open access) 7.3.2 Clean Data / Tidy Data: additional explanations Column names and row names are easy to use and informative. In general, it is a good practice to avoid spaces and special characters. Good example: western_cities Alternative good example: WesternCities Bad example: Western Cities (only the largest) Obvious mistakes in the data have been removed: Date format: YYYY-MM-DD is the most reliable format. Any thoughts why? There should be no empty cells: * If you have them, it might be that your data is not organized properly. * If your data is organized properly, NA must be used as an explicit indication that data point is not available. Each cell must contain only one piece of data. Variable values must be internally consistent Be consistent in coding your values: M and man are different values computationally, but may have the same meaning in the dataset; Keep track of your categories, i.e., keep a separate document where all codes used in the data set are explained. Preserve original values: If you are working with a historical dataset, it will most likely be inconsistent. For example, distances between cities are given in different formats: days of travel, miles, farsaḫs/parasangs, etc.). Instead of replacing original values, it is better to create an additional column, where this information will be homogenized according to some principle. Keeping original data will allow to homogenize data in multiple ways (example: day of travel). Clearly differentiate between the original and modified/modeled values. The use of suffixes can be convenient: Distance_Orig vs Distance_Modified. Most of editing operations should be performed in software other than R; any spreadsheet program will work, unless it cannot export into CSV/TSV format. Keep in mind that if you prepare your data in an Excel-like program, rich formatting (like manual highlights, bolds, and italics) is not data and it will be lost, when you export your data into CSV/TSV format. Keep in mind also that programs like Excel tend to overdo it. For example, they may try to guess the format of a cell and do something with your data that you do not want. (Example for pasting into Excel) Note: It might be useful, however, to use rule-based highlighting in order, for example, to identify bad values that need to be fixed. Back up your data! In order to avoid any data loss, you need to have a good strategy to preserve your data periodically. http://github.com is a great place for this, plus it allows to work collaboratively. Google spreadsheets is a decent alternative as it allows multiple people to work on the same dataset, but it lacks version control and detailed tracking of changes. Example for pasting into Excel: the very last value will be misinterpreted. (The example is: War-making capacity since 4000 BCE (in social development points), from: Morris, Ian. 2013. The Measure of Civilization: How Social Development Decides the Fate of Nations. Princeton: Princeton University Press.) DATE,West,East,DATE,West,East 4000 BCE,0,0,1 BCE/CE,0.12,0.08 3000 BCE,0.01,0,100 CE,0.12,0.08 2500 BCE,0.01,0,200 CE,0.11,0.07 2250 BCE,0.01,0,300 CE,0.10,0.07 2000 BCE,0.01,0,400 CE,0.09,0.07 1750 BCE,0.02,0,500 CE,0.07,0.08 1500 BCE,0.02,0.01,600 CE,0.04,0.09 1400 BCE,0.03,0.01,700 CE,0.04,0.11 1300 BCE,0.03,0.01,800 CE,0.04,0.07 1200 BCE,0.04,0.02,900 CE,0.05,0.07 1100 BCE,0.03,0.02,1000 CE,0.06,0.08 1000 BCE,0.03,0.03,1100 CE,0.07,0.09 900 BCE,0.04,0.03,1200 CE,0.08,0.09 800 BCE,0.05,0.02,1300 CE,0.09,0.11 700 BCE,0.07,0.02,1400 CE,0.11,0.12 600 BCE,0.07,0.03,1500 CE,0.13,0.10 500 BCE,0.08,0.04,1600 CE,0.18,0.12 400 BCE,0.09,0.05,1700 CE,0.35,0.15 300 BCE,0.09,0.06,1800 CE,0.50,0.12 200 BCE,0.10,0.07,1900 CE,5.00,1.00 100 BCE,0.11,0.08,2000 CE,250.00,12.50 7.3.3 Discussion: “A Bulliet Dataset” Dataset: The dataset shows chrono-geographical distribution of Islamic scholars, according to one of the medieval biographical sources. Source: Bulliet, Richard W. 2009. Cotton, Climate, and Camels in Early Islamic Iran: A Moment in World History. New York: Columbia University Press. P. 139 This data is formatted for presentation in a book; for data analysis this data needs to be converted into tidy format. What should be corrected? Think of how the data should look so that we could analyze it? 7.4 3. Modeling Data 7.4.1 Categorization as a Way of modeling data “[Modeling is] a continual process of coming to know by manipulating representations.” Willard McCarty, “Modeling: A Study in Words and Meanings,” in Susan Schreibman, Ray Siemens, and John Unsworth, A New Companion to Digital Humanities, 2nd ed. (Chichester, UK, 2016), http://www.digitalhumanities.org/companion/. One of the most common way of modeling data in historical research—joining items into broader categories. Categorization is important because it allows to group items with low frequencies into items with higher frequencies, and through those discern patterns and trends. Additionally, alternative categorizations allow one to test different perspectives on historical data. The overall process is rather simple in terms of technological implementation, but is quite complex in terms of subject knowledge and specialized expertise which is required to make well-informed decisions. For example, let’s say we have the following categories: baker, blacksmith, coppersmith, confectioner, and goldsmith. These can be categorized as occupations; Additionally, blacksmith, coppersmith, and goldsmith can also be categorized as ‘metal industry’, while baker and confectioner, can be categorized as ‘food industry’; Yet even more, one might want to introduce additional categories, such as luxury production to include items like goldsmith and confectioner; and regular production for items like baker, blacksmith, coppersmith. Such categorizations can be created in two different ways, with each having its advantages: first, one can create them as additional columns. This approach will allow to always have the original—or alternative—classifications at hand, which is helpful for re-thinking classifications and creating alternative ones where items will be reclassified differently, based on a different set of assumptions about your subject. second, these can be created in separate files, which might be easier as one does not have to stare at existing classifications and therefore will be less influenced by them in making classification decisions. Additionally, one can use some pre-existing classifications that have already been created in academic literature. These most likely need to be digitized and converted into properly formatted data, as we discussed above. 7.4.2 Normalization This is a rather simple, yet important procedure, which is, on the technical side, very similar to what was described above. In essence, the main goal of normalization is to remove insignificant differences that may hinder analysis. Most common examples would be: bringing information to the same format (e.g., dates, names, etc.) unifying spelling differences It is a safe practice to preserve the initial data, creating normalized data in separate columns (or tables) 7.4.3 Note: Proxies, Features, Abstractions These are the terms that refer to the same idea. The notion of proxies is used in data visualization, that of features—in computer science; that of abstractions—in the humanities (see, for example, Franco Moretti’s Graph, Maps, Trees). The main idea behind these terms is that some simple features of an object can act as proxies to some complex phenomena. For example, we can use individuals who are described as “jurists” as a proxy for the development of Islamic law. This way we use onomastic information as a proxy to the social history of Islamic law. While proxies are selected from what is available—usually not much, especially when it comes to historical data—as a way to approach something more complex. It may also be argued that abstractions are often arrived to from the opposite direction: we start with an object which is available in its complexity—in the case of PUA, the starting point is biographies written in natural language (Arabic). The PUA researchers then reduced the complexity of biographies in natural language to a more manageable form which—we expect—would represent specific aspects of the initial complex object. From this perspective, the PUA database itself is an abstraction of biographies of Andalusians. Most commonly (and automatically) abstractions are used with texts in natural languages. For example, in stylometry texts are reduced to frequency lists of most frequent features, which are expected to represent an authorial fingerprint. Using these frequency lists we can—with accuracy up to 99%—identify authors of particular texts. The complexity of texts can be reduced in a number of ways: into a list of lemmas (e.g., for topic modeling analysis), frequency lists (e.g., for document distance comparison, such as, for example, stylometry), keyword values (e.g., for identifying texts on a similar topic, using, for example, the TF-IDF method), syntactic structures, ngrams, etc. As you get to practice and experiment more, you will start coming up with your own ways of creating abstractions depending on your current research questions. 7.5 Conceptual Structure for Your Data … Let’s first take a look at the original AC data … 7.5.1 Reorganization: Conceptual Model For Your Data The model that you should choose depends significantly on what exactly you are doing. When the data is already collected and you are just reorganizing it for more efficiency, a much wider variety of approaches can be used. This is the case when the use of a relational database will be fully justified and very easy to implement, since the data is already collected—the most difficult and time-consuming part is already done. It is a completely different case when you are only beginning to collect your data. In cases when your objects have a very clear set of attributes and information on all or most of those attributes is available or relatively easily acquirable, the reliance on relational databases could be quite efficient. The cases of such objects include actual physical objects and the set of their attributes include descriptions of their physical properties. For example, editions, manuscripts, etc. In cases, when the set of attributed is not clear, relational databases will be significantly less efficient. Biographical/prosopographical data is one of such cases, mainly for the following reason. It is practically impossible to know beforehand what attributes will be of actual relevance; even though one may think that there is a clear set of attributes, the actual data from the sources may be skewed in a variety of ways not providing one with relevant data on most attributes. On the other hand, working with a particular source one inevitably discovers attributes that were not included in the initial thinking about a given problem, but happened to be important for thinking about specific groups of people. Relational databases are rather inflexible for modifications and would require significant re-design to be realigned with new realities of research. In the previous lesson, I have mentioned the subject-predicate-object model, with which you can describe pretty much everything that you can describe with a subject-predicate-object sentence. The traditional SPO model, however, is not exactly perfect for our purposes, especially if we want to record conflicting information from different sources. For this purpose a somewhat different model has been proposed by Tara Andrews, the Professor of Digital Humanities at the University of Vienna. since she is also a historian, working on the medieval Armenian history, she is interested in recording both the provenance, i.e. where the information is taken from, and the authority, i.e. who is making this assertion—since even the same information in the same source may be read differently by two different scholars. To solve this issue, she proposed the structured assertion model (STAR), that you see on the right. Thus, with this model, we would have a five column table, instead of a three-column one. Let’s take a look at how STAR method is used in the OpenITI project. We can use the following piece of information as an example of how biographical data can be encoded using STAR method in plain text format: al-Ḏahabī was a teacher (tafaqqaha ʿalay-hi) of al-Subkī in Damascus from 699 till 715 AH. (Source: a made-up example with the reference code 220607114503) Now, in order to do the actual encoding we use an ordered structure, URIs, and patterned statements. The ordered structure means that the category of data is defines by its location in our encoding statement (for example, the first element is always the subject, the second—always the predicate, etc.). The patterned statement means that we encode certain types of data with a sequence of characters that can be described with generalized patterns (regular expressions), which can later be used to both extract and interpret encoded data. The end result may look like the following statement, where five elements are connected with @, and details on all the coded information being available/collected in additional files (linked local data): [0748Dhahabi@]teacherOf_tafaqqahaCalayhi@0771Subki;DIMASHQ_363E335N_S;699_XXX_XX-715_XXX_XX@MGR@MacrufDahabi1976s_1_345 The first element is the subject, which is recorded using the Author URI—0748Dhahabi; in order to simplify things, we most commonly omit the subject, since it is implied by the name of the file in which metadata is collected (more on this in the discussion of YML files with metadata); The second element is our predicate—teacherOf_tafaqqahaCalayhi. We use unrestricted vocabulary, since we are still at the research stage and this appears to be more productive to explore possibilities; the normalization of predicates will take place periodically during the revisions of metadata. As you may have noticed, there is a pattern in the predicate—there are two parts connected with the “underscore”, which allows us to record the predicate with the reference to its original form in Arabic (although, one can also use simply teacherOf, if no Arabic equivalent is available). This kind of encoding is particularly important for developing classification schemes, where one has to keep track of the original vocabulary (for example, relationships among books—different types of abridgments, continuations, commentaries, etc.). The third element is our objects, of which we have three: 1) the “direct object”, which is a person also encoded with the Author URI (771Subki)—all individuals are to be encoded using such URIs;1 2) the “object of place”, which is encoded with the URI from an affiliated project al-Ṯurayyā (https://althurayya.github.io/)—all geographical entities are to be encoded with such URIs; 3) the “object of time”, which is encoded following a specific pattern YYYY_MMM_DD where sub-elements are also divided with an underscore. From this pattern we know that the first sub-element is the year, the second—the month, and the third—the day. In case, any of sub-elements are unknown, they are encoded with Xs. For the encoding of a period of time, two date statements are connected with “-” (hyphen). The fourth element is authority, encoded with the pattern AUTH_ContributorURI, with ContributorURIs recorded in a separate file that contains additional information on all contributors to the project. The last, fifth element contains references, which can be encoded in a variety of ways. Here, we use MacrufDahabi1976s_1_345, which consists of three elements separated with _. The first element is the bibTeX code of a publication (with the detailed information on the edition stored in a bibTex file with all the used bibliography), while the other two are the volume and page number. (Note: this reference is not real.) Note: even if some elements seem complicated for you at the moment because you do not have enough skills to process them and convert them into something more useful, you should always strive for consistency in how you encode your data—if you want to be able to analyze that data computationally. For example, the reference MacrufDahabi1976s_1_345 may seem weird, but it is structurally solid: there are three elements, separated with _ and it is very easy to break this coded piece of information into distinct elements. You already know how to do that with R, where, using tidyverse approach (more specifically, functions from the family of stringr) you can break such references into three separate columns: reference, volume, page(s). Even if you do not know now how to do that, it is always important to plan ahead and focus on encoding your data in a consistent manner. Now, let’s talk a bit about how to practically implement these models. The STAR model is meant to fit into existing standards, like CIDOC-CRM or/and FRBR-OO (see, https://cidoc-crm.org/collaborations), and implement Linked Open Data (LOD) approach, i.e. linking into existing online datasets. Since we are focusing more on Linked Local Data (LLD) approach, we also want a more pragmatic—i.e., simple—implementation of the STAR model (CIDOC-CRM is a rather complicated standard, see below for an example). CIDOC-CRM Encoding Example in RDF XML (*.xml). This examplecaptures a historical event, relevant for the history of art: Johann-Joachim Winkelmann (a German Scholar) has seen the so-called Laocoön Group in 1755 in the Vatican in Rome (at display in the Cortile del Belvedere). He described his impressions in 1764 in his “History of the Art of Antiquity”, (being the first to articulate the difference between Greek, Greco-Roman and Roman art, characterizing Greek art with the famous words “…noble simplicity, silent grandeur”). The sculpture, in Hellenistic “Pergamene baroque” style, is widely assumed to be a copy, made between 27 BC and 68 AD (following a Roman commission) from a Greek (no more extent) original. Johann-Joachim Winkelmann was born 1717 as child of Martin Winkelmann and Anna-Maria Meyer and died in 1768 in Trieste. Sources: https://www.cidoc-crm.org/sites/default/files/Winkelmann_time_label_1.xml; https://cidoc-crm.org/sites/default/files/CIDOC%20CRM_v.7.0_%2020-6-2020.pdf In fact, depending on your sources and your final goals, you can opt for one of the following three variations. As we discussed the simplest variation will be the S-P-O model, with SUBJECT, PREDICATE, and OBJECT. (As you can see, the SUBJECT is repeated many times, so, technically, one can simplify the data encoding even further, by keeping separate files for each person, and then having only PREDICATE and OBJECT columns—the SUBJECT will be encoded in the name of the file.): If your assertions are simple (i.e., they have single objects), you can use a “simple” STAR model, where we add PROVENANCE and AUTHORITY: If your assertions have multiple objects, then we would need a “robust” STAR model with an additional column for AssertionID. The complex event has id 17, where we describe that “al-Ḏahabī was a teacher (tafaqqaha ʿalay-hi) of al-Subkī in Damascus from 699 till 715 AH. (Source: a made-up example with the reference code 220607114503)”: Note: Please, watch Tara Andrews’ presentation “How Might We Make Data Collections More Useful for Historians?” (https://www.youtube.com/watch?v=JcBdthObApY) on the STAR model. The advantages of such models: they are symmetrical, i.e. you always know which column contains which data. This means you can easily manipulate this data in R. when you load this kind of data into R, you can recursively query your dataset. For example, if you want to find people who died in, say, Qurṭubaŧ, you can filter OBJECT to find all instances of QURTUBA_047W379N_S; then limit your results to PREDICATE == place_died. Then you would get all the unique IDs from SUBJECT. Then you will filter date_died (in PREDICATE) by the unique IDs that you got in the previous step. And now you have all the death dates of people who died in Qurṭubaŧ (don’t forget to pick a single date from date_died data!). they are expandable, since you can easily encode new types of information by simply introducing new predicates, and without changing the main structure of your data. Please, note how predicates are named: a broader category is followed by a subcategory, separated with _; This will allow you to easily pull out dates, places, etc. Keep in mind that there is no single correct way of modeling your predicates. this is something that you should periodically revise and improve. It helps to keep a visual scheme of your predicates and playing around with it, until you arrive to some stable scheme that allows you to encode everything you need. they allow to implement the Linked Data approach locally (i.e., Linked Local Data): for example, for places, I used settlement IDs from the al-Ṯurayyā Gazetteer (https://althurayya.github.io/); you can download and keep al-Ṯurayyā data in a TSV file and you can get coordinates and settlement classifications from this file easily. If some places are missing, you can expand the original al-Ṯurayyā dataset by adding new places into it (and assigning new IDs following the same pattern, but perhaps adding some suffix that would indicate that this place was not originally in al-Ṯurayyā). for authorities, if you have many of those, you can also keep a file with all the additional descriptions that would be connected to the IDs used in the main table. similarly for predicates (PREDICATE) and references (PROVENANCE), as well as other columns, as need arises. you will want to keep a master document where you would describe all other conventions that you adopt for your research project (for example, date encoding.) I have mentioned above the traditional subject-predicate-object model, with which you can describe pretty much everything that you can describe with a subject-predicate-object sentence. 7.6 Reference Materials Wickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10. (The article in open access) Check these slides: A. Ginolhac, E. Koncina, R. Krause. Principles of Tidy Data: tidyr https://lsru.github.io/tv_course/lecture05_tidyr.html (Also check their other lectures/slides: ) Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. The following book is a great example of modeling data for historical studies: Bulliet, Richard. 1979. Conversion to Islam in the Medieval Period: An Essay in Quantitative History. Cambridge: Harvard University Press. Morris, Ian. 2013. The Measure of Civilization: How Social Development Decides the Fate of Nations. Princeton: Princeton University Press. Note: This book is a methodological companion to: Morris, Ian. 2010. Why the West Rules—for Now: The Patterns of History, and What They Reveal about the Future. New York: Farrar, Straus and Giroux. 7.7 Appendix: OCR in R As was noted above, we can use R to OCR text in PDFs and images. The following libraries will be necessary. library(pdftools) library(tidyverse) library(tesseract) library(readr) This code we can use to OCR individual PNG files. text &lt;- tesseract::ocr(pathToPNGfile, engine = tesseract(&quot;eng&quot;)) readr::write_lines(text, str_replace(pathToPNGfile, &quot;.png&quot;, &quot;.txt&quot;)) This code can be used to process entire PDFs: imagesToProcess &lt;- pdftools::pdf_convert(pathToPDFfile, dpi = 600) text &lt;- tesseract::ocr(imagesToProcess, engine = tesseract(&quot;eng&quot;)) readr::write_lines(text, str_replace(pathToPDFfile, &quot;.pdf&quot;, &quot;.txt&quot;)) NB: I had issues running pdftools on Mac. Make sure that you install additional required tools for it. For more details, see: https://github.com/ropensci/pdftools. More details on how to use Tesseract with R you can find here: https://cran.r-project.org/web/packages/tesseract/vignettes/intro.html Tesseract—a separate software—must be installed first. The R library tesseract is simply a “wrapper application”, which essentially calls it from R. Using R scripts (or, Python scripts) can be particularly useful when one needs to process lots of different files in a particular manner. Thus, an R script will take care of all the additional steps, while Tesseract will only be called for one specific operation—optical character recognition. We are primarily interested in the relationships within the OpenITI projects, i.e., among works and among authors, but the coverage can be easily expanded to include non-authors.↩︎ "],["visualizations-plot-and-ggplot.html", "8 Visualizations: plot() and ggplot() 8.1 This Chapter 8.2 Conceptual Issues 8.3 R Built-in Graphics Functions: Quick Overview 8.4 Tidyverse Graphs: ggplot2 8.5 Simpler graphs 8.6 Most Relevant Types of Graphs (for Audition Certificates Data) 8.7 Appendices", " 8 Visualizations: plot() and ggplot() 8.1 This Chapter In this class we will move to the next important part of data analysis: data visualization. We will practice working with the most popular graphic library in R ggplot2. In the end, we will also take a quick look at the gt library, which was designed to prepare nice table suitable for presentations. 8.2 Conceptual Issues 8.2.1 Perception Creating visualizations, we have rather limited means of expressing information effectively. (The following images are from Tamara Munzner’s “Visualization Analysis and Design” (2014), which is considered to be a classical introduction into visualizations.) First, we are essentially limited to: points, lines, and areas—these are our geometric primitives: We can use position, color, shape, tilt, and size to encode additional information: When we do, we need to consider perceptual issues that are common to our species. In other words, depending on how the information is encoded, we can “see” it better or worse: These perceptual issues have been experimentally discovered and confirmed. In the image below you can see that, depending how you visualize your data, your readers will have less or more difficulties interpreting your visualizations correctly. In a nutshell, absolute positions are the easiest to interpret, and then everything becomes more and more complicated: relative positions, angles, circular areas, rectangular areas. The following illustration shows three different representations of exactly the same information. For our perception it is much easier to see the difference in (c), than in (a) or (b). Colors can also create a variety of optical illusions, even when not considering colorblindness. In the example below, our perception tells us that squares A and B in (a) are different in color, yet, if we superimpose a gray mask on the image, we can “see” that they are of the same color. Takeaway: “get it right in black and white”. 8.2.2 The Rules of Thumb for Visualizations Tamara Munzner in her “Visualization Analysis and Design” (2014) formulated eight rules of thumb (Chapter 6. “Rules of Thumb”), which should be followed in order to create visualizations that effectively convey complex information, avoid common pitfalls, and facilitate meaningful insights. (The PDF of the chapter is provided.) No Unjustified 3D: The Power of the Plane: Use 2D representations when possible, as they are easier to interpret and compare. The Disparity of Depth: Depth perception varies among individuals, making 3D depth cues unreliable for accurate comparisons. Occlusion Hides Information: 3D visualizations can lead to important data points being obscured by other elements. Perspective Distortion Dangers: 3D projections can distort the perception of sizes and distances, leading to incorrect interpretations. Tilted Text Isn’t Legible: 3D visualizations often use tilted text, which can be challenging to read. No Unjustified 2D: Avoid using 2D visualizations when they don’t provide clear benefits over simpler 1D representations. Eyes Beat Memory: Design visualizations that minimize the need for viewers to rely on their memory by placing related data points close together or using common baselines. Resolution over Immersion: Prioritize high-resolution displays over immersive environments, as they provide more detail and are easier to interpret. Overview First, Zoom and Filter, Detail on Demand: Present an overview of the data first, then allow users to zoom in and filter the data to focus on specific areas, and finally provide additional details on demand. Responsiveness Is Required: Ensure that the visualization responds quickly to user input, as slow responsiveness can be frustrating and lead to disengagement. Get It Right in Black and White: Make sure the visualization is effective in grayscale before adding color, as this ensures that the design relies on more perceptually accurate visual channels (e.g., position and length) rather than color alone. Function First, Form Next: Prioritize the functionality and effectiveness of the visualization, and then focus on aesthetics to create a visually pleasing and engaging design. 8.2.3 Types of Graphs and When to Use Them Visualizations can be grouped based on the types of data they are designed to represent. Here’s a categorization of common visualizations based on data types: 8.2.3.1 Categorical data (nominal or ordinal) For example, we have people with different activities in a specific place; each activity will be a category, while the umber of people associated with that activity will determine the magnitude of each activity. Bar chart: Compares values across categories. For example, each city can be represented as a series of activities (an activity per bar), thus each city should have a distinct visual profile. Pie chart: Represents proportions of different categories in a dataset. In a way, very similar to a bar chart in its function, but much more difficult to interpret, especially when you have multiple pies to compare (see, perception issues above.) A bar chart, or even a list/table may be a better alternative. (Remember, No Unjustified 2D.) do not use them, please. TreeMap: Visualizes hierarchical data structures and part-to-whole relationships in nested categories. Since we have difficulties interpreting “rectangular volumes”, you should not rely on them exclusively. Structure of al-Ṭabarī’s notes (Credit: Sarah Savant) Structure of the Islamic world, according to al-Muqaddasī (Credit: Masoumeh Seydi) Stacked bar chart: Displays the composition of different categories over time or across other categorical dimensions. We also saw above that these are quite difficult to read (only the layer at the very bottom is readable because of the same baseline; other layers will be very difficult to compare to each other). It is better to split your data into simpler objects and use a different visualization. Credit: Peter Verkinderen Radar chart, also known as a spider chart or a web chart, is a graphical representation of multivariate data. It is used to display multiple variables as points or lines emanating from a central point, with each variable represented by a different axis that is equally spaced around the center. The variables are plotted on these axes as points or lines that form a polygon, with the shape of the polygon reflecting the values of the variables. A radar chart is a much better alternative to a stacked bar chart and a TreeMap, both of which are difficult to read and do not allow for efficient comparison. Additionally, we can plot multiple objects on a radar chart for comparison. 8.2.3.2 Continuous data (interval or ratio). In most cases this means data that changes over time. For example, the number of people in specific places over time; the change over time in activities, in which individuals are involved; the general number of people over time, etc. Line chart: Displays trends or patterns in continuous data over time, like in examples above. “Bar” chart from above. Scatter plot: Explores correlations or relationships between two numerical variables. Usually, this chart type is used to check if there is any correlation. For example, with the scatter plot, we can check if there is any correlation between the age at which a person dies and the time when the person was born (if we assume that the living conditions were improving with time, people would live longer); a toy example of a strong correlation in our data will be to plot date of birth vs. date of death. Histogram: Analyzes the distribution and underlying patterns of a continuous variable. For example, we can use a histogram to check the distribution of age values. Box plot: Summarizes and compares the distribution of different groups or categories in continuous data. Box plot would be an even better alternative for checking the distribution of ages. Area chart: Represents the magnitude of continuous data over time, similar to a line chart, but with the area between the line and axis filled. Area charts are usually rather difficult to interpret, but sometimes they work. (This is not quite an area chart, but it is quite similar.) 8.2.3.3 Geographic data Although most visualization of geographical data are often calledmaps, there is rarely a need to use true geographical maps. In most cases we only need a suggestive layout—like shorelines or country borders—to convey sufficient sense of geography in order to make the data that we want to visualize geographically meaningful. It is perhaps best to refer to such “maps” as cartograms. Choropleth map visualizes geographic distribution of data and spatial patterns using color to represent data values in predefined geographic areas. Most commonly, predefined geographical units are confines of administrative units, like countries, states, counties, lands, districts, etc. The most common issue with choropleth maps is that categorical values are represented colors, while areas are represented with space: larger administrative units look and appear larger than smaller units—but their area may be misleading, like in the examples below that show voting patterns in the US. The biggest problem of choropleth maps is their unpracticality for pre-modern period, when we simply do not have boundaries to work with. Heatmap represents data in a matrix format, often used to display spatial data as a grid with color-coded cells based on data values. This one can be useful to visualize continuous areas, but may also be quite problematic because of its colors. Bubble map displays geographic data using circles or bubbles of varying sizes to represent data values at specific locations. This is probably the most common cartogram that you will be using. Network map can be viewed as an extension of the bubble map, where we also add connections between bubbles. 8.2.3.4 Relational data. In most cases this means some form of network data. Or, to put it differently, data that can be represented as a network. For example, in our personaje table we have individuals, whom we can connect together in a variety of ways: for example, we can bring individuals into a network based on the overlap between places that they visited and time periods that they shared (in this cases we can assume that they had an opportunity to meet); we can also aggregate places into a network, based on the numbers of individuals who visited the same places. Network graph: Visualizes complex relationships or connections between entities as nodes and edges in a graph. social networks: networks of individuals, which can be constructed in a wide variety of ways; here is an example of a social network of scholars in Middle Eastern Studies, based on MESA conference participation [[https://maximromanov.github.io/projects/mesa_network/]] geographical networks: visualization of connectedness of places based on some data: 1) routes that connect places; 2) people that connect places — different types of connections: a) connections among places based on the movement of people among those places &gt; more people, stronger the connection; b) migrations into/from specific cities (sort of ego-networks); etc. Matrix chart: Displays relationships between multiple categories or variables in a matrix format, often using color or symbols to represent data values. This can be used to visualize connections among cities (based on people movement) can also be represented by the number of people that moved between the pairs of cities: source, target, weight; 8.2.3.5 Complex Data, Complex Charts As you progress with your use of R (or any other programming language), you are likely to design some research experiments that will be unusual and will require some creative ways of visualizing results. For example, the following are a few graphs from the area of text analysis research where some creative thinking was necessary in order to display results in a powerful way. Examples of text reuse in Taʾrīḫ al-islām of al-Ḏahabī (d. 748/1347). [TOP] Identified text reuse from Dalāʾil al-nubuwwaŧ of al-Bayhaqī (d. 458/1066); [MIDDLE] Identified authorial signals of al-Ḏahabī (d. 748/1347) and of al-Bayhaqī (d. 458/1066) in Taʾrīḫ al-islām. [BOTTOM] Identified authorial signals of al-Ḏahabī (d. 748/1347) and of al-Bayhaqī (d. 458/1066) in Taʾrīḫ al-islām—authorial models are based on works of both authors other than Taʾrīḫ al-islām and Dalāʾil al-nubuwwaŧ (The MIDDLE AND BOTTOM graphs are produced with R package Stylo). Cumulative text reuse in Taʾrīḫ al-islām of al-Ḏahabī (d. 748/1347). Another example of text reuse visualization: Geographical data over time (based on Taʾrīḫ al-islām of al-Ḏahabī (d. 748/1347)): 8.3 R Built-in Graphics Functions: Quick Overview R provides a variety of built-in graphics functions for creating different types of plots and visualizations. These functions belong to the base R graphics system (package graphics, which is always automatically loaded) and provide a simple yet flexible way to create various plots and visualizations. plot() is a versatile, built-in function used to create various types of plots and visualizations based on the input data. It is part of the base R graphics system and provides a simple, yet flexible way to visualize relationships between variables or explore the distribution of a dataset. Additionally, the base R graphics package includes the following main functions: hist(): Creates a histogram of a given vector of values. barplot(): Creates a bar plot for categorical or discrete data. boxplot(): Creates a box plot to visualize the distribution and summary statistics of a dataset. pie(): Creates a pie chart to represent proportions or percentages. pairs(): Creates a scatterplot matrix for multiple continuous variables. dotchart(): Creates a dot chart (also known as a Cleveland dot plot) for comparing values across categories. curve(): Plots a mathematical function or expression as a curve. contour(): Creates contour plots for displaying three-dimensional data in two dimensions. image(): Creates a grid of colored or gray rectangles based on the values of a matrix. heatmap(): Creates a heatmap to visualize a matrix or a two-dimensional dataset with color gradients. mosaicplot(): Creates a mosaic plot to visualize the relationship between two or more categorical variables. We will not go over these functions. You can check documentation on them, using help function like ?plot, ?hist, etc. I strongly recommend you to take a look at the examples: the best way is to click on link “Run examples” — it will give you a nice overview of what is available. (Personally, I frequently use plot() and hist() to get a quick insight into a dataset; I will give examples below.) 8.4 Tidyverse Graphs: ggplot2 ggplot2 is an R package for creating advanced, customizable data visualizationsIt is part of the tidyverse ecosystem and excels at creating complex, multi-layered graphics with ease. This package was developed by Hadley Wickham and is based on the “Grammar of Graphics”, a framework proposed by Leland Wilkinson. The idea is to describe visualizations as a combination of components or layers that can be built up step-by-step. This approach provides a consistent and flexible way to create a wide range of graphics. ggplot2 became the library for data visualization and is probably the best out there. because of its popularity, additional libraries have been developed to extend the functionality of ggplot2 (ggthemes, ggraph, ggridges, ggmap, ggrepel, gganimate, etc.). 8.4.1 Grammar of Graphics The Grammar of Graphics is a framework for describing and constructing statistical graphics systematically and consistently. It was proposed by Leland Wilkinson in his book “The Grammar of Graphics” published in 1999. The main idea behind this framework is to break down a graphic into distinct components or layers, allowing for a more structured and modular approach to creating visualizations. The Grammar of Graphics is built on the idea that any statistical graphic can be expressed as a combination of the following components: Data: The dataset used for the visualization, typically a data frame or a tibble. In GoG, the data is considered the starting point, and all other elements are built upon it. Aesthetics: Aesthetics are the mappings between variables in the dataset and visual properties such as position, color, size, or shape. Aesthetics define how the data is represented in the plot and are crucial in determining the type and appearance of the graphic. Geoms: Geoms, short for geometric objects, are the visual elements that represent data points in the plot. Common geoms include points, lines, bars, and polygons. Geoms are responsible for the actual data representation on the graphic. Scales: Scales control the transformations applied to the data and aesthetics. They define the mapping between the data values and the visual properties (e.g., continuous to color gradients, discrete to shapes). Scales also handle the creation of legends and axes. Coordinate systems: Coordinate systems define the space in which the plot is drawn. The most common coordinate system is the Cartesian coordinate system (x and y axes), but other systems like polar coordinates can also be used to create different types of plots. Facets: Faceting is a technique used to create multiple small plots for each level of a categorical variable. It is particularly useful for exploring and comparing data across different subgroups or conditions. Stat transformations: Statistical transformations summarize or transform the data before it is plotted. For example, calculating a linear regression line or binning data for histograms. Themes: Themes control the visual appearance of non-data elements in the plot, such as the background, gridlines, text, and legends. They allow for customization and fine-tuning of the overall appearance of the graphic. By combining these components, the Grammar of Graphics provides a flexible and systematic way to create a wide range of graphics. Let’s take a close look at a complex example with many elements included. (In most cases you probably will not need so many elements/layers.) First, we need to load and prepare some data that we will need for graphing. The following data will include the chronological distribution (by 50 hijrī year periods) of individual in top 10 Andalusian cities. Again, for simplification, we group individuals by the years of their death. library(tidyverse) # ggplot2 will be loaded with this command ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.3 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors PUA &lt;- readRDS(&quot;data/PUA_allDataTables_asList.rds&quot;) # get readable names of places lugarNombres &lt;- PUA$lugar %&gt;% select(idLugar, nombre, nombre_castellano) # create personajeLite personajeLite &lt;- PUA$personaje %&gt;% select(idPersonaje, idFamilia, nombreA, nacimiento, muerte, edad) %&gt;% mutate(nacimiento = na_if(nacimiento, 0), muerte = na_if(muerte, 0), edad = na_if(edad, 0)) # count people in places lugar &lt;- PUA$personaje_lugar %&gt;% select(idLugar, idPersonaje, idRelacion) %&gt;% left_join(personajeLite, by = c(&quot;idPersonaje&quot; = &quot;idPersonaje&quot;)) %&gt;% select(-nombreA, -nacimiento, -edad) %&gt;% mutate(century = plyr::round_any(muerte, 50, f = ceiling)) %&gt;% left_join(lugarNombres) %&gt;% filter(!is.na(century)) ## Joining with `by = join_by(idLugar)` # sleect the top 10 paces lugarTop &lt;- lugar %&gt;% group_by(idLugar) %&gt;% summarize(total = n()) %&gt;% arrange(desc(total)) %&gt;% top_n(20, wt = total) # get only cities lugarTop10 &lt;- lugarTop %&gt;% left_join(lugarNombres) %&gt;% filter(!nombre_castellano %in% c(&quot;Oriente&quot;, &quot;al-Andalus&quot;, &quot;La Meca&quot;, &quot;El Cairo&quot;, &quot;Marraquech&quot;, &quot;Ceuta&quot;, &quot;Fez&quot;)) %&gt;% top_n(10, wt = total) ## Joining with `by = join_by(idLugar)` # creating the summary lugarSummary &lt;- lugar %&gt;% group_by(idLugar, century) %&gt;% summarize(individuals = n()) %&gt;% filter(idLugar %in% lugarTop10$idLugar) %&gt;% left_join(lugarNombres) %&gt;% mutate(label = paste0(nombre_castellano, &quot; (&quot;, nombre, &quot;)&quot;)) %&gt;% ungroup() %&gt;% select(label, individuals, century) ## `summarise()` has grouped output by &#39;idLugar&#39;. You can override using the ## `.groups` argument. ## Joining with `by = join_by(idLugar)` knitr::kable(lugarSummary) label individuals century Córdoba (قرطبة) 2 100 Córdoba (قرطبة) 8 150 Córdoba (قرطبة) 25 200 Córdoba (قرطبة) 60 250 Córdoba (قرطبة) 160 300 Córdoba (قرطبة) 329 350 Córdoba (قرطبة) 538 400 Córdoba (قرطبة) 416 450 Córdoba (قرطبة) 195 500 Córdoba (قرطبة) 242 550 Córdoba (قرطبة) 199 600 Córdoba (قرطبة) 179 650 Córdoba (قرطبة) 18 700 Córdoba (قرطبة) 8 750 Córdoba (قرطبة) 3 800 Zaragoza (سرقسطة) 2 100 Zaragoza (سرقسطة) 4 200 Zaragoza (سرقسطة) 4 250 Zaragoza (سرقسطة) 26 300 Zaragoza (سرقسطة) 17 350 Zaragoza (سرقسطة) 14 400 Zaragoza (سرقسطة) 55 450 Zaragoza (سرقسطة) 56 500 Zaragoza (سرقسطة) 57 550 Zaragoza (سرقسطة) 25 600 Zaragoza (سرقسطة) 11 650 Murcia (مرسية) 2 300 Murcia (مرسية) 1 350 Murcia (مرسية) 7 400 Murcia (مرسية) 23 450 Murcia (مرسية) 34 500 Murcia (مرسية) 92 550 Murcia (مرسية) 111 600 Murcia (مرسية) 106 650 Murcia (مرسية) 33 700 Murcia (مرسية) 16 750 Murcia (مرسية) 2 800 Játiva (شاطبة) 4 450 Játiva (شاطبة) 27 500 Játiva (شاطبة) 64 550 Játiva (شاطبة) 73 600 Játiva (شاطبة) 47 650 Játiva (شاطبة) 17 700 Játiva (شاطبة) 4 750 Játiva (شاطبة) 1 800 Sevilla (إشبيلية) 3 200 Sevilla (إشبيلية) 2 250 Sevilla (إشبيلية) 14 300 Sevilla (إشبيلية) 22 350 Sevilla (إشبيلية) 34 400 Sevilla (إشبيلية) 146 450 Sevilla (إشبيلية) 65 500 Sevilla (إشبيلية) 134 550 Sevilla (إشبيلية) 210 600 Sevilla (إشبيلية) 281 650 Sevilla (إشبيلية) 50 700 Sevilla (إشبيلية) 12 750 Sevilla (إشبيلية) 2 800 Sevilla (إشبيلية) 1 850 Almería (المرية) 1 300 Almería (المرية) 3 350 Almería (المرية) 3 400 Almería (المرية) 47 450 Almería (المرية) 69 500 Almería (المرية) 124 550 Almería (المرية) 70 600 Almería (المرية) 33 650 Almería (المرية) 19 700 Almería (المرية) 48 750 Almería (المرية) 12 800 Almería (المرية) 1 900 Granada (غرناطة) 1 300 Granada (غرناطة) 9 350 Granada (غرناطة) 6 400 Granada (غرناطة) 19 450 Granada (غرناطة) 39 500 Granada (غرناطة) 144 550 Granada (غرناطة) 173 600 Granada (غرناطة) 174 650 Granada (غرناطة) 74 700 Granada (غرناطة) 139 750 Granada (غرناطة) 55 800 Granada (غرناطة) 5 850 Granada (غرناطة) 7 900 Granada (غرناطة) 2 950 Málaga (مالقة) 2 300 Málaga (مالقة) 1 350 Málaga (مالقة) 5 400 Málaga (مالقة) 20 450 Málaga (مالقة) 23 500 Málaga (مالقة) 37 550 Málaga (مالقة) 91 600 Málaga (مالقة) 153 650 Málaga (مالقة) 56 700 Málaga (مالقة) 62 750 Málaga (مالقة) 22 800 Málaga (مالقة) 2 850 Valencia (بلنسية) 1 250 Valencia (بلنسية) 2 350 Valencia (بلنسية) 10 400 Valencia (بلنسية) 19 450 Valencia (بلنسية) 54 500 Valencia (بلنسية) 103 550 Valencia (بلنسية) 182 600 Valencia (بلنسية) 135 650 Valencia (بلنسية) 16 700 Valencia (بلنسية) 2 750 Valencia (بلنسية) 2 800 Toledo (طليطلة) 7 200 Toledo (طليطلة) 10 250 Toledo (طليطلة) 31 300 Toledo (طليطلة) 30 350 Toledo (طليطلة) 86 400 Toledo (طليطلة) 100 450 Toledo (طليطلة) 96 500 Toledo (طليطلة) 24 550 Toledo (طليطلة) 6 600 Toledo (طليطلة) 6 650 Toledo (طليطلة) 1 700 Toledo (طليطلة) 2 750 Now, let’s prepare some additional data to make our visualization more readable: we will create labels for our x axis. # prepare labels for dates axis AH2CE &lt;- function(AH) { CE &lt;- round(AH - AH/33 + 622) return(CE) } periodsAH &lt;- seq(100, 1000, 200) periodsCE &lt;- AH2CE(periodsAH) xChronLabels &lt;- paste0(periodsAH, &quot;/&quot;, periodsCE) xChronValues &lt;- periodsAH Before we proceed, we need to do one trick that is not really related to the ggplot(), but rather to the ability of R to display Arabic properly in graphs. We need to install and load library ragg (For more details: https://www.tidyverse.org/blog/2021/02/modern-text-features/). The Arabic will still not be displayed correctly in R, but it will in saved filed. Now, we can generate our graph: lugarGraph &lt;- ggplot() + geom_line(data = lugarSummary, aes(x = century, y = individuals), color = &quot;black&quot;, linewidth = 0.5) + labs( title = &quot;Top 10 Andalusian cities over time&quot;, subtitle = &quot;Chronological and geographical distribution of Andalusian scholars&quot;, caption = &quot;Based on “Prosopografía de ulemas de al-Andalus” (MICINN, FFI2010-20428)&quot;, x = &quot;&quot;, y = &quot;&quot;, position = &quot;left&quot; ) + scale_x_continuous(breaks = xChronValues, labels = xChronLabels) + facet_wrap(~ label, ncol = 2, scales = &quot;free_y&quot;) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Amiri&quot;), #panel.background = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), #panel.grid.major.x = element_blank(), #panel.grid.minor.x = element_blank(), axis.ticks = element_line(linewidth = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = &quot;none&quot;, legend.title = element_blank(), ) # print object on the screen (Arabic will not be displayed correctly!) lugarGraph and when we print our chart to screen, we can see something like this (it will look different on you screen, since the chart will be fitted into the available space in RStudio bottom-right section; you can adjust it, or open the chart in a separate window): This graph is actually loaded from a saved file. You can save your graph in the following manner: # saving the chart fileName &lt;- file.path(&quot;PUA_top10_AndalusianCities_OverTime.svg&quot;) ggsave( filename = fileName, plot = lugarGraph, device = &quot;svg&quot;, # can be changed to &quot;png&quot;, &quot;pdf&quot; scale = 1, width = 30, height = 30, units = c(&quot;cm&quot;) ) The format of the file is svg (scalable vector graphic). Since it is a vector format, it has the best quality possible—you can keep zooming in and you will never see pixels. This format, however, is a bit tricky—you can open it in a text editor (since it is XML-based), but not in your usual image editor. It is best for conventional printed publications as well as for online publications. You can also save your graphs into other formats like png (portable graphics format). png is a raster format, so you will need to take care of such parameters like resolution (dpi) to ensure its high quality. (Technically, you can also save it as a PDF, but you will most likely have to deal with lots of issues before you succeed. svg is a much easier solution; you can save a needed graph into a PDF using some free software like Inkscape: open svg, save as pdf.) 8.4.2 Code Breakdown Detailed description of the code. This R code creates a line graph using the ggplot2 package to visualize top 10 Andalusian cities over time. The graph shows how the number of people (scholars mostly, but not exclusively) changed over centuries, effectively telling us when these cities appeared on the cultural map of al-Andalus, how they faired over time, and when they dropped off the cultural map of al-Andalus. Here’s a breakdown of the code: Note that each unit/element of the code is connected to the next one with + (not with %&gt;%, like in tibble processing). lugarGraph &lt;- ggplot() +: This line initializes a ggplot object and assigns it to the variable lugarGraph. It sets the foundation for creating the plot. geom_line(data = lugarSummary, aes(x = century, y = individuals), color = \"black\", linewidth = 0.5) +: This line adds a line layer to the plot. It specifies the data frame lugarSummary as the data source. The aes() function defines the mapping of variables, where century is mapped to the x-axis and individuals is mapped to the y-axis. The line color is set to black, and the linewidth is set to 0.5. labs(...) +: This section sets the plot’s title, subtitle, and caption, as well as removes the x and y axis labels. The title argument specifies the main title of the plot, the subtitle argument provides additional information below the title, and the caption argument adds a caption to the plot. scale_x_continuous(breaks = xChronValues, labels = xChronLabels) +: This line adjusts the x-axis scale. It sets the breaks (ticks) on the x-axis using xChronValues and labels them using xChronLabels. This is how we can customize labels on any of axes. Essentially, you need to provide two vectors of the same length: one with the numbers, and another one is with labels that will sit on those numbers. facet_wrap(~ label, ncol = 2, scales = \"free_y\") +: This line creates a grid of subplots based on the label variable. The ~ label specifies that the plot should be facetted based on the label column in the data. ncol = 2 sets the number of columns in the grid, and scales = \"free_y\" allows each subplot to have its own y-axis scale. You can modify ncol to your liking. If you drop scales = \"free_y\", then all subplots will be using the same scale—this is more convenient when you want to see the largest and the smallest cities, while using individual scales allows one to get a more detailed picture for each subplot. theme_set(theme_minimal())+: This sets the theme of the plot to a minimal style. The theme_minimal() function provides a predefined theme with minimal visual elements. theme(...) section: This section modifies various visual aspects of the plot. It sets the font family to “Amiri” using element_text(family = \"Amiri\"). It also customizes the appearance of the grid lines, axis ticks, legend, and other elements. Notes: - There is a number of predefined themes that you can use for your graphs. You can find them all here, with examples: https://ggplot2.tidyverse.org/reference/ggtheme.html. - Most likely, you do not have Amiri font on your computer. You can install it from here: https://github.com/aliftype/amiri/releases. This is the best freely available Arabic font. in class: the best way to learn how any piece of code works is to try breaking it—try to un/comment lines of code, one by one, regenerate the results to see what changes; you can also try to change some parameters. If it breaks, use CTRL + z / Command + z to return your code to its initial state and try something else. 8.5 Simpler graphs The graphs do not have to be so complicated. We can generate the same graph (with less details, of course) with a much shorter code: ggplot() + geom_line(data = lugarSummary, aes(x = century, y = individuals), color = &quot;black&quot;, linewidth = 0.5) + facet_wrap(~ label, ncol = 2) ggsave( filename = &quot;PUA_top10_AndalusianCities_OverTime_01.svg&quot;, plot = last_plot(), device = &quot;svg&quot;, width = 30, height = 30, units = c(&quot;cm&quot;) ) ggplot() + geom_line(data = lugarSummary, aes(x = century, y = individuals, col = label), linewidth = 0.5) ggsave( filename = &quot;PUA_top10_AndalusianCities_OverTime_02.svg&quot;, plot = last_plot(), device = &quot;svg&quot;, width = 30, height = 15, units = c(&quot;cm&quot;) ) 8.6 Most Relevant Types of Graphs (for Audition Certificates Data) Let’s now take a look at a few examples of most relevant graphs scatter plots: useful for checking for correlations; line charts: useful for plotting change over time; histograms: useful for understanding distribution of values; barcharts: used for categorical data representation; NB: Check examples in the appendix to this section with graphs based on the PUA data for more information on relevant graphs. # data ac_data &lt;- readRDS(&quot;data/AS_data.rds&quot;) ac_metadata &lt;- readRDS(&quot;data/AS_metadata.rds&quot;) # some helping data and functions AH2CEa &lt;- function(AH) { CE &lt;- round(AH - AH/33 + 622) return(CE) } AH2CEb &lt;- function(AH) { CE &lt;- round(AH - AH/33 + 622) AH &lt;- ifelse(AH == 0, 1, AH) final &lt;- paste0(AH, &quot; AH\\n&quot;, CE, &quot; CE&quot;) return(final) } periodsAH &lt;- seq(0, 1400, 100) periodsCEa &lt;- AH2CEa(periodsAH) periodsCEb &lt;- AH2CEb(periodsAH) Let’s prepare dates data: ac_dates &lt;- ac_data %&gt;% filter(attribute_type == &quot;attributes.year&quot;) %&gt;% mutate(attribute_value = as.numeric(attribute_value)) 8.6.1 Histograms How do histograms work? 1) they group values into bins; 2) they draw “bars/columns”, whose height equals the number of values in a bin; This is the most useful and easy to use type when you need to understand the distribution of values: it is much easier to get correct distribution; Disadvantages: hard to do comparative graphs (although freq_poly may help). (Bar charts might seem similar, but they group data categorically.) # VERY SIMPLE ggplot() + geom_histogram(data = ac_dates,aes(x = attribute_value), binwidth = 10) ## Warning: Removed 9 rows containing non-finite values (`stat_bin()`). # PRETTIFIED ggplot() + geom_histogram(data = ac_dates, aes(x = attribute_value), binwidth = 10) + scale_x_continuous(breaks = periodsAH, labels = periodsCEb) + labs(x = &quot;&quot;, y = &quot;&quot;) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Brill&quot;), # face = &quot;italic&quot;, # makes all italic... panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.ticks = element_line(size = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = c(0.175,0.65), legend.title = element_blank(), ) + labs(title = &quot;Chronological Distribution of Audition Certificates (per decade)&quot;, caption = &quot;Data Source: Audition Certificates Platform (2023)&quot;, x = &quot;&quot;, y = &quot;&quot;, family = &quot;Brill&quot;) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## Removed 9 rows containing non-finite values (`stat_bin()`). ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. #ggsave(&quot;./tests/chronological_distribution_AC_per_decade_histogram.jpg&quot;, # plot = last_plot(), width = 400, height = 200, # units = &quot;mm&quot;, dpi = &quot;retina&quot;) # USING FREQPOLY ggplot() + geom_freqpoly(data = ac_dates, aes(x = attribute_value), binwidth = 10) ## Warning: Removed 9 rows containing non-finite values (`stat_bin()`). HW Question: What will change if we change the value of bins? What other histograms can we generate with our data? 8.6.2 Line charts Line charts draw lines between a series of dots. They are a most common type of graphs for chronological distributions, however, they require much more care at the step of data preparation, especially to ensure that missing values are reflected at the final graph. # gaps lost chronologyA &lt;- ac_data %&gt;% filter(attribute_type == &quot;attributes.year&quot;) %&gt;% mutate(attribute_value = as.integer(attribute_value)) %&gt;% mutate(attribute_value = as.integer(ceiling(attribute_value / 10) * 10)) %&gt;% # convert into decades group_by(attribute_value) %&gt;% summarize(count = n()) %&gt;% rename(year = attribute_value, certificates = count) %&gt;% ungroup() ggplot() + geom_line(data = chronologyA, aes(x = year, y = certificates)) ## Warning: Removed 1 row containing missing values (`geom_line()`). # gaps kept dummy &lt;- tibble(year = seq(350, 1400, 10), certificates = 0) chronologyB &lt;- ac_data %&gt;% filter(attribute_type == &quot;attributes.year&quot;) %&gt;% mutate(attribute_value = as.integer(attribute_value)) %&gt;% mutate(attribute_value = as.integer(ceiling(attribute_value / 10) * 10)) %&gt;% # convert into decades group_by(attribute_value) %&gt;% summarize(count = n()) %&gt;% rename(year = attribute_value, certificates = count) %&gt;% add_row(dummy) %&gt;% group_by(year) %&gt;% summarize(count = sum(certificates)) %&gt;% ungroup() #%&gt;% mutate(count = ifelse(count == 0, NA, count)) ggplot() + geom_line(data = chronologyB, aes(x = year, y = count)) ## Warning: Removed 1 row containing missing values (`geom_line()`). # together ggplot() + geom_line(data = chronologyA, aes(x = year, y = certificates), col = &quot;blue&quot;) + geom_line(data = chronologyB, aes(x = year, y = count), col = &quot;darkgreen&quot;) + geom_segment(data = chronologyB, aes(x = year, xend = year, y = 0, yend = count), col = &quot;red&quot;, linetype = 3) ## Warning: Removed 1 row containing missing values (`geom_line()`). ## Removed 1 row containing missing values (`geom_line()`). ## Warning: Removed 1 rows containing missing values (`geom_segment()`). # prettified ggplot() + geom_step(data = chronologyB, aes(x = year, y = count))+ scale_x_continuous(breaks = periodsAH, labels = periodsCEb) + labs(x = &quot;&quot;, y = &quot;&quot;) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Brill&quot;), # face = &quot;italic&quot;, # makes all italic... panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.ticks = element_line(size = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = c(0.175,0.65), legend.title = element_blank(), ) + labs(title = &quot;Chronological Distribution of Audition Certificates (per decade)&quot;, caption = &quot;Data Source: Audition Certificates Platform (2023)&quot;, x = &quot;&quot;, y = &quot;&quot;, family = &quot;Brill&quot;) ## Warning: Removed 1 row containing missing values (`geom_step()`). #ggsave(&quot;./tests/chronological_distribution_AC_per_year.jpg&quot;, # plot = last_plot(), width = 400, height = 200, # units = &quot;mm&quot;, dpi = &quot;retina&quot;) HW questions: Generate graphs for 20 year periods; for 50 year periods; Generate graphs for different geographical locations; 8.6.3 Scatterplots Scatterplots are used to plot relationships between two variables in order to see if there is any corelation between them. Let’s take a look at how it works on a completely different example, using mtcars standard dataset. ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point(color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + ggtitle(&quot;Scatterplot of MPG vs Car Weight&quot;) + xlab(&quot;Weight (1000 lbs)&quot;) + ylab(&quot;Miles per Gallon&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; In this graph we compare weights of cars and their mileage per gallon (i.e. how many miles they can run on about 4 liters of gas). Each dot represents a car (a specific make and model) and it is plotted using its weight (x-axis) and its mileage (y-axis). Even without the red line, which is meant to represent the trend in data, you can see that the heavier the car, the shorter the distance it can run per gallon. The graph would be usually interpreted in the following manner: This scatterplot visually represents the relationship between the weight of the cars and their fuel efficiency. The points show the actual data, while the red line represents the best linear fit, illustrating the general trend. In this case, the negative slope of the line indicates a negative correlation, suggesting that as the weight of the car increases, its fuel efficiency (miles per gallon) tends to decrease. Now, let’s try with our data: check if the number of participants was increasing with time? I.e., in the later period these groups were larger than in earlier; check if there is some correlation between the day of the week and the number of participants; ac_participants &lt;- ac_data %&gt;% filter(type == &quot;person&quot;) %&gt;% group_by(AS_ID) %&gt;% summarize(count = n()) # year Vs participants ac_participants_year &lt;- ac_data %&gt;% filter(attribute_type == &quot;attributes.year&quot;) %&gt;% mutate(attribute_value = as.numeric(attribute_value)) %&gt;% left_join(ac_participants) %&gt;% # REMOVE UNDATES certificates: losing about 200 certificates filter(!is.na(attribute_value)) ## Joining with `by = join_by(AS_ID)` ggplot(ac_participants_year, aes(x = attribute_value, y = count)) + geom_point(color = &quot;blue&quot;, alpha = 0.25) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + scale_x_continuous(breaks = periodsAH, labels = periodsCEb) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Brill&quot;), # face = &quot;italic&quot;, # makes all italic... panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.ticks = element_line(size = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = c(0.175,0.65), legend.title = element_blank(), ) + labs(title = &quot;Year Vs Participants&quot;, caption = &quot;Data Source: Audition Certificates Platform (2023)&quot;, x = &quot;year AH&quot;, y = &quot;# of participants&quot;, family = &quot;Brill&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ac_participants_year_filt &lt;- ac_participants_year %&gt;% filter(attribute_value &gt;= 500) %&gt;% filter(attribute_value &lt;= 700) %&gt;% filter(count &lt;= 100) ggplot(ac_participants_year_filt, aes(x = attribute_value, y = count)) + geom_point(color = &quot;blue&quot;, alpha = 0.25) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + scale_x_continuous(breaks = periodsAH, labels = periodsCEb) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Brill&quot;), # face = &quot;italic&quot;, # makes all italic... panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.ticks = element_line(size = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = c(0.175,0.65), legend.title = element_blank(), ) + labs(title = &quot;Year Vs Participants&quot;, caption = &quot;Data Source: Audition Certificates Platform (2023)&quot;, x = &quot;year AH&quot;, y = &quot;# of participants&quot;, family = &quot;Brill&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # day Vs participants #unique(ac_participants_dow$attribute_value) DOWtibble &lt;- tibble( attribute_value = c(&quot;Yawm al-ahad&quot;, &quot;Yawm al-ithnayn&quot;, &quot;Yawm al-thalatha&quot;, &quot;Yawm al-arba&#39;a&quot;, &quot;Yawm al-khamis&quot;, &quot;Yawm al-juma&quot;, &quot;Yawm al-sabt&quot;), attribute_label = c(&quot;1. yawm al-aḥad&quot;, &quot;2. yawm al-iṯnayn&quot;, &quot;3. yawm al-ṯulaṯāʾ&quot;, &quot;4. yawm al-arbiʿāʾ&quot;, &quot;5. yawm al-ḫamīs&quot;, &quot;6. yawm al-jumʿaŧ&quot;, &quot;7. yawm al-sabt&quot;), attribute_index = c(1,2,3,4,5,6,7) ) ac_participants_dow &lt;- ac_data %&gt;% filter(attribute_type == &quot;attributes.weekday&quot;) %&gt;% left_join(ac_participants) %&gt;% # REMOVE UNDATES certificates: losing about 200 certificates filter(!is.na(attribute_value)) %&gt;% left_join(DOWtibble) %&gt;% filter(count &lt;= 100) ## Joining with `by = join_by(AS_ID)` ## Joining with `by = join_by(attribute_value)` ggplot(ac_participants_dow, aes(x = attribute_index, y = count)) + geom_point(color = &quot;blue&quot;, alpha = 0.25, position = position_jitter(width=0.25, height=0.25)) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + scale_x_continuous(breaks = DOWtibble$attribute_index, labels = DOWtibble$attribute_label) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Brill&quot;), # face = &quot;italic&quot;, # makes all italic... panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.ticks = element_line(size = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = c(0.175,0.65), legend.title = element_blank(), ) + labs(title = &quot;Day of the week Vs Participants&quot;, caption = &quot;Data Source: Audition Certificates Platform (2023)&quot;, x = &quot;day of the week&quot;, y = &quot;# of participants&quot;, family = &quot;Brill&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## perhaps, types of books would be more relevant?! 8.6.4 Bar charts Bar chart represent values per category. For example, we can check how many sessions we have per day of the week. ac_participants_dow &lt;- ac_data %&gt;% filter(attribute_type == &quot;attributes.weekday&quot;) %&gt;% left_join(ac_participants) %&gt;% # REMOVE UNDATES certificates: losing about 200 certificates filter(!is.na(attribute_value)) %&gt;% left_join(DOWtibble) ## Joining with `by = join_by(AS_ID)` ## Joining with `by = join_by(attribute_value)` ac_participants_per_dow &lt;- ac_participants_dow %&gt;% group_by(attribute_label, attribute_index) %&gt;% summarize( sessions = n(), people_total = sum(count), participants_mean = mean(count), participants_median = median(count) ) %&gt;% arrange(attribute_index) ## `summarise()` has grouped output by &#39;attribute_label&#39;. You can override using ## the `.groups` argument. ac_participants_per_dow ## # A tibble: 7 × 6 ## # Groups: attribute_label [7] ## attribute_label attribute_index sessions people_total participants_mean ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1. yawm al-aḥad 1 239 3021 12.6 ## 2 2. yawm al-iṯnayn 2 252 4341 17.2 ## 3 3. yawm al-ṯulaṯāʾ 3 335 5187 15.5 ## 4 4. yawm al-arbiʿāʾ 4 240 3175 13.2 ## 5 5. yawm al-ḫamīs 5 289 3877 13.4 ## 6 6. yawm al-jumʿaŧ 6 348 4166 12.0 ## 7 7. yawm al-sabt 7 282 4341 15.4 ## # ℹ 1 more variable: participants_median &lt;dbl&gt; ggplot(ac_participants_per_dow, aes(x = factor(attribute_label), y = sessions)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;grey&quot;) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Brill&quot;), # face = &quot;italic&quot;, # makes all italic... panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.ticks = element_line(size = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = c(0.175,0.65), legend.title = element_blank(), ) + ggtitle(&quot;Total number of sessions by DOW&quot;) + xlab(&quot;&quot;) + ylab(&quot;people&quot;) ggplot(ac_participants_per_dow, aes(x = factor(attribute_label), y = people_total)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;grey&quot;) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Brill&quot;), # face = &quot;italic&quot;, # makes all italic... panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.ticks = element_line(size = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = c(0.175,0.65), legend.title = element_blank(), ) + ggtitle(&quot;Total number of people by DOW&quot;) + xlab(&quot;&quot;) + ylab(&quot;people&quot;) ggplot(ac_participants_per_dow, aes(x = factor(attribute_label), y = participants_mean)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;grey&quot;) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Brill&quot;), # face = &quot;italic&quot;, # makes all italic... panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.ticks = element_line(size = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = c(0.175,0.65), legend.title = element_blank(), ) + ggtitle(&quot;Mean number of people by DOW&quot;) + xlab(&quot;&quot;) + ylab(&quot;people&quot;) ggplot(ac_participants_per_dow, aes(x = factor(attribute_label), y = participants_median)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;grey&quot;) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Brill&quot;), # face = &quot;italic&quot;, # makes all italic... panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.ticks = element_line(size = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = c(0.175,0.65), legend.title = element_blank(), ) + ggtitle(&quot;Median number of people by DOW&quot;) + xlab(&quot;&quot;) + ylab(&quot;people&quot;) 8.6.5 Alternatives summary(vector) library(gtExtras) summary table with pregenerated stats; ac_participants_per_dow &lt;- ac_participants_dow %&gt;% group_by(attribute_label, attribute_index) %&gt;% summarize( sessions = n(), p_total = sum(count), p_mean = mean(count), p_median = median(count), p_sd = sd(count), p25 = quantile(count, 0.25), p75 = quantile(count, 0.75), ) %&gt;% arrange(attribute_label) %&gt;% select(-attribute_index) ## `summarise()` has grouped output by &#39;attribute_label&#39;. You can override using ## the `.groups` argument. ac_participants_per_dow ## # A tibble: 7 × 8 ## # Groups: attribute_label [7] ## attribute_label sessions p_total p_mean p_median p_sd p25 p75 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1. yawm al-aḥad 239 3021 12.6 8 13.9 5 16 ## 2 2. yawm al-iṯnayn 252 4341 17.2 9 54.8 5 16 ## 3 3. yawm al-ṯulaṯāʾ 335 5187 15.5 8 20.7 5 15 ## 4 4. yawm al-arbiʿāʾ 240 3175 13.2 8 28.7 4 13 ## 5 5. yawm al-ḫamīs 289 3877 13.4 9 16.6 5 15 ## 6 6. yawm al-jumʿaŧ 348 4166 12.0 8 13.4 6 14 ## 7 7. yawm al-sabt 282 4341 15.4 9 19.6 5 17 knitr::kable(ac_participants_per_dow) attribute_label sessions p_total p_mean p_median p_sd p25 p75 1. yawm al-aḥad 239 3021 12.64017 8 13.94332 5 16 2. yawm al-iṯnayn 252 4341 17.22619 9 54.81254 5 16 3. yawm al-ṯulaṯāʾ 335 5187 15.48358 8 20.69566 5 15 4. yawm al-arbiʿāʾ 240 3175 13.22917 8 28.66882 4 13 5. yawm al-ḫamīs 289 3877 13.41522 9 16.60616 5 15 6. yawm al-jumʿaŧ 348 4166 11.97126 8 13.40520 6 14 7. yawm al-sabt 282 4341 15.39362 9 19.56296 5 17 Important: A high standard deviation indicates that the number of persons per meeting varies greatly from the average (mean) on that day. In other words, some meetings may have many attendees, while others have few. A low standard deviation suggests that the number of persons per meeting is more consistent and doesn’t deviate much from the average on that day. Days with a lower standard deviation would imply more uniform attendance across meetings. 8.7 Appendices 8.7.1 Appendix 1: Most useful graphs with examples Let’s look as the examples of most useful graphs. (We will leave maps and networks for later, since they are more complicated and require preparing data in a specific manner.) introduce major types of graphs; give examples; create “simple” graphs: students’ task #1 is to make them detailed and fully annotated; 8.7.1.1 Scatter plots Let’s try something very simple. First, the correlation between the date of birth and the date of death—this is a silly example, because we do know that the later one is born, the later one dies; but it will show you how a correlation looks visually. Remember, you can save it into a file with: ggsave(\"PUAR_scatterplot01.png\", w = 6, h = 5) ggplot(personajeLite) + geom_point(aes(x = nacimiento, y = muerte)) Note: this can also be done like this: you supply the data into ggplot(), and then simply use aes() with the geom. This approach is used more commonly, but the one above is more convenient when you may want to combine different datasets into one graph. this is particularly useful with maps. ggplot(personajeLite) + geom_point(aes(x = nacimiento, y = muerte)) Now, let’s try something more interesting. Is there a correlation between the age and the date of death. In other words, do people live longer in later periods? ggplot() + geom_point(data = personajeLite, aes(x = nacimiento, y = muerte)) From the graph below, we can say is that there is no correlation between these two variables. At least not in our PUA data. Now, let’s try something else. Do people who travel a lot live longer than those who do not travel much, or at all? In other words, we need to build a scatter plot between the number of places that an individual visited and their ages when they died. We need to prepare data. lugares_visitados &lt;- PUA$personaje_lugar %&gt;% group_by(idPersonaje) %&gt;% summarize(lugaresVisitados = n()) datos_para_el_grafico &lt;- personajeLite %&gt;% left_join(lugares_visitados, by = join_by(idPersonaje)) %&gt;% select(idPersonaje, lugaresVisitados, edad) %&gt;% filter(!is.na(edad)) %&gt;% filter(!is.na(lugaresVisitados)) We only have c. 1,920 individuals with the data, which is only about 15% of all data. But graphing is now easy: lugares_y_edad &lt;- ggplot(datos_para_el_grafico) + geom_point(aes(x = edad, y = lugaresVisitados), alpha = 0.2) You can add jitter parameter to avoid overplotting: ggplot(datos_para_el_grafico) + geom_point(aes(x = edad, y = lugaresVisitados), position = position_jitter(width=0.5, height=0.5), alpha = 0.2) HW: add detailed annotation to the graph, similar to the main example above. Note: plot() can be quite useful for quick scatter plots. The code below shows how the same graphs (although they will look slightly different) can be produced with plot(). Try that. plot(x = personajeLite$edad, y = personajeLite$muerte) plot(x = personajeLite$nacimiento, y = personajeLite$muerte) 8.7.1.2 2. Line charts We already have an example of a line chart. ggplot() + geom_line(data = lugarSummary, aes(x = century, y = individuals), color = &quot;black&quot;, linewidth = 0.5) + facet_wrap(~ label, ncol = 2) HW: create a fully-annotated chronological chart for top 10 professions (actividad); create two fully-annotated charts for top 10 professions in two cities of your choice; 8.7.1.3 3. Bar charts Let’s graph bars of individuals in the top 10 Andalusian cities. We will need to do some data processing though. lugarPersonaje &lt;- lugarSummary %&gt;% group_by(label) %&gt;% summarize(personaje = sum(individuals)) ggplot(lugarPersonaje) + geom_col(aes(label, personaje)) HW: - add annotation to the cities graph; - create a fully-annotated bar chart for 10 most common professions - for the entire al-Andalus; - and for the top 10 cities; 8.7.1.4 4. Histograms Histograms are best for understanding the distribution of values. For example, ages (edad) ggplot() + geom_histogram(data = personajeLite, aes(x = edad)) ggplot() + geom_histogram(data = personajeLite, aes(x = edad), binwidth = 10) Note: this is where hist() may also be very useful: hist(personajeLite$edad) HW: create a fully annotated histogram of the distribution of ages; create a fully annotated histogram for the number of places visited by each individual in the PUA database; 8.7.1.5 5. Wordclouds Everyone seems to want to use wordclouds, but wordclouds are most commonly misused. Any issues with the following wordcloud? (Think of it this way: well-designed visualizations use position, color, shape, tile, and size in informative ways — ideally, each of them must encode/convey some information.) Better variants with R (unfortunately, does not work with Arabic text.). First, prepare data: places and frequencies (sizes). lugarFreqs &lt;- lugar %&gt;% group_by(idLugar) %&gt;% summarize(total = n()) %&gt;% left_join(lugarNombres, by = join_by(idLugar)) The graphing is somewhat different, since we need to use a different library—not ggplot()—and a different way of saving graphs. library(wordcloud) library(RColorBrewer) set.seed(786) # for replication png(file = &quot;PUAR_wordcloud_Castellano.png&quot;, width = 10, height = 10, res = 300, units = &quot;cm&quot;) wordcloud(words = lugarFreqs$nombre_castellano, freq = lugarFreqs$total, min.freq = 1, max.words = 200, random.order = FALSE, rot.per = 0, colors=brewer.pal(8, &quot;Dark2&quot;), family = &quot;Brill&quot;) dev.off() As you can see below, it does not work for Arabic. I will get back to you if I find a solution… Update! I found the way to make it work (thanks to Till Grallert). We need to use the same library ragg as we did before for ggplot graphs, but here we need to use a slightly different command to save our graph: agg_png() instead of png() library(ragg) set.seed(786) # for replication agg_png(file = &quot;PUAR_wordcloud_arabe.png&quot;, width = 10, height = 10, res = 300, units = &quot;cm&quot;) wordcloud(words = lugarFreqs$nombre, freq = lugarFreqs$total, min.freq = 1, max.words = 200, random.order = FALSE, rot.per = 0, colors=brewer.pal(8, &quot;Dark2&quot;), family = &quot;Amiri&quot;) invisible(dev.off()) 8.7.2 Appendix 2: When a “Simple” Table May Suffice The gt package is all about making it simple to produce nice-looking display tables. Display tables? Well yes, we are trying to distinguish between data tables (e.g., tibbles,data.frames, etc.) and those tables you’d find in a web page, a journal article, or in a magazine. Such tables can likewise be called presentation tables, summary tables, or just tables really. For more: https://gt.rstudio.com/articles/intro-creating-gt-tables.html More links of on gt() and gtExtra(): - https://gt.rstudio.com/articles/intro-creating-gt-tables.html - Latest update: https://posit.co/blog/new-formatting-functions-in-gt-0-9-0/ - Extension package gtExtras: https://jthomasmock.github.io/gtExtras/ - A digital “book” on creating gt tables: https://gt.albert-rapp.de/ - This is primarily for your records; just flip through its digital pages so that you have an idea what kind of things you can create with this library. When you have a specific project or idea, you can always come back to that book and work your way through relevant chapters to present your data in a similar manner. As a quick example, let’s summarize our personajeLite table: personajeLite &lt;- PUA$personaje %&gt;% select(idPersonaje, idFamilia, nombreA, nacimiento, muerte, edad) %&gt;% mutate(nacimiento = na_if(nacimiento, 0)) %&gt;% mutate(muerte = na_if(muerte, 0)) %&gt;% mutate(edad = na_if(edad, 0)) %&gt;% mutate(idFamilia = na_if(idFamilia, 0)) %&gt;% mutate(muerteCE = ifelse(is.na(muerte), NA, round(muerte - muerte/33 + 622))) Now, we can get a quick and nice-looking summary in the following manner: library(gt) library(gtExtras) gt_plt_summary(personajeLite) 8.7.3 Appendix 3: Interactive Graphs If you want to create an interactive graph, you can use plotly library —it can add some interactivity to a ggplot graph (https://plotly.com/ggplot2/); or 2) you can use a slightly different syntax to create plotly graphs directly (https://plotly.com/r/). Example: a Genre Classification Experiment on OpenITI texts, https://maximromanov.shinyapps.io/adhfais_app2/ 8.7.4 Appendix 4: Checklist for Creating Visualizations Creating effective visualizations involves taking into account several factors related to human perception and design principles. Here are some key issues to consider when designing charts, graphs, or other visualizations: Choose the right chart type: Select the most appropriate chart type that effectively represents the data and relationships you want to convey. Different chart types are better suited for different kinds of data and purposes. Maintain simplicity: Avoid clutter and unnecessary elements in your visualizations. A clean and simple design helps the viewer to focus on the main message of the chart. Use color wisely: Use color to highlight important aspects or to distinguish between different data points or categories. However, avoid using too many colors, which can make the chart confusing. Be mindful of color-blind users and choose color schemes that are easily distinguishable for them. Label axes and provide legends: Clearly label the axes of your chart and provide a legend to explain symbols, colors, or patterns used in the visualization. This helps viewers understand the context and meaning of the data. Use consistent scales: Ensure that the scales of your axes are consistent and appropriate for the data being presented. This makes it easier for viewers to compare values and understand trends. Be cautious with 3D effects: Although 3D charts can look visually appealing, they can also introduce distortions and make it difficult for viewers to accurately perceive the data. Stick to 2D charts whenever possible. Maintain aspect ratio: Choose an appropriate aspect ratio for your chart to prevent distortion and misinterpretation of the data. For example, using a square aspect ratio for a scatter plot can help viewers accurately perceive the relationship between variables. Tell a story: Focus on the main message you want to convey with your visualization. Emphasize the key insights, trends, or relationships within the data to make the chart informative and engaging. Test with your audience: To ensure that your visualization is effective, share it with a sample of your intended audience and gather feedback. This can help you identify any issues or confusion that may arise and refine the visualization accordingly. 8.7.5 Appendix 5: Scientific Notation Sooner or later you will run into numbers that are expressed with scientific notation. Here’s an example of a number in scientific notation and its equivalent representation without scientific notation: Scientific notation: 3.25e+03 Without scientific notation: 3250 In scientific notation, the number is expressed as a product of a coefficient (3.25) and a power of 10 (103). This notation is especially useful for representing very large or very small numbers in a more compact form. In this example, 3.25e+03 means 3.25 times 10 raised to the power of 3, which is equal to 3250. Since such notation will be problematic for most of us, it may be best to switch it off. In R, you can turn off scientific notation by setting the scipen option to a large positive value. This will force R to display numbers in fixed-point notation instead of scientific notation. You can set the scipen option using the options() function: options(scipen = 999) By setting scipen to 999, you’re essentially telling R to avoid using scientific notation unless the number is extremely large or extremely small. To reset the option to its default value, you can set scipen back to 0: options(scipen = 0) Keep in mind that these settings only affect how the numbers are displayed in R, not how they are stored or used in calculations. 8.7.6 Appendix 6: Solutions for PUA Graphs (partial) Solutions are given only to the data problems; you should be able to figure out prettification assignments on your own. 8.7.6.1 Line charts create a fully-annotated chronological chart for top 10 professions (actividad); The code is actually pretty much the same as we used above to graph top 10 Andalusian cities. We just need to swap the tables on places with tables on activities. Keep in mind that we are filtering out people for whom we do not have chronological information, even if those individuals are representing the top 10 professions. This means that you will need to make a note about that in an annotation to the graph and make sure to discuss this in your academic prose (for example: there are X representatives of profession Y, but only Z are shown on the graph (Z%) since others do not have chronological information.). # get readable names of activities actividadNombres &lt;- PUA$actividad %&gt;% select(idActividad, nombre, nombre_castellano) # count people by activities actividad &lt;- PUA$personaje_actividad %&gt;% select(idActividad, idPersonaje) %&gt;% left_join(personajeLite, by = c(&quot;idPersonaje&quot; = &quot;idPersonaje&quot;)) %&gt;% select(-nombreA, -nacimiento, -edad) %&gt;% mutate(century = plyr::round_any(muerte, 50, f = ceiling)) %&gt;% filter(!is.na(century)) # select the top 10 activities actividadTop &lt;- actividad %&gt;% group_by(idActividad) %&gt;% summarize(total = n()) %&gt;% arrange(desc(total)) %&gt;% top_n(10, wt = total) %&gt;% left_join(actividadNombres) # creating the summary actividadSummary &lt;- actividad %&gt;% group_by(idActividad, century) %&gt;% summarize(individuals = n()) %&gt;% filter(idActividad %in% actividadTop$idActividad) %&gt;% left_join(actividadNombres) %&gt;% mutate(label = paste0(nombre_castellano, &quot; (&quot;, nombre, &quot;)&quot;)) %&gt;% ungroup() %&gt;% select(label, individuals, century) actividadSummaryNew &lt;- actividadSummary %&gt;% pivot_wider(names_from = century, values_from = individuals) %&gt;% pivot_longer(!label, names_to = &quot;century&quot;, values_to = &quot;individuals&quot;) %&gt;% mutate(century = as.numeric(century)) Now, graphing. Again, quite the same if compared to the code on top 10 Andalusian cities. actividadGraph &lt;- ggplot() + geom_line(data = actividadSummaryNew, aes(x = century, y = individuals), color = &quot;black&quot;, linewidth = 0.5) + labs( title = &quot;Top 10 Andalusian professions over time&quot;, subtitle = &quot;Chronological and geographical distribution of Andalusian scholars by professions&quot;, caption = &quot;Based on “Prosopografía de ulemas de al-Andalus” (MICINN, FFI2010-20428)&quot;, x = &quot;&quot;, y = &quot;&quot;, position = &quot;left&quot; ) + scale_x_continuous(breaks = xChronValues, labels = xChronLabels) + facet_wrap(~ label, ncol = 2) + # , scales = &quot;free_y&quot; theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Amiri&quot;), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), axis.ticks = element_line(linewidth = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = &quot;none&quot;, legend.title = element_blank(), ) actividadGraph ggsave( filename = file.path(&quot;PUA_top10_AndalusianProfessions_OverTime_01.svg&quot;), plot = actividadGraph, device = &quot;svg&quot;, scale = 1, width = 30, height = 30, units = c(&quot;cm&quot;) ) create two fully-annotated charts for top 10 professions in two cities of your choice; I will limit this to a single city—Cordoba; you should be able to get another city on your own. We already have the code for professions. We can reuse the code from above, but we ned pre-filter our personajeLite, keeping only those with connections to Cordoba. cordobans &lt;- PUA$personaje_lugar %&gt;% filter(idLugar == 4) # you can find that the id of Cordoba is 4 personajeLiteCordoba &lt;- personajeLite %&gt;% filter(idPersonaje %in% cordobans$idPersonaje) Now, the light version of what we did for cities and professions above: # count people by professions actividadCordoba &lt;- PUA$personaje_actividad %&gt;% select(idActividad, idPersonaje) %&gt;% left_join(personajeLiteCordoba, by = c(&quot;idPersonaje&quot; = &quot;idPersonaje&quot;)) %&gt;% select(-nombreA, -nacimiento, -edad) %&gt;% mutate(century = plyr::round_any(muerte, 50, f = ceiling)) %&gt;% filter(!is.na(century)) # creating the summary actividadSummaryCordoba &lt;- actividadCordoba %&gt;% group_by(idActividad, century) %&gt;% summarize(individuals = n()) %&gt;% filter(idActividad %in% actividadTop$idActividad) %&gt;% left_join(actividadNombres) %&gt;% mutate(label = paste0(nombre_castellano, &quot; (&quot;, nombre, &quot;)&quot;)) %&gt;% ungroup() %&gt;% select(label, individuals, century) actividadSummaryCordobaNew &lt;- actividadSummaryCordoba %&gt;% pivot_wider(names_from = century, values_from = individuals) %&gt;% pivot_longer(!label, names_to = &quot;century&quot;, values_to = &quot;individuals&quot;) %&gt;% mutate(century = as.numeric(century)) And, now, the graph: actividadGraphCordoba &lt;- ggplot() + geom_line(data = actividadSummaryCordobaNew, aes(x = century, y = individuals), color = &quot;black&quot;, linewidth = 0.5) + labs( title = &quot;Top 10 Cordoban professions over time&quot;, subtitle = &quot;Chronological distribution of Cordoban scholars by professions&quot;, caption = &quot;Based on “Prosopografía de ulemas de al-Andalus” (MICINN, FFI2010-20428)&quot;, x = &quot;&quot;, y = &quot;&quot;, position = &quot;left&quot; ) + scale_x_continuous(breaks = xChronValues, labels = xChronLabels) + facet_wrap(~ label, ncol = 2) + # , scales = &quot;free_y&quot; theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Amiri&quot;), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), axis.ticks = element_line(linewidth = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = &quot;none&quot;, legend.title = element_blank(), ) actividadGraphCordoba ggsave( filename = file.path(&quot;PUA_top10_CordobanProfessions_OverTime_01.svg&quot;), plot = actividadGraphCordoba, device = &quot;svg&quot;, scale = 1, width = 30, height = 30, units = c(&quot;cm&quot;) ) 8.7.6.2 Bar charts add annotation to the cities graph; create a fully-annotated bar chart for 10 most common professions for the entire al-Andalus; and for the top 10 cities; I will add only a graph for all professions. The previous solutions should help you to produce bar charts for the top ten cities. (Essentially, you need to pre-filter personajeLite to include only people from the top ten cities; and then to apply the code that we used for the entire al-Andalus.) actividad &lt;- PUA$personaje_actividad %&gt;% select(idActividad, idPersonaje) %&gt;% left_join(personajeLite, by = c(&quot;idPersonaje&quot; = &quot;idPersonaje&quot;)) %&gt;% select(-nombreA, -nacimiento, -edad) %&gt;% group_by(idActividad) %&gt;% summarize(total = n()) %&gt;% arrange(desc(total)) %&gt;% top_n(10, wt = total) %&gt;% left_join(actividadNombres) %&gt;% mutate(label = paste0(nombre_castellano, &quot; (&quot;, nombre, &quot;)&quot;)) %&gt;% ungroup() %&gt;% select(label, total) And now the graph: ggplot(actividad) + geom_col(aes(label, total)) + # geom_col(aes(label, personaje)) coord_flip() + labs( title = &quot;Top 10 Andalusian Professions&quot;, subtitle = &quot;Overall counts for top Andalusian professions&quot;, caption = &quot;Based on “Prosopografía de ulemas de al-Andalus” (MICINN, FFI2010-20428)&quot;, x = &quot;&quot;, y = &quot;&quot;, position = &quot;left&quot; ) + theme_set(theme_minimal())+ theme(text = element_text(family = &quot;Amiri&quot;), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), axis.ticks = element_line(linewidth = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = &quot;none&quot;, legend.title = element_blank()) ggsave( filename = file.path(&quot;PUA_top10_AndalusianProfessions_Total01.svg&quot;), plot = last_plot(), device = &quot;svg&quot;, scale = 1, width = 30, height = 10, units = c(&quot;cm&quot;) ) We can also build a slightly different looking graph. Note: for some reason, on my computer R crashes if I use Amiri font—for this specific graph only. So, I switched to a different font here. Modifications: I have added counts to the labels; bars look a bit more subtle (line + point); data is arranged by frequencies, not alphabetically as in the first example. actividadNew &lt;- actividad %&gt;% mutate(label = paste0(&quot;[&quot;, total, &quot;] &quot;, label)) ggplot(actividadNew) + geom_point(aes(y=reorder(label, desc(total)), x = total)) + geom_segment(aes(y=reorder(label, desc(total)), yend=reorder(label, desc(total)), x=0, xend=total), linewidth=0.5) + labs( title = &quot;Top 10 Andalusian Professions&quot;, subtitle = &quot;Overall counts for top Andalusian professions&quot;, caption = &quot;Based on “Prosopografía de ulemas de al-Andalus” (MICINN, FFI2010-20428)&quot;, x = &quot;&quot;, y = &quot;&quot;, position = &quot;left&quot; ) + theme_set(theme_minimal()) + theme(text = element_text(family = &quot;Brill&quot;), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), axis.ticks = element_line(linewidth = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = &quot;none&quot;, legend.title = element_blank()) ggsave( filename = file.path(&quot;PUA_top10_AndalusianProfessions_Total02.svg&quot;), plot = last_plot(), device = &quot;svg&quot;, scale = 1, width = 30, height = 10, units = c(&quot;cm&quot;) ) 8.7.6.3 Histograms create a fully annotated histogram of the distribution of ages; create a fully annotated histogram for the number of places visited by each individual in the PUA database; Only the second one here. This, in fact, is super easy. Note: using summary() on the vector of values would provide additional details. visits &lt;- PUA$personaje_lugar %&gt;% group_by(idPersonaje) %&gt;% summarize(places = n()) visitors &lt;- personajeLite %&gt;% left_join(visits) %&gt;% select(idPersonaje, places) %&gt;% mutate(places = replace_na(places, 0)) ggplot(visitors) + geom_histogram(aes(places), binwidth = 1) + # the rest is just the layers of prettification labs( title = &quot;Histogram of Visited Places per Person&quot;, subtitle = &quot;The histogram shows the density of travels, as based on data&quot;, caption = &quot;Based on “Prosopografía de ulemas de al-Andalus” (MICINN, FFI2010-20428)&quot;, x = &quot;&quot;, y = &quot;&quot;, position = &quot;left&quot; ) + theme_set(theme_minimal()) + theme(text = element_text(family = &quot;Brill&quot;), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), axis.ticks = element_line(linewidth = 0.5), axis.ticks.length.y = unit(.25, &quot;cm&quot;), axis.ticks.length.x = unit(.25, &quot;cm&quot;), legend.position = &quot;none&quot;, legend.title = element_blank()) ggsave( filename = file.path(&quot;PUA_density_of_travels.svg&quot;), plot = last_plot(), device = &quot;svg&quot;, scale = 1, width = 30, height = 10, units = c(&quot;cm&quot;) ) 8.7.6.4 Wordclouds - Homework ! you cannot annotate these wordclouds in a similar manner. create wordclouds of professions: for the entire al-Andalus; for top ten cities; The solution is for the entire al-Andalus. Wordclouds for the top 10 cities should be easy to get using the main solution. This is actually a great case where you can give a visual comparison to the professional profiles of the cities. (Keep in mind that there is an issue with these graphs here — you might remember that there are several different ways with which ‘jurists’ are described; the data needs to be normalized.) # count people by professions actividadWordcloud &lt;- PUA$personaje_actividad %&gt;% select(idActividad, idPersonaje) %&gt;% left_join(personajeLite, by = c(&quot;idPersonaje&quot; = &quot;idPersonaje&quot;)) %&gt;% select(-nombreA, -nacimiento, -edad) %&gt;% group_by(idActividad) %&gt;% summarize(total = n()) %&gt;% arrange(desc(total)) %&gt;% left_join(actividadNombres) Now, the graphs: # ARABIC WORDCLOUD library(ragg) set.seed(786) # for replication agg_png(file = &quot;PUAR_wordcloud_professions_arabe.png&quot;, width = 10, height = 10, res = 300, units = &quot;cm&quot;) wordcloud(words = actividadWordcloud$nombre, freq = actividadWordcloud$total, min.freq = 2, max.words = 200, random.order = FALSE, rot.per = 0, colors=brewer.pal(8, &quot;Dark2&quot;), family = &quot;Amiri&quot;) invisible(dev.off()) # SPANISH WORDCLOUD library(ragg) set.seed(786) # for replication agg_png(file = &quot;PUAR_wordcloud_professions_castellano.png&quot;, width = 10, height = 10, res = 300, units = &quot;cm&quot;) wordcloud(words = actividadWordcloud$nombre_castellano, freq = actividadWordcloud$total, min.freq = 2, max.words = 200, random.order = FALSE, rot.per = 0, colors=brewer.pal(8, &quot;Dark2&quot;), family = &quot;Amiri&quot;) invisible(dev.off()) Note: There is one important thing to keep in mind about wordclouds. When you save them into a file, make sure to make the graph big enough (width, height), otherwise some words on the sides may be dropped without you knowing. It is safer to save a very large graph and then manually trim it. Below is an example of a wordcloud whose size was too small to fit all words (width = 5, height = 5). "],["an-introduction-to-git-github-and-rstudios-git-integration.html", "9 An Introduction to Git, Github and RStudio’s Git Integration 9.1 Perks of Using Git 9.2 SSH Key 9.3 Local and Remote Repositories 9.4 Basic Git Commands 9.5 Git and Collaboration 9.6 Git and RStudio 9.7 Additional Materials", " 9 An Introduction to Git, Github and RStudio’s Git Integration Objectives: By the end of the session, students will be able to: Understand the concept of version control and the benefits of using Git. Set up a GitHub account and install Git. Initialize and manage a local Git repository. Connect a local repository to a remote GitHub repository. Use RStudio’s Git integration. Why bother? Digital humanists use technologies and computational methods to conduct research and project work in the humanities. Git and GitHub are two truly essential tools for any DH project. Here is the list of main reasons to consider: Version Control: Digital humanists often work on complex projects that evolve over time. With Git, they can maintain different versions of their work and easily revert to previous versions if necessary. If anything is committed (i.e., saved), it practically cannot be lost and one can always go back to one of the saved states of the project. Project members can also start a new branch to experiment with ideas without affecting the main project. Collaboration: Git facilitates collaborative work on a project. It provides a tool with which team members can work on the same project without overriding each other’s work, thanks to its version control system. Transparency and Reproducibility: By hosting their work on GitHub, digital humanists can make their methods and data transparent, which is crucial in academic research. Other researchers can reproduce their work, validate their findings, or build upon their projects. In case you want to keep your project to yourself, GitHub also offers “private” repositories which will be accessible only to you and those you explicitly share it with. Publishing and Dissemination: GitHub Pages offers a way for digital humanists to easily publish their projects online, which is particularly useful for creating project websites or online portfolios. Every GitHub user automatically gets an address like username.github.io to host their website. For example, Maxim Romanov’s old website is https://maximromanov.github.io/; think carefully when you select your username. It makes a lot of sense to use firstNameLastName pattern. GitHub also offers integration with Zenodo, https://zenodo.org/, the official EU open science platform for publishing research projects and research data. Publishing your research data on Zenodo will create a citable version of your data (with DOI number) which will be preserved and made accessible by perhaps the most reliable academic agency. Integration with other Tools: Git can integrate with many other tools digital humanists might use, such as text editors, data visualization tools, digital exhibit software, and more. Preservation and Sustainability: GitHub provides a durable and sustainable platform for preserving digital projects. By hosting their projects on GitHub, digital humanists ensure that their work remains accessible and usable for the long term. See the point on the integration with Zenodo above. Issue Tracking and Task Management: GitHub’s issue tracking feature provides a way to organize tasks and track progress, making it useful for project management. Community and Learning: Published source code on GitHub means that digital humanists can learn from others’ projects and also contribute to projects outside their immediate team. This shared knowledge is a vital resource for skills development and innovation. In sum, Git and GitHub offer digital humanists a set of tools to improve their workflows, make their work more accessible and transparent, and foster collaboration and learning. Materials Needed: Computers with internet access. RStudio installed. GitHub account (students should sign up before the lesson). Git installed. On iOS: SSH key (but recommended for all systems). 9.1 Perks of Using Git Git comes as a command line interface (CLI). As a regular user of your system, you probably use graphical user interfaces (GUI) exclusively. That means, most of your interactions with the system happen through navigation with your mouse cursor, you point the system to what you want it to do (visually/graphically). An alternative to this type of navigating is to describe verbally to the system what you want to do through commands. So after installing Git on your computer, you will not get a new application on your system - there is no icon you can use to point your system to that program. Instead, you need to use your system’s console to work with Git. A console is a program whose type is command line interpreter - it takes written commands and executes these. Depending on your system, your console is called: iOS: Terminal Windows: Bash, PowerShell, cmd Linux: Bash, Shell For example, when you want to open the folder containing your project, with the GUI (your file browser, called Explorer for Windows and Finder for iOS), you just click on the folder and then the files inside that folder will be shown to you. Say your folder is called project and inside that folder you have a text file called abc.txt. You would click on the folder project and then you can see that it contains the file abc.txt. The equivalent for that step in the console is typing: cd project which opens the folder project. And then typing: ls which will list the files in that folder. If you are not used to navigating in the console, there is a trick to avoid this step. Open the folder of your project and do a right click. Somewhere in the pop-up menu, there is an item called something like: ‘Open Terminal’ (Linux) or ‘Git Bash here’ (Windows). For iOS, activate the path bar (in Finder go to options: View &gt; Show Path Bar). It will be shown at the bottom of your Finder window. Right click in this path bar on your project folder and select ‘Open in Terminal’. 9.2 SSH Key SSH key is an alternative authentication method to giving your user name and password. As an iOS user this method is mandatory to connect with your Github account due to security reasons. But it can be used by everyone who does not want to type in their user name and password all the time. SSH key is a pair of two files which have the exact same name but one of the files has the ending .pub. You never give the content without the .pub ending to any service, this file is only your computer and should kept secret - it is very sensible data because everyone who has that can authenticate as you. Think about this file as the key to your home. You do not want to share that with anybody unless you really want them in your home at any time. For the other file, the ending .pub indicates that you can share it publicly. This is the part you will give to the service. So first step is the creation of such a SSH key pair. Github’s documentation will guide you through that process - it differs depending on your system. Do not forget to edit the SSH config file! https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent You are strongly advised to put the SSH key in the hidden folder .ssh and NOT in your project folder. Try to stick to Github’s guide as tight as possible - again, this key is something you need to take care of very well. After you finished the process of setting up your SSH key, we need to add a SSH config file. For Windows, use PowerShell to navigate into your .ssh folder, on Linux or Mac use your bash (the command is the same for all three systems): cd $HOME/.ssh/ Make sure you are in the correct place by typing ls which should list you your SSH key pair. Now, let’s create the config file. In PowerShell: New-Item config -ItemType File On Linux or Mac: touch config Open that config file in an editor of your choice and add the following lines, just replace &lt;username&gt; with your actual Github username (and maybe change the IdentityFile path to your private key). Host github.com HostName github.com User &lt;username&gt; IdentityFile ~/.ssh/id_ed25519 IdentitiesOnly yes Final step, test your setup! In Linux and Mac, just type the following command. On Windows, change back to cmd and enter the following command. ssh -T git@github.com 9.3 Local and Remote Repositories Create a new remote repository and connect it with a local repository. The local repository can be a new, empty repository or a folder where you have your data already but which was not set-up (i.e. initialized) as a git repository yet. Start from creating the remote repository on Github since it will give you all the necessary instructions on how to set-up your local repository with git. 9.3.1 Create new remote repository on Github Go to GitHub and start a new repository there. In the top right corner on your start-page, there is a + button. Click it and the pop-up will show you multiple options. Select New repository. Start with giving your repository a meaningful name. This name should be representative for your project as it will be used to search for your project, as well as for the URL of the project. The URL for the repository will have the following pattern, where is your user name and is the name you are about to give the repository: https://github.com/&lt;user_name&gt;/&lt;project_name&gt; Is your work public or private? Public means everyone can have a look at your files which you put on Github, private means you have to grant access to your repository before someone else can see it. You can make a repository public at any time with just a few clicks but once your repository is out there, it is not as easy to make it private again. After successfully creating the repository, the next screen will give you instructions on how to push your local files to the new remote repository. Change to your files now and go to your local folder which you want to push to the remote repository. Right-click there and select Open Terminal or Git Bash here or whatever your system provides. Your console will open with the working directory set to your current folder. Now you can execute the instructions from Github. Please copy the commands from your Github and not from this tutorial. They will differ a bit for you since this again depends on the system your on. So please refer to the steps provided by Github. First create the README.md file by: echo &quot;#&lt;project_name&gt;&quot; &gt;&gt; README.md The magical transformation from plain folder to repository happens with: git init This command tells the folder to become a Git repository. Next, create a README.md file (capitalizations is important) and add a short introduction/description to your project there. This will be shown to everyone who takes a look at your project. Use git add . or git add * to add all the files from the folder to the git repository. After this, you commit the files with: git commit -m &quot;first commit&quot; Now that you have a local repository, you need to connect it with the remote equivalent by executing: git remote add origin &lt;project_url&gt; Finally, bring your local repository to the remote Github repository by pushing it with git push -u origin main If you go back to Github, you can see the pushed files there. Do not worry if this does not work with the first try. You can repeat the steps, but before you do so make sure that you remove the hidden folder .git. Since it is hidden, you probably need to activate the option to show hidden files and folders in your file browser (Finder, Explorer or whatever your system provides). 9.4 Basic Git Commands Now that you have you repository set-up, you can work with it. Just work with your files as you have always done. When you finished a step in your work, you not only save the file but now you will also do a commit. A commit commemorates the current state of the repository, so others can use that state as well and you can return to it any time. If you mess up any step which follows the commit, you can simply return to the state of the commit. Furthermore, you make that state accessible to all your collaborators and everyone who has access to the remote repository. To do so, go back to your console. Git status: Check which files have been modified by typing git status. This command will list you all the files which have been changed since the last commit. Red files are not marked for the current commit, green files will be commited. Git add: git status probably shows you all the files as red. You can now add the files to your commit by git add . or git add *. This will add all changed files. If you only want to add an individual file, do git add &lt;file&gt; where is the file name. If you run git status again, all files affected by git add are now listed in green. Git commit: Now we tell Git to memorize the repository with all the changed files by git commit -m \"&lt;commit_message&gt;\". Use the commit message to describe the changes or the current state in a few words. Git push: Push the local commit to the remote repository, so this state is accessible to everyone by git push. Git pull: Everyone who wants to continue working with the new state and the latest changes will use git pull and the commit is pulled from the remote repository to their local one. 9.5 Git and Collaboration Before anyone else than you can make changes to your project, you need to invite this person as a collaborator to your project. On Github, navigate to our projects settings. The first item in the settings menu is Collaborators. Click on it and in the next step you have to enter the user name of the person you want to add as a collaborator to your project. This person will then receive an email with an invitation they have to accept. After accepting the invitation, this person can push to your project. You can grand different types of access. If your project is private, you can add persons to the project with only read and pull rights. That means the person can see the project, but is not allowed to make changes to your project with push. 9.6 Git and RStudio RStudio has an integration for Git. To activate the integration, go to: Tools &gt; Global Options. Select Git/SVN. When you start a new project, you can now select Version Controll and check out the repository directly into RStudio. To clone the repository, give the necessary information. You can also define the path as to where that repository is cloned to on your computer. You can now do the status, add, commit, push and pull commands with RStudio. Go to Tools &gt; Version Controll &gt; Commit.... Select the files you want to commit as Staged by ticking the check-box. RStudio will also highlight the changes made in that file. I edited the README.md and all the changes I have done are now listed at the bottom. Do not forget to give a commit message! 9.7 Additional Materials Paul Vierthaler, “A quick Git and Github Tutorial”, a video tutorial https://www.youtube.com/watch?v=YetC-gxgIVY&amp;list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17&amp;index=22; Watch this tutorial. It will help you to go over the new material and pick up a few more useful git &amp; gitHub tricks. A recommended detailed tutorial: https://swcarpentry.github.io/git-novice/ "],["optimization-and-exploratory-data-analysis.html", "10 Optimization and Exploratory Data Analysis 10.1 Optimizations 10.2 Scripts 10.3 Scripts for Automated Workflows 10.4 Code chunks 10.5 Exploratory Data Analysis 10.6 Tasks to discuss", " 10 Optimization and Exploratory Data Analysis 10.1 Optimizations 10.1.1 Data and Functions At the moment we have our data pretty much in the original format and it will require a few steps to load and pre-process some elements so that it becomes suitable for further analyses. We can also prepare some data that we know for sure we are going to use. This is the case when we might also prepare it once and save so that we can reuse it easier. This is where a list is the best data structure to use—each new table can be added as an item to the list and the entire list can ve saved as an RSD object. library(tidyverse) ac_data &lt;- read_delim(&quot;data/AS_data.tsv&quot;, delim = &quot;\\t&quot;) %&gt;% mutate(across(where(is.character), trimws)) ac_metadata &lt;- read_delim(&quot;data/AS_metadata.tsv&quot;, delim = &quot;\\t&quot;) %&gt;% mutate(across(where(is.character), trimws)) ac_texts &lt;- read_delim(&quot;data/AS_texts.tsv&quot;, delim = &quot;\\t&quot;) %&gt;% mutate(across(where(is.character), trimws)) acList &lt;- list(metadata = ac_metadata, data = ac_data, texts = ac_texts) ac_dates &lt;- acList$data %&gt;% filter(attribute_type == &quot;attributes.year&quot;) %&gt;% mutate(attribute_value = as.numeric(attribute_value)) %&gt;% rename(date_year = attribute_value) %&gt;% select(AS_ID, date_year) acList$dates &lt;- ac_dates saveRDS(acList, &quot;data/acData.rds&quot;) HW: Think which subsets of data are most useful to have prepared, then prepare them and add them to the main data list. We are often may need to reuse the same functions and variables again and again. What we can do is to collect those functions into a separate file and load almost like we do with libraries. For example, we can save the code below into a separate file (custom_functions.R) # some helping data and functions AH2CEa &lt;- function(AH) { CE &lt;- round(AH - AH/33 + 622) return(CE) } AH2CEb &lt;- function(AH) { CE &lt;- round(AH - AH/33 + 622) AH &lt;- ifelse(AH == 0, 1, AH) final &lt;- paste0(AH, &quot; AH\\n&quot;, CE, &quot; CE&quot;) return(final) } periodsAH &lt;- seq(0, 1400, 100) periodsCEa &lt;- AH2CEa(periodsAH) periodsCEb &lt;- AH2CEb(periodsAH) We can load these functions in the following manner (keep in mind that you need to give the correct path to the file; this path — or no path, other than the file name itself — means that our file is in the main folder of the project): source(&quot;custom_functions.R&quot;) print(periodsCEb) ## [1] &quot;1 AH\\n622 CE&quot; &quot;100 AH\\n719 CE&quot; &quot;200 AH\\n816 CE&quot; &quot;300 AH\\n913 CE&quot; ## [5] &quot;400 AH\\n1010 CE&quot; &quot;500 AH\\n1107 CE&quot; &quot;600 AH\\n1204 CE&quot; &quot;700 AH\\n1301 CE&quot; ## [9] &quot;800 AH\\n1398 CE&quot; &quot;900 AH\\n1495 CE&quot; &quot;1000 AH\\n1592 CE&quot; &quot;1100 AH\\n1689 CE&quot; ## [13] &quot;1200 AH\\n1786 CE&quot; &quot;1300 AH\\n1883 CE&quot; &quot;1400 AH\\n1980 CE&quot; We can also write a script with all te necessary steps of preparing our data, which will make all the necessary tibbles and variables available. This could be even a better approach than keeping all the data in RDS, especially when we know that the data will be periodically updated, which is the case with the Audition Certificates data. This way our script can prepare data for analyses from from the latest versions of AC initial data. 10.2 Scripts So far we have been working with notebooks, which are convenient in a variety of ways, but they are not necessarily the most optimal way to run your code, especially when you need to run complex tasks and when you need to repeat this process multiple times. This is where scripts are very convenient — preparing data from the latest data files would be a job for a script, for example, as we discussed above. Running scripts: you can open it on RStudio, select everything and click Run button, or, more conveniently you can run it from the command line tool on your computer. The command is usually: Rscript yourScript.R, provided your script is in the same folder from which you are running the command. Advantages of scripts: running a script from the command line takes less resources than using RStudio; it also usually faster; one can run multiple scripts at the same time, which is impossible with RStudio; one can still use RStudio, while a script is running on the command line; 10.3 Scripts for Automated Workflows What we discussed above as a script for preparing data can be categorized as an automated workflow—if your script systematically processes data, performs analysis, and perhaps even generates reports or visualizations without manual intervention, it can be considered an automated workflow. This term emphasizes the efficiency and repeatability of the process. For example, we can design a script (it can also be a notebook), which will perform specific analytical routines on supplied data. For example, we can design a process that will run exploratory data analysis on our data—and we can rerun it on new data anytime it becomes available. 10.4 Code chunks Code chunks are what make all the analyses, but you may not want to include those chunks into reports which you want to share with those who have not use for them. Solution: you can change the settings for each chunk. You can toggle different settings with the gear button at the top right corner of any code chunk. You can also code them manually. For example, {r echo=TRUE, eval=FALSE} will show the code, but will not execute it during knitting; {r echo=FALSE, eval=TRUE} will run the code, but will not show the code chunk in the knitted document. When you include images into your notebook, you can also control the size of the image at the beginning of the code chunk: {r fig.width = 10, fig.height = 3} will generate a plot of 10 inch wide and 3 inch tall. 10.5 Exploratory Data Analysis Exploratory Data Analysis (EDA) is a critical step in the data analysis process, which involves examining and summarizing the main characteristics of a dataset, often with visual methods. The primary objective of EDA is to understand the data, discover patterns and anomalies, and formulate hypotheses. Here are some key aspects of EDA: Understanding the Data Structure: This involves getting familiar with the dataset by looking at the number of rows and columns, types of variables (categorical, numerical), and identifying any missing values. Summary Statistics: Calculating basic statistical measures like mean, median, mode, standard deviation, etc., for each column in the dataset to understand the distribution and central tendencies of the data. Data Cleaning: Identifying and correcting errors or inconsistencies in the data, dealing with missing values, and potentially removing outliers that could skew the analysis. Data Visualization: Using graphs and plots to understand trends, patterns, and relationships within the data. Common visualizations include histograms, scatter plots, box plots, and bar charts. Identifying Patterns and Relationships: Looking for correlations or associations between variables. This can help in understanding the dynamics within the data and can be pivotal for predictive modeling. Formulating Hypotheses: Based on the insights gained, hypotheses can be developed about the data, which might then be tested more rigorously through statistical methods or predictive modeling. Feature Engineering: Creating new variables or modifying existing ones to improve the effectiveness of statistical models. Checking for Assumptions: In the context of applying statistical models or machine learning algorithms, it’s important to check if the data meet the assumptions required for these methods. Documenting Findings: Keeping a record of findings and insights gained during EDA, which can be valuable for future reference and decision-making. Iterative Process: EDA is often an iterative process where the findings lead to more questions and subsequent analysis, refining the understanding of the dataset. EDA is a flexible and context-dependent process. The specific methods and tools used can vary depending on the nature of the data and the goals of the analysis. The key is to remain open to new insights and be prepared to adjust hypotheses and approaches as new information is discovered. 10.6 Tasks to discuss We looked at the number of sessions and participants by the days of the week; we can also check months: are certain months (Ramaḍān?) are more important in this context? we can also try to look at the solar months, perhaps there is some correlation with seasons (too hot or too cold)? here the problem is in the conversion of Hijrī dates to Gregorian dates, but we should be able to get months more or less right. converter: https://www.muqawwim.com/, code: https://github.com/theodore-s-beers/muqawwim; very-hard-but-cool task: 1) find the code for this app; 2) convert it into R code; 3) use it to convert all our dates date; Network potential: We can start by simply counting how many times the same names appear across our dataset; a histograms can help us interpret the numbers; we can try to identify names that looks similar and may point to the same individuals Roles in sessions: We can check what individuals, based on their roles, appear most frequently; Women, children, slaves, etc.: How can we identify different categories of participants? Onomastic analysis: length of the name and the presence of “specific indicators” (bint or umm as an indicator of a woman); Machine learning: classify participants into groups (what groups can we single out?) and then use some machine learning technique to classify other participants Finding biographies of these individuals: we can automatically match these names against some source of biographical data; 10.6.1 Code for onomastic data tokenize names; create frequency list of name elements; manually classify them; importance of normalization: discuss necessary steps names &lt;- acList$data %&gt;% filter(type == &quot;person&quot;) %&gt;% mutate(person.id = paste0(AS_ID, &quot;__&quot;, startIndex)) %&gt;% select(person.id, text, attribute_value) %&gt;% rename(name = text, role = attribute_value) peopleCount &lt;- names %&gt;% group_by(name) %&gt;% summarize(freq = n()) %&gt;% ungroup() "],["class-11.html", "11 Class 11", " 11 Class 11 [subject to updates and corrections] "],["class-12.html", "12 Class 12", " 12 Class 12 [subject to updates and corrections] "],["class-13.html", "13 Class 13", " 13 Class 13 [subject to updates and corrections] "],["class-14.html", "14 Class 14", " 14 Class 14 [subject to updates and corrections] "],["glossary-of-computer-terms.html", "15 Glossary of Computer Terms 15.1 General Computer Terms 15.2 R-Specific Terms 15.3 NLP and Linguistics 15.4 Machine Learning", " 15 Glossary of Computer Terms NB: The idea is to create a detailed glossary in multiple languages to facilitate understanding. Languages: English, German, Arabic, Persian, Turkish, etc. Entries may be supplied by illustration for more efficiency and clarity. 15.1 General Computer Terms file manager: a software application that provides a user interface to manage files and directories. German: Dateimanager; Arabic: مدير الملفات (Mudīr al-mulafāt); Persian: مدیر پرونده (Modir-e Parvandeh); command line: A text-based interface where users input commands to control the computer. German: Befehlszeile; Arabic: سطر الأوامر (satr al-awamir); Persian: خط فرمان (xatt-e farman); path: a reference to a specific location in a file system, typically consisting of a sequence of directories leading to a specific file or directory. German: Pfad; Arabic: مسار (masar); Persian: مسیر (masir); folder: a virtual container within a file system used to store and organize files and other folders. German: Ordner; Arabic: مجلد (majallad); Persian: پوشه (poshe); file: A digital container for storing data or information on a computer system. German: Datei; Arabic: ملف (malaf); Persian: فایل (file); 15.2 R-Specific Terms … 15.3 NLP and Linguistics natural language processing (NLP): The branch of artificial intelligence focused on enabling computers to understand, interpret, and generate human language. German: Natürliche Sprachverarbeitung; Arabic: معالجة اللغة الطبيعية (mu‘ālajat al-lughah al-ṭabī‘īyah); Persian: پردازش زبان طبیعی (pardazesh-e zabān-e tabi’i); ngram: A contiguous sequence of n items (such as letters, words, or symbols) from a given sample of text or speech. German: N-Gramm; Arabic: ن-جرام (n-gram); Persian: ن-گرام (n-gram); token: A sequence of characters in text that represents a unit of meaning, often corresponding to a word or a symbol. German: Token; Arabic: رمز (ramz); Persian: توکن (token); corpus: A large and structured set of texts, often used for linguistic research and natural language processing tasks. German: Korpus; Arabic: مدونة (madwana); Persian: متن‌نگار (matn-negar); corpus linguistics: The study of language based on large collections of “real life” language use stored in corpora. German: Korpuslinguistik; Arabic: علم اللغة المدوني (’ilm al-lughah al-madwani); Persian: زبان‌شناسی متنی (zabān-shenāsi-ye matni); computational linguistics: An interdisciplinary field concerned with the computational aspects of the human language capacity and the application of computational methods to linguistic questions. German: Computerlinguistik; Arabic: اللسانيات الحاسوبية (al-lisāniyāt al-ḥāsūbīyah); Persian: زبان‌شناسی محاسباتی (zabān-shenāsi-ye moḥāsebatī); 15.4 Machine Learning machine learning deep learning optical character recognition (OCR) handwritten tet recognition (HTR) training a model artificial intelligence "],["appendices-1.html", "16 Appendices 16.1 Appendix I: Importance of basic computer skills for humanists", " 16 Appendices 16.1 Appendix I: Importance of basic computer skills for humanists 16.1.1 Importance of basic computer skills for humanists The importance of basic computer skills for historians lies in several key areas: Access to information: Historians rely heavily on accessing various sources of information, such as archives, databases, and digital libraries. Basic computer skills enable historians to efficiently search, navigate, and retrieve relevant documents and data from these sources. Organization and management of research material: Historians often work with vast amounts of data and documents. Basic computer skills help them to efficiently organize, categorize, and manage their research material using tools such as file management systems, bibliographic software, and spreadsheets. Communication and collaboration: In today’s interconnected world, historians need to communicate their research findings effectively with peers, students, and the general public. Basic computer skills enable them to use various communication tools, such as email, video conferencing, and social media, as well as collaborative platforms like Google Docs or Microsoft Teams. Data analysis and visualization: Increasingly, historians are using digital tools to analyze and visualize data to uncover trends, patterns, and relationships within their research. Basic computer skills enable historians to work with software programs, such as Excel or R, for quantitative analysis and visualization of data, helping them to gain new insights and perspectives on historical events and processes. This is closely connected to the notion of “Digital humanities”, which now combines traditional humanities research with digital tools and methods. Basic computer skills allow historians to engage with this interdisciplinary field, exploring innovative ways to analyze, interpret, and present historical data using computational techniques. Presentation and publication: Historians need to present their research findings in various formats, such as papers, presentations, or online publications. Basic computer skills enable them to use word processing, presentation software, and content management systems to create well-structured, visually appealing, and accessible materials. Professional development: As technology continues to advance, historians must stay up-to-date with new tools and methods relevant to their field. Basic computer skills facilitate their ongoing professional development by making it easier to learn and adopt new software, platforms, and digital research methods. In summary, basic computer skills are essential for historians in the digital age. They help to streamline research processes, enable effective communication and collaboration, and open up new possibilities for data analysis and visualization. As a result, historians with strong computer skills are better positioned to uncover new insights and contribute to the ongoing advancement of historical research. 16.1.2 Access to information Access to information is a critical aspect of historical research. Historians need to consult a wide range of sources to gather evidence and build a comprehensive understanding of the past. With the digitization of archives, databases, and libraries, basic computer skills have become increasingly important for historians to access, search, and navigate these sources effectively. Here are some ways basic computer skills can facilitate access to information for historians: Search engines and databases: Basic computer skills enable historians to use search engines like Google, specialized databases like JSTOR or Project MUSE, and online catalogs of libraries and archives to locate relevant books, articles, and primary sources. Knowing how to use advanced search functions and apply filters to refine search results can save time and improve the accuracy of search results. Navigating digital archives and libraries: Many archives and libraries around the world have digitized their collections and made them available online, providing historians with unprecedented access to primary sources. Basic computer skills allow historians to navigate these digital repositories, view high-resolution scans of documents and images, and download or request copies of materials for further analysis. Using metadata and finding aids: Understanding how to work with metadata and finding aids is essential for locating specific documents or collections within digital archives and libraries. Basic computer skills enable historians to interpret metadata, search finding aids, and use controlled vocabularies to locate relevant materials more efficiently. Optical Character Recognition (OCR) and text mining: OCR technology allows historians to convert scanned images of text into searchable and editable documents. Basic computer skills enable historians to use OCR software and apply text mining techniques to search for specific keywords or phrases within large volumes of text, greatly improving the efficiency of their research. Accessing subscription-based resources: Many valuable resources, such as academic journals and databases, require a subscription or institutional access. Basic computer skills help historians to manage their subscriptions, log in to restricted resources, and navigate paywalls to access the information they need. Utilizing online forums and communities: Historians can benefit from participating in online forums, discussion groups, and social media platforms where scholars and experts share resources, knowledge, and insights. Basic computer skills enable historians to join these communities, contribute to discussions, and tap into the collective expertise of their peers. Staying up-to-date with new resources: As new digital resources and tools become available, basic computer skills help historians stay informed about the latest developments in their field. This includes learning about new digital archives, databases, or software that can improve their research process and facilitate access to information. In summary, basic computer skills are essential for historians to effectively access, search, and navigate the vast array of digital resources available today. These skills not only save time and effort but also open up new research possibilities by providing easier access to primary and secondary sources that may have been difficult or impossible to access in the past. 16.1.3 Organization and management of research material Organization and management of research material are crucial skills for historians, as they often deal with a large number of documents, data, and other resources. Effective organization and management of these materials enable historians to work more efficiently and improve the quality of their research. Here are some strategies and tools historians can use to better organize and manage their research material: File organization: Creating a well-structured file organization system is essential for easy access and retrieval of research materials. This can include creating separate folders for different projects or topics, organizing files by date, type, or source, and using clear, descriptive file names. Bibliography/Document management software: Using document management software, such as Zotero, Mendeley, or EndNote, can help historians to collect, organize, and annotate various types of research materials, including articles, books, and websites. These tools also facilitate creating bibliographies and managing citations, making it easier to keep track of sources and references throughout the research process. Note-taking, outlining, writing tools: Effective note-taking is crucial for capturing and organizing key ideas, arguments, and evidence from research materials. Tools like Evernote, OneNote, or Notion allow historians to create and organize notes digitally, making it easier to search, link, and cross-reference information. Outlining tools like Scrivener or OmniOutliner can also help structure and organize research material in a more coherent and accessible way. Obsidian is a great solution for academic writing using the Zettelkasten approach; combining Obsidian with markdown, bibLaTeX, and Pandoc will allow anyone to write most complex research projects and convert the final document in whatever format that one might need to use in different scenarios (html, PDF, MS Word, etc.). Spreadsheets and databases: Historians can use spreadsheets (e.g., Excel, Google Sheets) or database software (e.g., Microsoft Access, Airtable) to manage and organize large amounts of data or quantitative information. These tools enable historians to filter, sort, and analyze data, as well as to visualize trends and patterns within the information. Cloud storage and synchronization: Using cloud storage services like Google Drive, Dropbox, or OneDrive allows historians to store, access, and share research materials across different devices and locations. This ensures that research materials are always up-to-date, secure, and easily accessible, facilitating collaboration with colleagues and students. Version control: Keeping track of changes and revisions to research materials can be challenging, especially when working with collaborators. Version control systems like Git or Subversion enable historians to manage multiple versions of documents and track changes over time, making it easier to collaborate and maintain a clear revision history. Task and project management: Managing research projects often involves coordinating multiple tasks and deadlines. Task and project management tools like Trello, Asana, or Basecamp can help historians to break down complex projects into smaller tasks, set deadlines, and track progress, ensuring that all aspects of the research process are well-organized and on schedule. By effectively organizing and managing research materials, historians can streamline their research process, reduce the risk of losing or overlooking important information, and enhance the overall quality of their work. Utilizing digital tools and strategies for organization and management can greatly improve the efficiency and effectiveness of historical research. 16.1.4 File Organization File organization is essential for managing research materials, projects, and documents effectively. Implementing best practices in file organization can help save time, reduce stress, and improve the overall efficiency of your work. Here are some best practices for file organization: Establish a clear hierarchy: Create a well-structured folder hierarchy based on your needs and workflow. This can include organizing folders by projects, topics, or themes, and then subdividing them into subfolders for specific aspects or components. A clear hierarchy makes it easier to locate and navigate files quickly. Use descriptive and consistent naming conventions: Choose clear, descriptive names for files and folders that convey their contents and purpose. Implement a consistent naming convention, which can include elements like dates, version numbers, and keywords. Consistency makes it easier to search for and identify files. Include dates in file names: Incorporate dates (in a consistent format like YYYY-MM-DD) in file names to easily track when documents were created or modified. This is particularly helpful when dealing with multiple versions or drafts of a document. Use version control: Keep track of different versions of documents by incorporating version numbers in file names or using version control software like Git or Subversion. This practice helps maintain an organized revision history and prevents confusion caused by multiple copies of similar files. Separate ongoing and completed work: Create separate folders for ongoing and completed projects or tasks to keep your workspace organized and focused. This also makes it easier to archive completed work or share it with colleagues. Avoid excessively nested folders: While creating a folder hierarchy is essential, avoid nesting folders too deeply, as this can make it difficult to navigate and locate files. Aim for a balance between simplicity and organization. Regularly review and clean up files: Set aside time to periodically review and clean up your files and folders. This can involve deleting or archiving old files, updating folder structures, and renaming files for better clarity. Backup your files: Develop a regular backup routine to protect your files from data loss due to hardware failures or accidents. Use a combination of local backups (external hard drives, USB drives) and cloud-based storage services (Google Drive, Dropbox, OneDrive) for added security. Use tags or labels: Many operating systems allow you to add tags or labels to files, making it easier to search for and group related files across different folders. This can be particularly useful for organizing research materials that span multiple projects or themes. Document your organization system: Maintain a document or guide that explains your file organization system, naming conventions, and folder hierarchy. This can be helpful for your reference, as well as for colleagues or collaborators who need to navigate your files. By following these best practices for file organization, you can create a more efficient and streamlined workspace, making it easier to locate, access, and manage your files and documents. 16.1.5 Finding your way around the computer 16.1.5.1 Explanation of file system and directories A file system is a method used by an operating system (OS) to organize, store, and manage files and directories (also known as folders) on a storage device, such as a hard drive, SSD, or USB flash drive. The file system controls how data is stored, accessed, and retrieved, enabling users to create, modify, and delete files and directories. Directories (or folders) are containers used to organize files hierarchically, making it easier to manage and navigate files within the file system. A directory can contain other directories, creating a tree-like structure that starts from a root directory. Here is an explanation of file systems and directories using a simple example: Root directory: This is the top-level directory in a file system hierarchy. On Windows, the root directory is usually represented as C: (or another letter for additional drives), while on macOS and Linux systems, it is represented as /. Example: Windows: C:\\ macOS or Linux: / Subdirectories: Subdirectories (or subfolders) are directories that are contained within other directories. They can be nested within each other to create a hierarchical structure that groups related files together. Example: Projects (a top-level directory) History_Project (a subdirectory within the Projects directory) Research (a subdirectory within the History_Project directory) Drafts (another subdirectory within the History_Project directory) File paths: A file path is a representation of the location of a file or directory within the file system hierarchy. It shows the sequence of directories that must be navigated to reach a specific file or folder. Example: Windows: C:\\Projects\\History_Project\\Research\\primary_source.pdf macOS or Linux: /Projects/History_Project/Research/primary_source.pdf File extensions: File extensions are short suffixes (usually three or four characters) added to the end of a file name, preceded by a period. They indicate the file type and determine which programs can open and edit the file. Example: Document: report.docx (Microsoft Word document) Image: photo.jpeg (JPEG image) Spreadsheet: budget.xlsx (Microsoft Excel spreadsheet) Understanding file systems and directories is essential for organizing, managing, and navigating files on a computer. By creating a well-structured directory hierarchy and using clear naming conventions for files and folders, users can easily locate and access the information they need. Notes: An operating system (OS) is a collection of software that manages computer hardware resources and provides a range of services and functions for the computer’s software applications. The OS acts as an intermediary between the user and the computer hardware, making it possible for users to interact with the computer and run various software programs. Some key functions of an operating system include: Managing hardware resources: The OS is responsible for managing the computer’s hardware, such as the CPU, memory, storage devices, and input/output devices like the keyboard, mouse, and display. It allocates resources to different software applications and ensures that they can access the hardware they need to function properly. Task management: The OS is responsible for managing the execution of processes and threads on the computer. It schedules tasks, assigns priority levels, and ensures that processes do not interfere with each other. Memory management: The OS is responsible for managing the computer’s memory, including allocating and deallocating memory for applications and handling memory fragmentation. It also manages virtual memory, which allows the computer to use disk space as additional memory when needed. File system management: The OS provides a file system that organizes and manages files and directories on storage devices. It controls how files are created, accessed, modified, and deleted, and it maintains metadata about the files, such as their location, size, and creation date. User interface: The OS provides a user interface that allows users to interact with the computer and run software applications. This can be a graphical user interface (GUI), which uses windows, icons, menus, and pointers, or a command-line interface (CLI), which uses text-based commands and keyboard input. Security and access control: The OS is responsible for managing user accounts, authentication, and access control, ensuring that only authorized users can access specific resources and applications on the computer. Networking and communication: The OS manages the computer’s networking capabilities, including connecting to the internet, setting up local networks, and facilitating communication between different devices and applications. Some popular operating systems include Microsoft Windows, macOS, Linux, iOS, and Android. Each OS has its own unique features, user interface, and compatibility with specific hardware and software applications. 16.1.6 Common file formats and extensions File formats and extensions are used to identify the type and structure of a file, which helps determine the appropriate software to open, edit, and manipulate the file. Below are some common file formats and extensions, along with the important things to know when using them on Windows and Mac operating systems: Document formats: .doc and .docx (Microsoft Word): Word processing files used for creating text documents, reports, and letters. Microsoft Word is the primary software for opening these files, but other applications like Google Docs, LibreOffice, and Apple Pages can also open and edit them. .pdf (Portable Document Format): A widely used format for sharing documents, maintaining their original layout and formatting. PDF files can be opened with Adobe Acrobat Reader, web browsers, and various other applications on both Windows and Mac. Spreadsheet formats: .xls and .xlsx (Microsoft Excel): Spreadsheet files used for calculations, data analysis, and creating charts. Microsoft Excel is the primary software for opening these files, but other applications like Google Sheets and LibreOffice Calc can also open and edit them. Presentation formats: .ppt and .pptx (Microsoft PowerPoint): Presentation files used for creating slideshows and visual presentations. Microsoft PowerPoint is the primary software for opening these files, but other applications like Google Slides and Apple Keynote can also open and edit them. Image formats: .jpeg or .jpg (Joint Photographic Experts Group): A common format for digital photos and images, with good quality and compression. Can be opened and edited with various image editors and viewers, such as Microsoft Paint, Adobe Photoshop, and Apple Preview. .png (Portable Network Graphics): A widely used format for images that supports transparency and lossless compression. Can be opened and edited with most image editors and viewers on Windows and Mac. .gif (Graphics Interchange Format): An image format commonly used for simple animations and small graphics on the web. Can be opened and edited with most image editors and viewers on Windows and Mac. Audio formats: .mp3 (MPEG Audio Layer-3): A popular audio format with lossy compression, widely used for music and other audio content. Can be played on most media players and devices, such as Windows Media Player, VLC, and iTunes. .wav (Waveform Audio File Format): An uncompressed audio format that maintains high-quality audio. Can be played on most media players and devices on Windows and Mac. Video formats: .mp4 (MPEG-4 Part 14): A popular video format used for streaming and sharing video content. Can be played on most media players and devices, such as Windows Media Player, VLC, and QuickTime Player. .avi (Audio Video Interleave): A widely used video format for storing video and audio data in a single file. Can be played on most media players and devices on Windows and Mac. When working with files on Windows and Mac, it’s important to know the appropriate software and applications needed to open and edit specific file formats. Additionally, understanding how file formats affect the quality and compatibility of files can help you choose the best format for your needs. If you need to share a file with someone who has a different operating system or software, consider using a widely supported file format or exporting the file in a format that can be opened by their software. Both Mac and Windows may hide file extensions and you may want to change settings of your file manager (Windows Explorer on Windows or Finder on MacOS) to show this information. How exactly this should be done may differ from one version of the same operating system to another. It is always best to “google” information relevant to your OS. 16.1.7 Organizing files and folders for research projects Organizing files and folders for research projects is crucial for efficient project management and easy access to relevant information. A well-organized structure helps you keep track of your work, collaborate with others, and prevent loss of data. Here are some tips for organizing files and folders for research projects: Create a main project folder: Start by creating a main folder for the entire research project. Name it clearly and descriptively, so you can easily identify it among other folders. Establish a consistent folder structure: Within the main project folder, create subfolders for different components of the research project, such as data, literature, drafts, and presentations. It’s essential to establish a consistent folder structure that is easy to understand and navigate. Example of a folder structure: 57-528_DH_in_AAS_PUA_R/ ├── Data/ │ ├── Raw_Data/ │ └── Processed_Data/ ├── Literature/ │ ├── Articles/ │ └── Books/ ├── Classes/ │ ├── Class_01/ │ ├── Class_02/ │ └── Class_03/ ├── Presentations/ └── Meeting_Notes/ Use clear and descriptive file and folder names: When naming files and folders, use clear, descriptive, and concise names that provide context and indicate the content. Avoid using vague or generic names, and try to include relevant information like dates, version numbers, or authors. Example of clear file names: 2021-05-01_Interview_Transcript_JaneDoe.docx Literature_Review_v3.pdf Use version control: When working on drafts or revising documents, use version numbers or dates in file names to keep track of different iterations. This practice helps you maintain a clear history of changes and makes it easier to locate previous versions if needed. Example of version control in file names: Research_Paper_v1.docx Research_Paper_v2.docx Research_Paper_v3_Final.docx Keep raw and processed data separate: When working with data, create separate folders for raw data (unaltered, original files) and processed data (cleaned or transformed data). This practice helps prevent accidental changes to the original data and makes it easier to track the data processing steps. Regularly backup your data: To prevent data loss, regularly backup your project files and folders to an external storage device or a cloud storage service. Schedule routine backups and ensure that all team members are aware of the backup process. Document your organization system: Create a readme file or a project documentation file that explains the organization system, including the folder structure, file naming conventions, and any other relevant information. This documentation helps maintain consistency and makes it easier for team members or future researchers to understand and navigate the project files. 16.1.8 Demonstration of file navigation using a file explorer (Windows) or Finder (Mac) Windows Here is a step-by-step description of screenshots demonstrating file navigation using the File Explorer on Windows: Screenshot 1 - Opening File Explorer: The first screenshot shows the taskbar at the bottom of the screen, with the File Explorer icon highlighted. This is typically represented by a folder icon. The user clicks on the icon to open File Explorer. Screenshot 2 - File Explorer Window: The second screenshot displays the open File Explorer window, showing the default view when the application is launched. In Windows, this could be the “Quick access” or “This PC” view, with a list of frequently used folders, drives, and network locations. Screenshot 3 - Navigating to a Specific Folder: The third screenshot shows the user clicking on one of the available drives (e.g., “C:”) or folders listed in the left-hand navigation pane, which then displays the contents of the selected drive or folder in the main window. Screenshot 4 - Opening a Folder: The fourth screenshot demonstrates the user double-clicking on a folder within the main window to open it and view its contents. The folder path is displayed at the top of the window, indicating the user’s current location within the file system. Screenshot 5 - Navigating Within a Folder: The fifth screenshot shows the user browsing through the contents of the open folder, which may include files and subfolders. The user can click on column headers (e.g., “Name,” “Date modified,” or “Type”) to sort the contents alphabetically, chronologically, or by file type. Screenshot 6 - Using the Back Button: The sixth screenshot shows the user clicking the “Back” button (a left-pointing arrow) in the upper-left corner of the File Explorer window to return to the previous folder or drive. Screenshot 7 - Searching for Files or Folders: The seventh screenshot demonstrates the user entering a keyword or phrase in the search bar located in the upper-right corner of the File Explorer window. As the user types, a list of matching files and folders appears in the main window. Screenshot 8 - Getting Path of the Open Folder: The eighth screenshot demonstrates the user clicking on the path bar (top part of the window), where, after a click, the proper path appears — now, it can be copied and used as a universal reference to the folder. By following these steps and using the features available in File Explorer, Windows users can effectively navigate and manage their files and folders. Mac Here is a step-by-step description of screenshots demonstrating file navigation using Finder on Mac: Screenshot 1 - Opening Finder: The first screenshot shows the Dock at the bottom of the screen, with the Finder icon highlighted. This is typically represented by a blue, smiling face icon. The user clicks on the icon to open Finder. Screenshot 2 - Finder Window: The second screenshot displays the open Finder window, showing the default view when the application is launched. In macOS, this could be the “Recents” view or a list of locations such as “iCloud Drive,” “AirDrop,” “Applications,” “Desktop,” “Documents,” and “Downloads” in the left-hand sidebar. Screenshot 3 - Navigating to a Specific Folder: The third screenshot shows the user clicking on one of the available locations or folders listed in the left-hand sidebar, which then displays the contents of the selected location or folder in the main window. Screenshot 4 - Opening a Folder: The fourth screenshot demonstrates the user double-clicking on a folder within the main window to open it and view its contents. The folder path is displayed at the bottom of the window, indicating the user’s current location within the file system. Screenshot 5 - Navigating Within a Folder: The fifth screenshot shows the user browsing through the contents of the open folder, which may include files and subfolders. The user can click on the “Sort By” button (represented by a gear icon) in the upper-right corner of the Finder window to sort the contents by various criteria such as “Name,” “Date Modified,” “Date Created,” “Size,” or “Kind.” Screenshot 6 - Using the Back Button: The sixth screenshot shows the user clicking the “Back” button (a left-pointing arrow) in the upper-left corner of the Finder window to return to the previous folder or location. Screenshot 7 - Searching for Files or Folders: The seventh screenshot demonstrates the user entering a keyword or phrase in the search bar located in the upper-right corner of the Finder window. As the user types, a list of matching files and folders appears in the main window. Screenshot 8 - Getting Path of the Open Folder: The eighth screenshot demonstrates the user right-clicking on the folder and selecting option to copy folder as a path (OPTION must be pressed): “Copy FOLDER as Pathname” — now, the path can used as a universal reference to the folder. By following these steps and using the features available in Finder, Mac users can effectively navigate and manage their files and folders. 16.1.9 Command Line Basics 16.1.9.1 General Introduction Command line basics refer to the fundamental concepts and commands used when working with a command-line interface (CLI) on an operating system. The CLI allows users to interact with the computer by entering text-based commands instead of using a graphical user interface (GUI). Here is an outline of command line basics: Understanding the Command Line Interface (CLI) Difference between CLI and GUI Advantages and disadvantages of using CLI Common command-line environments (e.g., Command Prompt and PowerShell on Windows, Terminal on macOS and Linux) Navigating the Command Line Interface Opening the command-line environment (e.g., Terminal, Command Prompt, or PowerShell) Understanding the command prompt (e.g., user@hostname:~$ on Linux/macOS, C:&gt; on Windows) Basic anatomy of a command (command, options, and arguments) Essential Commands Navigation commands cd (change directory) ls or dir (list directory contents) pwd or cd (print working directory) File and directory management commands mkdir (make directory) rmdir or rd (remove directory) cp or copy (copy files and directories) mv or move (move or rename files and directories) rm or del (remove files) File content and manipulation commands cat, more, less, or type (view file contents) grep (search for text in files) nano, vi, or notepad (edit files using a text editor) Advanced Commands Input/output redirection &gt; (redirect output to a file) &lt; (redirect input from a file) &gt;&gt; (append output to a file) Pipes | (pipe output from one command as input to another) Command substitution $(command) or `command` (execute a command and use its output as an argument) Command Line Customization Customizing the command prompt appearance Creating command aliases Setting environment variables Scripting Basics Understanding shell scripts (e.g., .sh for Unix-based systems or .bat for Windows) Creating and executing a basic script Script variables and control structures (loops, conditionals) By mastering these command line basics, users can become more efficient and proficient at navigating and managing files, directories, and processes within their operating system. 16.1.10 Introduction to command line interfaces 16.1.10.1 Windows Command Prompt In the world of computing, there are two primary ways to interact with an operating system: graphical user interfaces (GUI) and command line interfaces (CLI). While graphical user interfaces have become the norm due to their ease of use and visual appeal, command line interfaces remain a powerful and versatile tool, especially for advanced users and system administrators. In this introduction, we will focus on the Windows Command Prompt, a widely used command line interface for the Windows operating system. The Windows Command Prompt, also known as “cmd.exe” or simply “cmd,” provides a text-based interface for interacting with your computer. By entering commands into the Command Prompt, you can navigate the file system, manage files and directories, launch applications, and perform various system tasks without using a mouse or any visual elements. For many users, the Command Prompt can offer more precise control and enable faster, more efficient operations. To open the Command Prompt in Windows, press the “Win + R” keys on your keyboard to open the “Run” dialog box, then type “cmd” and hit “Enter” or click “OK.” Alternatively, you can search for “Command Prompt” in the Windows search bar and select the corresponding result. Once the Command Prompt window is open, you will see a text-based prompt, usually displaying the current user’s name and the current directory (e.g., “C:&gt;”). This is where you can begin entering commands. Some basic commands to help you get started with the Command Prompt include: cd: Change the current directory dir: List the contents of the current directory mkdir: Create a new directory rmdir: Remove an existing directory copy: Copy files from one location to another move: Move or rename files del: Delete files cls: Clear the Command Prompt screen As you become more familiar with the Command Prompt, you can begin to explore more advanced commands, such as input/output redirection, piping, and scripting. These techniques can greatly enhance your productivity and enable you to perform complex tasks with relative ease. In conclusion, the Windows Command Prompt is a powerful tool for users who wish to interact with their computer through a text-based interface. With practice and knowledge of various commands, you can unlock the full potential of the Command Prompt, making it an invaluable asset in your computing toolkit. 16.1.10.2 Mac Terminal Introduction to Command Line Interfaces - Mac Terminal In the world of computing, there are two primary ways to interact with an operating system: graphical user interfaces (GUI) and command line interfaces (CLI). While graphical user interfaces have become the norm due to their ease of use and visual appeal, command line interfaces remain a powerful and versatile tool, especially for advanced users and system administrators. In this introduction, we will focus on the Mac Terminal, a widely used command line interface for the macOS operating system. The Mac Terminal, simply referred to as “Terminal,” provides a text-based interface for interacting with your computer. By entering commands into the Terminal, you can navigate the file system, manage files and directories, launch applications, and perform various system tasks without using a mouse or any visual elements. For many users, the Terminal can offer more precise control and enable faster, more efficient operations. To open the Terminal in macOS, navigate to the “Applications” folder, then the “Utilities” folder, and double-click on “Terminal.” Alternatively, you can use Spotlight search by pressing “Cmd + Space” on your keyboard, typing “Terminal,” and hitting “Enter” or selecting the corresponding result. Once the Terminal window is open, you will see a text-based prompt, usually displaying the current user’s name, hostname, and the current directory (e.g., “user@hostname:~$”). This is where you can begin entering commands. Some basic commands to help you get started with the Terminal include: cd: Change the current directory ls: List the contents of the current directory pwd: Print the current directory path mkdir: Create a new directory rmdir: Remove an existing directory cp: Copy files from one location to another mv: Move or rename files rm: Delete files clear: Clear the Terminal screen As you become more familiar with the Terminal, you can begin to explore more advanced commands, such as input/output redirection, piping, and scripting. These techniques can greatly enhance your productivity and enable you to perform complex tasks with relative ease. In conclusion, the Mac Terminal is a powerful tool for users who wish to interact with their computer through a text-based interface. With practice and knowledge of various commands, you can unlock the full potential of the Terminal, making it an invaluable asset in your computing toolkit. 16.1.11 Command Line: Basic Commands 16.1.11.1 Windows Command Prompt Here is a list of basic commands for the Windows Command Prompt that will help you get started with navigating and managing files and directories: cd: Change directory Usage: cd &lt;directory&gt; Example: cd C:\\Users\\Username\\Documents To switch to a different drive, simply type the letter of the drive followed by colon: D: or C:, after that you can use cd command to navigate to the folder that you need dir: List the contents of the current directory Usage: dir Example: dir mkdir: Create a new directory Usage: mkdir &lt;directory_name&gt; Example: mkdir NewFolder rmdir: Remove an existing directory Usage: rmdir &lt;directory_name&gt; Example: rmdir OldFolder copy: Copy files from one location to another Usage: copy &lt;source&gt; &lt;destination&gt; Example: copy C:\\Users\\Username\\Documents\\file.txt C:\\Users\\Username\\Desktop move: Move or rename files Usage: move &lt;source&gt; &lt;destination&gt; Example: move C:\\Users\\Username\\Documents\\file.txt C:\\Users\\Username\\Desktop del: Delete files Usage: del &lt;file_name&gt; Example: del C:\\Users\\Username\\Documents\\file.txt cls: Clear the Command Prompt screen Usage: cls Example: cls echo: Display text on the screen Usage: echo &lt;text&gt; Example: echo Hello, World! type: Display the contents of a text file Usage: type &lt;file_name&gt; Example: type C:\\Users\\Username\\Documents\\file.txt find: Search for a text string in a file - Usage: find \"&lt;text_to_find&gt;\" &lt;file_name&gt; - Example: find \"search term\" C:\\Users\\Username\\Documents\\file.txt These basic commands will help you navigate and manage files and directories using the Windows Command Prompt. As you become more familiar with the Command Prompt, you can explore more advanced commands and techniques to further enhance your productivity. 16.1.11.2 Mac Terminal Here is a list of basic commands for the Mac Terminal that will help you get started with navigating and managing files and directories: cd: Change directory Usage: cd &lt;directory&gt; Example: cd /Users/Username/Documents ls: List the contents of the current directory Usage: ls Example: ls pwd: Print the current directory path Usage: pwd Example: pwd mkdir: Create a new directory Usage: mkdir &lt;directory_name&gt; Example: mkdir NewFolder rmdir: Remove an existing directory Usage: rmdir &lt;directory_name&gt; Example: rmdir OldFolder cp: Copy files from one location to another Usage: cp &lt;source&gt; &lt;destination&gt; Example: cp /Users/Username/Documents/file.txt /Users/Username/Desktop mv: Move or rename files Usage: mv &lt;source&gt; &lt;destination&gt; Example: mv /Users/Username/Documents/file.txt /Users/Username/Desktop rm: Delete files Usage: rm &lt;file_name&gt; Example: rm /Users/Username/Documents/file.txt clear: Clear the Terminal screen Usage: clear Example: clear echo: Display text on the screen - Usage: echo &lt;text&gt; - Example: echo Hello, World! cat: Display the contents of a text file - Usage: cat &lt;file_name&gt; - Example: cat /Users/Username/Documents/file.txt grep: Search for a text string in a file - Usage: grep \"&lt;text_to_find&gt;\" &lt;file_name&gt; - Example: grep \"search term\" /Users/Username/Documents/file.txt These basic commands will help you navigate and manage files and directories using the Mac Terminal. As you become more familiar with the Terminal, you can explore more advanced commands and techniques to further enhance your productivity. 16.1.12 Creating, moving, renaming, and deleting files and folders 16.1.12.1 Windows Command Prompt In this short tutorial, you will learn how to create, move, rename, and delete files and folders using the Windows Command Prompt. This will help you manage your files and directories more efficiently using a text-based interface. Navigating file system: Go to a specific folder: To navigate to a specific folder, use the cd command followed by the folder path. For example, if you want to go to the folder C:\\Users\\YourUsername\\Documents\\Projects, type: cd C:\\Users\\YourUsername\\Documents\\Projects Go to a parallel folder: To navigate to a parallel folder, first go back to the parent directory using cd .., and then go to the parallel folder. For example, if you are currently in C:\\Users\\YourUsername\\Documents\\Projects and want to go to C:\\Users\\YourUsername\\Documents\\Photos, type: cd ..\\Photos Go two levels up: To go up two levels in the directory structure, use the cd command with double dots (..) twice. For example, if you are in the folder C:\\Users\\YourUsername\\Documents\\Projects\\Project1 and want to go up two levels to C:\\Users\\YourUsername\\Documents, type: cd ..\\.. Creating Files and Folders: Create a new folder using the mkdir command: Usage: mkdir &lt;folder_name&gt; Example: mkdir MyNewFolder Create a new empty file using the type nul &gt; command: Usage: type nul &gt; &lt;file_name&gt; Example: type nul &gt; example.txt Moving and Renaming Files and Folders: Move a file or folder from one location to another using the move command: Usage: move &lt;source&gt; &lt;destination&gt; Example: move example.txt MyNewFolder Rename a file or folder using the rename or ren command: Usage: rename &lt;old_name&gt; &lt;new_name&gt; or ren &lt;old_name&gt; &lt;new_name&gt; Example: rename example.txt new_example.txt Deleting Files and Folders: Delete a file using the del command: Usage: del &lt;file_name&gt; Example: del new_example.txt Delete an empty folder using the rmdir command: Usage: rmdir &lt;folder_name&gt; Example: rmdir MyNewFolder Delete a folder with all its contents using the rmdir command with the /s and /q options: Usage: rmdir /s /q &lt;folder_name&gt; Example: rmdir /s /q MyNewFolder By following this tutorial, you can create, move, rename, and delete files and folders using the Windows Command Prompt. Practice these commands to improve your file management skills and gain confidence in using the Command Prompt. 16.1.12.2 Mac Terminal In this short tutorial, you will learn how to create, move, rename, and delete files and folders using the Mac Terminal. This will help you manage your files and directories more efficiently using a text-based interface. Navigating file system: Go to a specific folder: To navigate to a specific folder, use the cd command followed by the folder path. For example, if you want to go to the folder /Users/YourUsername/Documents/Projects, type: cd /Users/YourUsername/Documents/Projects Go to a parallel folder: To navigate to a parallel folder, first go back to the parent directory using cd .., and then go to the parallel folder. For example, if you are currently in /Users/YourUsername/Documents/Projects and want to go to /Users/YourUsername/Documents/Photos, type: cd ../Photos Go two levels up: To go up two levels in the directory structure, use the cd command with double dots (..) twice. For example, if you are in the folder /Users/YourUsername/Documents/Projects/Project1 and want to go up two levels to /Users/YourUsername/Documents, type: cd ../.. Creating Files and Folders: Create a new folder using the mkdir command: Usage: mkdir &lt;folder_name&gt; Example: mkdir MyNewFolder Create a new empty file using the touch command: Usage: touch &lt;file_name&gt; Example: touch example.txt Moving and Renaming Files and Folders: Move a file or folder from one location to another using the mv command: Usage: mv &lt;source&gt; &lt;destination&gt; Example: mv example.txt MyNewFolder Rename a file or folder using the mv command: Usage: mv &lt;old_name&gt; &lt;new_name&gt; Example: mv example.txt new_example.txt Deleting Files and Folders: Delete a file using the rm command: Usage: rm &lt;file_name&gt; Example: rm new_example.txt Delete an empty folder using the rmdir command: Usage: rmdir &lt;folder_name&gt; Example: rmdir MyNewFolder Delete a folder with all its contents using the rm command with the -r option: Usage: rm -r &lt;folder_name&gt; Example: rm -r MyNewFolder By following this tutorial, you can create, move, rename, and delete files and folders using the Mac Terminal. Practice these commands to improve your file management skills and gain confidence in using the Terminal. 16.1.13 Tips for efficient command line usage Here is a list of tips for efficient command line usage that can help you improve your productivity and make your command line experience more enjoyable: Learn keyboard shortcuts: Familiarize yourself with keyboard shortcuts for common tasks, such as copying, pasting, and moving between words or lines. For example, Ctrl+C and Ctrl+V for copying and pasting, respectively, or Ctrl+Left/Right to move between words. Use command history: Both Windows Command Prompt and Mac Terminal keep a history of commands you have previously executed. Use the Up and Down arrow keys to navigate through your command history and avoid retyping the same commands repeatedly. Auto-completion: Use the Tab key to auto-complete file and folder names, command names, or command options. Pressing Tab once will complete the name if it is unique or show the available options if multiple choices exist. Pressing Tab twice will display all the matching options. Pipelining and redirection: Learn how to use pipes (|) and redirection (&gt;, &gt;&gt;, &lt;) to combine commands and manipulate input/output. For example, you can pipe the output of one command as input to another command or redirect the output of a command to a file. Batch scripting or shell scripting: Automate repetitive tasks by creating batch scripts (Windows) or shell scripts (Mac/Linux). These scripts can execute multiple commands in sequence or even include control structures like loops and conditionals. Aliases and custom functions: Create aliases or custom functions for frequently used commands or command sequences. This can help you save time and avoid typing long commands repeatedly. Learn grep, awk, and sed: Familiarize yourself with text processing utilities like grep, awk, and sed to filter, transform, and manipulate text data efficiently. Version control systems: Learn to use version control systems like Git to manage your code and projects. It helps you keep track of changes and collaborate with others effectively. Explore command-line tools: Discover and experiment with command-line tools and utilities available for your operating system. Many tools can help you accomplish tasks more efficiently than using graphical applications. Customize your environment: Customize your command line environment by tweaking settings, appearance, and behavior to suit your preferences. This can make your command line experience more enjoyable and personalized. By incorporating these tips into your command line usage, you can become more efficient and productive while navigating and managing tasks using the command line interface. "],["bibliography.html", "17 Bibliography", " 17 Bibliography "]]
